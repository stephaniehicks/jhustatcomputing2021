[
  {
    "path": "posts/2021-10-19-introduction-to-machine-learning/",
    "title": "Introduction to machine learning",
    "description": "A brief intro to the why and what for machine learning",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-10-19",
    "categories": [
      "module 3",
      "week 8",
      "machine learning",
      "R",
      "programming"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nWhy machine learning?\nWhat is machine learning?\nNotation\nML as an optimization problem\nThe parts of an ML problem\n\nExample: QuickDraw!\nStart with a question\nGoal setting\nData collection\nData loading and EDA\nData pre-processing\nFeature engineering\n\nSplitting into training, testing, validation\nOverfitting\nBias variance tradeoff\nWhat do you do in training/tuning/testing\nCross validation\n\nModel selection and fitting\nTypes of models\nTrees\n\nModel evaluation\n\nPost-lecture materials\nAdditional Resources\n\n\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://rafalab.github.io/dsbook/introduction-to-machine-learning.html\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://jhu-advdatasci.github.io/2019/lectures/16-intro-ml.html\nhttps://rafalab.github.io/dsbook/introduction-to-machine-learning.html\nLearning objectives\n\nAt the end of this lesson you will:\nBe able to define machine learning (ML) and why we use it?\nBe able to recognize some misuses of ML\nUnderstand the parts of a ML problem\nImplement a machine learning model to classify images into two types: clouds and axes\n\nWhy machine learning?\nToday we are going to be talking about machine learning. This is one of the hottest areas in all of statistics/data science. Machine learning is in the news a ton lately. Some examples include:\nSelf driving cars\nPredicting poverty with satellites\nPredicting breast cancer using Google’s DeepMind AI\nPredictive policing\nThis is such a big idea that universities are investing major dollars to hire faculty in AI and machine learning.\nOn a more personal note you might be interested in AI and machine learning because it is one of the most in demand parts of being a data scientist right now. If you get really good at it you can make a lot of money.\nThe other really exciting reason to focus on AI and ML right now is that there is a lot of room for statistical science. Some of the biggest open problems include:\nFairness in machine learning - tons of work on sampling, causal inference, etc.\nMorality in machine learning - studies of psychology, bias, reporting.\nThere are a ton more, including how to do EDA for machine learning, understanding the potential confounders and bias, understanding the predictive value of a positive and more.\nWhat is machine learning?\nOk so machine learning is super hot right now, but what is machine learning really? You may have learned about the central dogma of statistics that you sample from a population\n\n\n\nFigure 1: Central dogma of statistics: inference (part 1)\n\n\n\n[Source]\nThen you try to guess what will happen in the population from the sample.\n\n\n\nFigure 2: Central dogma of statistics: inference (part 2)\n\n\n\n[Source]\nFor prediction, we have a similar sampling problem\n\n\n\nFigure 3: Central dogma of statistics: prediction (part 1)\n\n\n\n[Source]\nBut now we are trying to build a rule that can be used to predict a single observation’s value of some characteristic using the others.\n\n\n\nFigure 4: Central dogma of statistics: prediction (part 2)\n\n\n\n[Source]\nWe can make this more concrete with a little mathematical notation.\nNotation\nThis section borrowed from Rafa Irizarry’s excellent Data Science Book\nIn machine learning, data comes in the form of:\nthe outcome we want to predict and\nthe features that we will use to predict the outcome.\nHere, we will use \\(Y\\) to denote the outcome and \\(X_1, \\dots, X_p\\) to denote features. Note that features are sometimes referred to as predictors or covariates. We consider all these to be synonyms.\n\nGoal: we want to build an algorithm that takes feature values as input and returns a prediction for the outcome when we do not know the outcome.\nThe machine learning approach is to train an algorithm using a dataset for which we do know the outcome to identify patterns in the training data, and then apply this algorithm in the future to make a prediction when we do not know the outcome.\n\nTypes of prediction problems\nPrediction problems can be divided into categorical and continuous outcomes.\nFor categorical outcomes, \\(Y\\) can be any one of \\(K\\) classes. The number of classes can vary greatly across applications. For example, in the digit reader data, \\(K=10\\) with the classes being the digits 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9. In speech recognition, the outcome are all possible words or phrases we are trying to detect. Spam detection has two outcomes: spam or not spam.\nIn this lesson, we denote the \\(K\\) categories with indexes \\(k=1,\\dots,K\\). However, for binary data we will use \\(k=0,1\\) for mathematical conveniences.\nThe general set-up\nThe general set-up is as follows. We have a series of features and an unknown outcome we want to predict:\n\noutcome\nfeature_1\nfeature_2\nfeature_3\nfeature_4\nfeature_5\n?\nX_1\nX_2\nX_3\nX_4\nX_5\n\nTo build a model that provides a prediction for any set of values \\(X_1=x_1, X_2=x_2, \\dots X_5=x_5\\), we collect data for which we know the outcome:\n\noutcome\nfeature_1\nfeature_2\nfeature_3\nfeature_4\nfeature_5\nY_1\nX_1,1\nX_1,2\nX_1,3\nX_1,4\nX_1,5\nY_2\nX_2,1\nX_2,2\nX_2,3\nX_2,4\nX_2,5\nY_3\nX_3,1\nX_3,2\nX_3,3\nX_3,4\nX_3,5\nY_4\nX_4,1\nX_4,2\nX_4,3\nX_4,4\nX_4,5\nY_5\nX_5,1\nX_5,2\nX_5,3\nX_5,4\nX_5,5\nY_6\nX_6,1\nX_6,2\nX_6,3\nX_6,4\nX_6,5\nY_7\nX_7,1\nX_7,2\nX_7,3\nX_7,4\nX_7,5\nY_8\nX_8,1\nX_8,2\nX_8,3\nX_8,4\nX_8,5\nY_9\nX_9,1\nX_9,2\nX_9,3\nX_9,4\nX_9,5\nY_10\nX_10,1\nX_10,2\nX_10,3\nX_10,4\nX_10,5\n\nWe use the notation \\(\\hat{Y}\\) to denote the prediction. We use the term actual outcome to denote what we ended up observing. So we want the prediction \\(\\hat{Y}\\) to match the actual outcome.\nML as an optimization problem\nThe central problem in machine learning can be thus written very simply as minimizing a distance metric. Let \\(\\hat{Y} = f(\\vec{X})\\) then our goal is to minimize the distance from our estimated function of the predictors to the actual value.\n\\[d(Y - f(\\vec{X}))\\]\n\\(d(\\cdot)\\) could be something as simple as the mean squared distance or something much more complex. The bulk of machine learning research in theoretical computer science and statistics departments focuses on defining different values of \\(d\\) and \\(f\\). We will talk a bit more about this in the next lesson.\nThe parts of an ML problem\nA machine learning problem consists of a few different parts and its important to consider each one. To solve a (standard) machine learning problem you need:\nA data set to train from.\nAn algorithm or set of algorithms you can use to try values of \\(f\\) (e.g. logistic regression, random forest, support vector machine)\nA distance metric \\(d\\) for measuring how close \\(Y\\) is to \\(\\hat{Y}\\) (e.g. mean squared error)\nA definition of what a “good” distance is (e.g. different types of performance metrics)\nWhile each of these components is a technical problem, there has been a ton of work addressing those technical details. The most pressing open issue in machine learning is realizing that though these are technical steps they are not objective steps.\n\nIn other words, how you choose the data, algorithm, metric, and definition of “good” says what you value and can dramatically change the results. A couple of cases where this was a big deal are:\nMachine learning for recidivism - people built ML models to predict who would re-commit a crime. But these predictions were based on historically biased data which led to biased predictions about who would commit new crimes.\nDeciding how self driving cars should act - self driving cars will have to make decisions about how to drive, who they might injure, and how to avoid accidents. Depending on our choices for \\(f\\) and \\(d\\) these might lead to wildly different kinds of self driving cars.\nTry out the moralmachine to see how this looks in practice.\n\nExample: QuickDraw!\nQuick,Draw! is an online game where you are given an object to draw (like a cello, axe, airplane, etc.) and then you have to draw it with your finger. Then a pre-trained deep learning algorithm is applied to guess what kind of a drawing you have made. You can try it out here.\nhttps://quickdraw.withgoogle.com/\nOne interesting thing about this project and something to keep in mind if you are thinking about ways to get cool data is the exchange that Google is making here. They are giving you a fun game to play for free and in return you are giving them a ton of free data. This is the same exchange made by other successful startups:\nreCAPTCHA you click on images to prove you are a human, they give you access to a website.\nDuoLingo you practice learning words, they collect information on the way you say those words or your translations\nBefore going any further, we load a few R packages we will need\n\n\nlibrary(here)\nlibrary(LaF)  # Fast Access to Large ASCII Files\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(dplyr)\nlibrary(rjson)\nlibrary(tibble)\nlibrary(Hmisc)\nlibrary(tidyr)\nlibrary(rpart.plot)\nlibrary(pheatmap)\n\n\n\n\nThe main steps in a machine learning problem are:\nQuestion definition\nGoal setting\nData collection\nTraining/testing/validation splits\nData exploration\nData processing\nModel selection and fitting\nModel evaluation\n\nWe will use the Quick, Draw! dataset to discuss a few different parts of the ML process.\nStart with a question\nThis is the most commonly missed step when developing a machine learning algorithm. ML can very easily be turned into an engineering problem. Just dump the outcome and the features into a black box algorithm and viola!\n\n\n\nBut this kind of thinking can lead to major problems. In general good ML questions:\nHave a plausible explanation for why the features predict the outcome.\nConsider potential variation in both the features and the outcome over time\nAre consistently re-evaluated on criteria 1 and 2 over time.\nFor example, there is a famous case where Google predicted Flu outbreaks based on search results. But the way people searched (due to changes in the algorithm, changes in culture, or other unexplained reasons) led to variation in the search terms people were using. This led to the algorithm predicting wildly badly over time.\n\n\n\nThis is just one example of a spurious correlation, which is one of the big worries. In general all of the reasons for skepticism apply here.\n\nIn our QuickDraw! example, there are a ton of interesting analyses you could do with these data, but we are going to start with something simple. Can we predict from the drawing data what kind of object was drawn? To make matters even simpler we are going to just look at a couple of types of images: axes and clouds.\n\nGoal setting\nOne important part of any machine learning problem is defining what success looks like. This choice very much depends on the application and what you are trying to do.\nFor example, when we talk about the goal in ML we are usually talking about the error rate we want to minimize and how small we want to make it. Consider for each observation we have an outcome \\(y\\) and a set of features \\(\\vec{x}\\). Our goal is to create a function \\(\\hat{y} = \\hat{f}(\\vec{x})\\) such that the distance, \\(d(y,\\hat{f}(\\vec{x}))\\), between the observed and the predicted \\(y\\) is minimized.\nThe two most common distances that show up in machine learning (and the ones you will always be using if you don’t change the defaults!) are:\nRoot mean squared error (RMSE) - this is the most common error measure for regression (read: continuous outcome) problems.\n\\(d(y,\\hat{f}(\\vec{x})) = \\sqrt{\\sum_i \\left(y_i-\\hat{f}(\\vec{x}_i)\\right)^2}\\)\n\nAccuracy - this is the most common error measure for classification (read: factor outcomes) problems.\n\\(d(y,\\hat{f}(\\vec{x})) = \\sum_i 1\\left(y=\\hat{f}(\\vec{x})\\right)\\)\n\nHere we are going to use simple accuracy and say that anything better than guessing is “good enough”.\nBut in general there are a number of other potential error measures:\n\n\n\nHere are a few examples of how they might be relevant.\nPredictive value of a positive - in classification if one group is much less frequent than another, then even high sensitivity/high specificity tests can produce lots of false positives (the classic example is cancer screening, but very relevant for any screening problem).\nMean absolute error - in regression sometimes you want your error to be less sensitive to a few outliers (this might be true in predicting highly skewed outcomes like income or property values) and MAE can reduce the influence of those outliers.\nSpecificity - when the cost of a false negative is really high compared to a false positive and you never want to miss any negatives (say for example missing a person standing in front of a self driving car)\nIn general you need to spend a good amount of time thinking about what the goal is, what the tradeoff of various different errors are and then build that into your model.\nData collection\nHere we’ll focus on one somewhat unique issue that arises often in ML problems - the data are often huge and not sampled randomly.\n\nQuestions:\nWe know that a 5% random sample is better than a 5% non-random sample in measurable ways (e.g. bias, uncertainty assessment).\nBut is an 80% non-random sample “better” than a 5% random sample in measurable terms? 90%? 95%? 99%? (Jeremy Wu 2012)\n“Which one should we trust more: a 1% survey with 60% response rate or a non-probabilistic dataset covering 80% of the population?” (Keiding and Louis, 2016, Journal of Royal Statistical Society, Series B)\n\nThere is a pretty detailed, but really good paper addressing this question by Xiao-Li Meng.\nThe surprising answer is that it depends! If there is correlation between the outcome value and the sampling probability even huge data sets can actually be very small in “effective sample size”. So it is worth thinking very hard about potential correlation between the outcome and the (designed or de facto) sampling scheme.\n\n\n\nBack to quick draw\nOK back to our example. Google has released some of the data from the Quick, Draw! challenge. You can either get the data in raw form or you can get some pre-processed data.\nI downloaded the pre-processed data sets for clouds and axes. These data are available from Google Cloud Platform.\nAlign the drawing to the top-left corner, to have minimum values of 0.\nUniformly scale the drawing, to have a maximum value of 255.\nResample all strokes with a 1 pixel spacing.\nSimplify all strokes using the Ramer–Douglas–Peucker algorithm (strokes are simplified) with an epsilon value of 2.0.\nAll these things make data easier to manage and to represent into a plot. This already represents a lot of work, but even so we still have some more pre-processing to do.\nFirst, we are going to load some of the data into R, it comes in ndjson format and there are lots of drawings.\n\n\nif(!file.exists(here(\"data\", \"axe.ndjson\"))){\n  file_url_axe <- paste0(\"https://storage.googleapis.com/quickdraw_dataset/full/simplified/axe.ndjson\")\n  download.file(file_url_axe, \n                destfile=here(\"data\", \"axe.ndjson\"))\n\n  file_url_cloud <- paste0(\"https://storage.googleapis.com/quickdraw_dataset/full/simplified/cloud.ndjson\")\n  download.file(file_url_cloud, \n                destfile=here(\"data\", \"cloud.ndjson\"))\n\n}\nlist.files(here(\"data\"))\n\n\n [1] \"2016-07-19.csv.bz2\"       \"axe.ndjson\"              \n [3] \"bmi_pm25_no2_sim.csv\"     \"chicago.rds\"             \n [5] \"Chinook.sqlite\"           \"chopped.RDS\"             \n [7] \"cloud.ndjson\"             \"flights.csv\"             \n [9] \"maacs_sim.csv\"            \"nycflights13\"            \n[11] \"storms_2004.csv.gz\"       \"team_standings.csv\"      \n[13] \"tuesdata_rainfall.RDS\"    \"tuesdata_temperature.RDS\"\n\nNext, we are going to read in 100 drawings of each class using the sample_lines() function from the LaF package.\n\n\nset.seed(123)\naxes_json = LaF::sample_lines(here(\"data\",\"axe.ndjson\"), n = 100) \nset.seed(123)\nclouds_json = LaF::sample_lines(here(\"data\",\"cloud.ndjson\"), n = 100)\n\n\n\nData loading and EDA\nBefore we talk about data exploration and processing, it is important to look at your data and think about what you find in it.\n\nAlso, I want to point out you should do this exploration only in the training set. However, in this example, we are going split the data at a later point.\nIf you want to know more about this concept, read about data leakage.\n\nOK the data are not in a format we can do anything with yet. Each line is a json object:\n\n\naxes_json[[1]]\n\n\n[1] \"{\\\"word\\\":\\\"axe\\\",\\\"countrycode\\\":\\\"US\\\",\\\"timestamp\\\":\\\"2017-01-23 21:25:30.06067 UTC\\\",\\\"recognized\\\":true,\\\"key_id\\\":\\\"4842320119726080\\\",\\\"drawing\\\":[[[69,74,75,73,70,79,98,105,111,110,70],[76,92,118,239,252,255,252,208,133,73,66]],[[70,57,20,0,28,91,107,114,115,140,134,123,116,112],[66,74,87,0,17,31,45,59,78,95,75,55,50,37]],[[45,50,48],[20,51,74]]]}\"\n\nSo the next thing I did was google “quick draw data ndjson rstats”. I found a tutorial and lifted some code for processing ndjson data into data frames.\n\n\nparse_drawing = function(list)\n{\n  lapply(list$drawing, function(z) {tibble(x=z[[1]], y=z[[2]])}) %>% \n    bind_rows(.id = \"line\") %>% \n    mutate(drawing=list$key_id, row_id=row_number())\n}\n\n\n\nUsing this code I can get our first axe out\n\n\nfirst_axe = rjson::fromJSON(axes_json[[1]]) %>% \n  parse_drawing()\nfirst_axe\n\n\n# A tibble: 28 × 5\n   line      x     y drawing          row_id\n   <chr> <dbl> <dbl> <chr>             <int>\n 1 1        69    76 4842320119726080      1\n 2 1        74    92 4842320119726080      2\n 3 1        75   118 4842320119726080      3\n 4 1        73   239 4842320119726080      4\n 5 1        70   252 4842320119726080      5\n 6 1        79   255 4842320119726080      6\n 7 1        98   252 4842320119726080      7\n 8 1       105   208 4842320119726080      8\n 9 1       111   133 4842320119726080      9\n10 1       110    73 4842320119726080     10\n# … with 18 more rows\n\nOk this doesn’t look like much, but we could plot it to see if it looks like an axe.\n\n\nggplot(first_axe,aes(x, y)) +\n    geom_point() + \n    scale_x_continuous(limits=c(0, 255))+\n    scale_y_reverse(limits=c(255, 0))+\n    theme_minimal()\n\n\n\n\nThis sort of looks ok, but maybe a better way to look at it is to actually draw the lines.\n\n\nggplot(first_axe,aes(x, y)) +\n    geom_path(aes(group = line), lwd=1)+\n    scale_x_continuous(limits=c(0, 255))+\n    scale_y_reverse(limits=c(255, 0))+\n    theme_minimal()\n\n\n\n\nHey that sort of looks like an axe! Let’s see another one.\n\n\nrjson::fromJSON(axes_json[[2]]) %>% \n    parse_drawing() %>% \n    ggplot(aes(x, y)) +\n    geom_path(aes(group = line), lwd=1)+\n    scale_x_continuous(limits=c(0, 255))+\n    scale_y_reverse(limits=c(255, 0))+\n    theme_minimal()\n\n\n\n\nIf we were doing this for real, I’d make plots for a large sample of these, understand the variation (and look for mislabeled drawings, messed up observations, etc.).\nNext let’s look at a cloud\n\n\nrjson::fromJSON(clouds_json[[1]]) %>% \n    parse_drawing() %>% \n    ggplot(aes(x, y)) +\n    geom_path(aes(group = line), lwd=1)+\n    scale_x_continuous(limits=c(0, 255))+\n    scale_y_reverse(limits=c(255, 0))+\n    theme_minimal()\n\n\n\n\nYup, looks like a cloud!\nData pre-processing\nFeature engineering\nOne of the key issues in building a model is feature engineering. Feature engineering is a step in a machine learning model where we construct the covariates (or features, \\(\\vec{x}\\)) that you will feed into the prediction algorithms.\nIn general, feature engineering is particularly important for “unstructured” data. For example taking a pile of text like this Emily Dickenson quote from the tidy text tutorial:\n\n\nlibrary(tidytext)\ntext <- c(\"Because I could not stop for Death -\",\n          \"He kindly stopped for me -\",\n          \"The Carriage held but just Ourselves -\",\n          \"and Immortality\")\n\n\n\nAnd turn it into something like counts of each word\n\n\ntext_df <- tibble(line = 1:4, text = text)\n\ntext_df %>%\n  unnest_tokens(word, text) %>%\n    count(word)\n\n\n# A tibble: 19 × 2\n   word            n\n   <chr>       <int>\n 1 and             1\n 2 because         1\n 3 but             1\n 4 carriage        1\n 5 could           1\n 6 death           1\n 7 for             2\n 8 he              1\n 9 held            1\n10 i               1\n11 immortality     1\n12 just            1\n13 kindly          1\n14 me              1\n15 not             1\n16 ourselves       1\n17 stop            1\n18 stopped         1\n19 the             1\n\nThis used to be something that was almost exclusively done by expert humans, but is now often done by deep learning algorithms which do “automatic feature selection”.\nBack to our data above, e.g. \n\n\nrjson::fromJSON(axes_json[[1]]) %>% parse_drawing()\n\n\n# A tibble: 28 × 5\n   line      x     y drawing          row_id\n   <chr> <dbl> <dbl> <chr>             <int>\n 1 1        69    76 4842320119726080      1\n 2 1        74    92 4842320119726080      2\n 3 1        75   118 4842320119726080      3\n 4 1        73   239 4842320119726080      4\n 5 1        70   252 4842320119726080      5\n 6 1        79   255 4842320119726080      6\n 7 1        98   252 4842320119726080      7\n 8 1       105   208 4842320119726080      8\n 9 1       111   133 4842320119726080      9\n10 1       110    73 4842320119726080     10\n# … with 18 more rows\n\nA bunch of data processing has been done for us, but the data are not quite ready to be fed into an algorithm yet.\nTo do that, we would need a data frame with each row equal to one drawing and each column equal to one feature for that drawing, with an extra column for the drawing output.\nTo do this, we need to think about creating a standardized grid for storing our data on. However, the choice of grid is decision left up to us (you can think of this as feature engineering).\nAnother think we might want is for our data to be of a manageable size (again the choice of how we do this is another decision left up to us – more feature engineering).\nPoints on a regular grid\nLet’s start by creating a regular grid of 256 x and y values.\n\n\ngrid_dat = as_tibble(expand.grid(x = 1:256,y=1:256))\ndim(grid_dat)\n\n\n[1] 65536     2\n\nhead(grid_dat)\n\n\n# A tibble: 6 × 2\n      x     y\n  <int> <int>\n1     1     1\n2     2     1\n3     3     1\n4     4     1\n5     5     1\n6     6     1\n\nNow we could make each x, y value be a grid point with a join (this is overkill)\n\n\ngrid_axe = left_join(grid_dat,first_axe)\ngrid_axe\n\n\n# A tibble: 65,537 × 5\n       x     y line  drawing row_id\n   <dbl> <dbl> <chr> <chr>    <int>\n 1     1     1 <NA>  <NA>        NA\n 2     2     1 <NA>  <NA>        NA\n 3     3     1 <NA>  <NA>        NA\n 4     4     1 <NA>  <NA>        NA\n 5     5     1 <NA>  <NA>        NA\n 6     6     1 <NA>  <NA>        NA\n 7     7     1 <NA>  <NA>        NA\n 8     8     1 <NA>  <NA>        NA\n 9     9     1 <NA>  <NA>        NA\n10    10     1 <NA>  <NA>        NA\n# … with 65,527 more rows\n\ngrid_axe %>% count(is.na(line))\n\n\n# A tibble: 2 × 2\n  `is.na(line)`     n\n  <lgl>         <int>\n1 FALSE            27\n2 TRUE          65510\n\nWe see most of the lines are NA. Let’s add an indicator of whether a particular value is NA or not.\n\n\ngrid_axe = grid_axe %>%\n   mutate(pixel = ifelse(is.na(line),0,1))\nwhich(grid_axe$pixel == 1)\n\n\n [1]  4124  4909  7771  9328 11371 12660 12850 13947 14962 16710 16711\n[12] 18543 18737 18746 19079 19270 19828 22037 23371 24205 30028 33904\n[23] 53098 61002 64327 64355 65104\n\n\n\ngrid_axe[which(grid_axe$pixel == 1),]\n\n\n# A tibble: 27 × 6\n       x     y line  drawing          row_id pixel\n   <dbl> <dbl> <chr> <chr>             <int> <dbl>\n 1    28    17 2     4842320119726080     16     1\n 2    45    20 3     4842320119726080     26     1\n 3    91    31 2     4842320119726080     17     1\n 4   112    37 2     4842320119726080     25     1\n 5   107    45 2     4842320119726080     18     1\n 6   116    50 2     4842320119726080     24     1\n 7    50    51 3     4842320119726080     27     1\n 8   123    55 2     4842320119726080     23     1\n 9   114    59 2     4842320119726080     19     1\n10    70    66 1     4842320119726080     11     1\n# … with 17 more rows\n\nData set of a manageable size\nLet’s try subsampling this down to a smaller image. We’ll use Hmisc::cut2() to cut a numeric variable into intervials. It’s similar to cut(), but left endpoints are inclusive and labels are of the form [lower, upper), except that last interval is [lower,upper]. However, we will use levels.mean=TRUE to make the new categorical vector have levels attribute that is the group means of grid_axe$x instead of interval endpoint labels.\n\n\ngrid_axe$xgroup = Hmisc::cut2(grid_axe$x,g=16,levels.mean=TRUE) # g is number of quantile groups\ntable(grid_axe$xgroup)\n\n\n\n  9.000  25.500  41.500  57.500  72.999  88.500 104.500 120.500 \n   4352    4096    4096    4096    3841    4096    4096    4096 \n136.500 152.500 168.500 184.500 200.500 216.500 232.500 248.500 \n   4096    4096    4096    4096    4096    4096    4096    4096 \n\n\n\ngrid_axe$ygroup = Hmisc::cut2(grid_axe$y,g=16,levels.mean=TRUE)\ngrid_axe\n\n\n# A tibble: 65,537 × 8\n       x     y line  drawing row_id pixel xgroup    ygroup   \n   <dbl> <dbl> <chr> <chr>    <int> <dbl> <fct>     <fct>    \n 1     1     1 <NA>  <NA>        NA     0 \"  9.000\" \"  9.000\"\n 2     2     1 <NA>  <NA>        NA     0 \"  9.000\" \"  9.000\"\n 3     3     1 <NA>  <NA>        NA     0 \"  9.000\" \"  9.000\"\n 4     4     1 <NA>  <NA>        NA     0 \"  9.000\" \"  9.000\"\n 5     5     1 <NA>  <NA>        NA     0 \"  9.000\" \"  9.000\"\n 6     6     1 <NA>  <NA>        NA     0 \"  9.000\" \"  9.000\"\n 7     7     1 <NA>  <NA>        NA     0 \"  9.000\" \"  9.000\"\n 8     8     1 <NA>  <NA>        NA     0 \"  9.000\" \"  9.000\"\n 9     9     1 <NA>  <NA>        NA     0 \"  9.000\" \"  9.000\"\n10    10     1 <NA>  <NA>        NA     0 \"  9.000\" \"  9.000\"\n# … with 65,527 more rows\n\nNow I can convert these to numbers so we’ll have them later\n\n\ngrid_axe = grid_axe %>% \n    mutate(xgroup = as.numeric(as.character(xgroup)) - 7.5) %>%\n    mutate(ygroup = as.numeric(as.character(ygroup)) - 7.5)\n\ntable(grid_axe$xgroup)\n\n\n\n   1.5     18     34     50 65.499     81     97    113    129    145 \n  4352   4096   4096   4096   3841   4096   4096   4096   4096   4096 \n   161    177    193    209    225    241 \n  4096   4096   4096   4096   4096   4096 \n\ngrid_axe\n\n\n# A tibble: 65,537 × 8\n       x     y line  drawing row_id pixel xgroup ygroup\n   <dbl> <dbl> <chr> <chr>    <int> <dbl>  <dbl>  <dbl>\n 1     1     1 <NA>  <NA>        NA     0    1.5    1.5\n 2     2     1 <NA>  <NA>        NA     0    1.5    1.5\n 3     3     1 <NA>  <NA>        NA     0    1.5    1.5\n 4     4     1 <NA>  <NA>        NA     0    1.5    1.5\n 5     5     1 <NA>  <NA>        NA     0    1.5    1.5\n 6     6     1 <NA>  <NA>        NA     0    1.5    1.5\n 7     7     1 <NA>  <NA>        NA     0    1.5    1.5\n 8     8     1 <NA>  <NA>        NA     0    1.5    1.5\n 9     9     1 <NA>  <NA>        NA     0    1.5    1.5\n10    10     1 <NA>  <NA>        NA     0    1.5    1.5\n# … with 65,527 more rows\n\nNow we can average within groups of pixels to get a smaller image\n\n\nsmall_axe = grid_axe %>% \n    group_by(xgroup,ygroup) %>%\n    summarise(pixel=mean(pixel))\n\nsmall_axe\n\n\n# A tibble: 256 × 3\n# Groups:   xgroup [16]\n   xgroup ygroup pixel\n    <dbl>  <dbl> <dbl>\n 1    1.5    1.5     0\n 2    1.5   18       0\n 3    1.5   34       0\n 4    1.5   50       0\n 5    1.5   65.5     0\n 6    1.5   81       0\n 7    1.5   97       0\n 8    1.5  113       0\n 9    1.5  129       0\n10    1.5  145       0\n# … with 246 more rows\n\nRemember this was our original axe\n\n\nggplot(first_axe,aes(x, y)) +\n    geom_point() +\n    scale_x_continuous(limits=c(0, 255))+\n    scale_y_reverse(limits=c(255, 0))+\n    theme_minimal()\n\n\n\n\nNow we can look at the small version - it looks similar - whew! :)\n\n\nsmall_axe %>% \n  filter(pixel > 0) %>%\n  ggplot(aes(xgroup, ygroup)) +\n    geom_point() +\n    scale_x_continuous(limits=c(0, 255))+\n    scale_y_reverse(limits=c(255, 0))+\n    theme_minimal()\n\n\n\n\nDoing this for all axes and clouds\nNow let’s do this for all axes and clouds\n\n\nimg_dat = tibble(pixel=NA, type=NA, drawing=NA, pixel_number=NA)\n\n\n\n\n\n#First axes\n\nfor(i in 1:100){\n    tmp_draw = rjson::fromJSON(axes_json[[i]]) %>% parse_drawing()\n    \n    grid_draw = left_join(grid_dat,tmp_draw) %>%\n           mutate(pixel = ifelse(is.na(line),0,1)) \n    \n    grid_draw$xgroup = cut2(grid_draw$x,g=16,levels.mean=TRUE)\n    grid_draw$ygroup = cut2(grid_draw$y,g=16,levels.mean=TRUE)\n    \n    small_draw = grid_draw %>% \n        mutate(xgroup = as.numeric(as.character(xgroup)) - 7.5) %>%\n        mutate(ygroup = as.numeric(as.character(ygroup)) - 7.5) %>%\n    group_by(xgroup,ygroup) %>%\n    summarise(pixel=mean(pixel)) %>% ungroup() %>%\n        select(pixel) %>%\n        mutate(type=\"axe\",drawing=i,pixel_number=row_number())\n    img_dat = img_dat %>% bind_rows(small_draw)\n}\n\n\n\n#Then clouds\n\nfor(i in 1:100){\n    tmp_draw = rjson::fromJSON(clouds_json[[i]]) %>% parse_drawing()\n    \n    grid_draw = left_join(grid_dat,tmp_draw) %>%\n           mutate(pixel = ifelse(is.na(line),0,1)) \n    \n    grid_draw$xgroup = cut2(grid_draw$x,g=16,levels.mean=TRUE)\n    grid_draw$ygroup = cut2(grid_draw$y,g=16,levels.mean=TRUE)\n    \n    small_draw = grid_draw %>% \n        mutate(xgroup = as.numeric(as.character(xgroup)) - 7.5) %>%\n        mutate(ygroup = as.numeric(as.character(ygroup)) - 7.5) %>%\n    group_by(xgroup,ygroup) %>%\n    summarise(pixel=mean(pixel)) %>% ungroup() %>%\n        select(pixel) %>%\n        mutate(type=\"cloud\",drawing=i,pixel_number=row_number())\n    img_dat = img_dat %>% bind_rows(small_draw)\n}\n\n\n\nNow let’s look at this new data frame\n\n\nimg_dat = img_dat[-1,]\nimg_dat\n\n\n# A tibble: 51,200 × 4\n   pixel type  drawing pixel_number\n   <dbl> <chr>   <int>        <int>\n 1     0 axe         1            1\n 2     0 axe         1            2\n 3     0 axe         1            3\n 4     0 axe         1            4\n 5     0 axe         1            5\n 6     0 axe         1            6\n 7     0 axe         1            7\n 8     0 axe         1            8\n 9     0 axe         1            9\n10     0 axe         1           10\n# … with 51,190 more rows\n\nWe can use pivot_wider() and viola we finally have a processed data set!\n\n\nlibrary(tidyr)\nimg_final <- \n  img_dat %>%\n    pivot_wider(names_from = pixel_number, values_from = pixel)\nnames(img_final) = c(\"type\",\"drawing\", paste0(\"pixel\",1:256))\nimg_final \n\n\n# A tibble: 200 × 258\n   type  drawing  pixel1  pixel2  pixel3  pixel4 pixel5  pixel6\n   <chr>   <int>   <dbl>   <dbl>   <dbl>   <dbl>  <dbl>   <dbl>\n 1 axe         1 0       0       0       0            0 0      \n 2 axe         2 0       0.00368 0.00368 0            0 0      \n 3 axe         3 0       0.00368 0       0            0 0.00368\n 4 axe         4 0.00391 0       0.00391 0            0 0      \n 5 axe         5 0       0       0       0.00368      0 0      \n 6 axe         6 0       0       0.00391 0            0 0      \n 7 axe         7 0       0       0.00781 0            0 0      \n 8 axe         8 0       0       0.00391 0            0 0      \n 9 axe         9 0.00781 0       0.00391 0            0 0      \n10 axe        10 0.0117  0       0       0.00391      0 0      \n# … with 190 more rows, and 250 more variables: pixel7 <dbl>,\n#   pixel8 <dbl>, pixel9 <dbl>, pixel10 <dbl>, pixel11 <dbl>,\n#   pixel12 <dbl>, pixel13 <dbl>, pixel14 <dbl>, pixel15 <dbl>,\n#   pixel16 <dbl>, pixel17 <dbl>, pixel18 <dbl>, pixel19 <dbl>,\n#   pixel20 <dbl>, pixel21 <dbl>, pixel22 <dbl>, pixel23 <dbl>,\n#   pixel24 <dbl>, pixel25 <dbl>, pixel26 <dbl>, pixel27 <dbl>,\n#   pixel28 <dbl>, pixel29 <dbl>, pixel30 <dbl>, pixel31 <dbl>, …\n\nSplitting into training, testing, validation\nNow that we have our data processed an important step is to break the data up into a training, testing, and validation set. In general, people use these words in different ways:\n\n\n\nI actually like this proposal to call them “training, tuning, and testing” sets, so let’s use that.\n\n\n\nBut the reason for this splitting is that we want to avoid being overly optimistic or “overfitting” on the training data. That would prevent us from predicting well on new samples.\nOverfitting\nHere is a funny example from XKCD to illustrate overfitting\n\n\n\nThe basic idea is that if you keep adding predictors, the model will “predict well” on the data you have, no matter how well we expect it to do in the future.\nThe key thing to keep in mind is that there are two types of variation in any data set, the “signal” and the “noise”. Using math notation, imagine that the “real” model for a data set is:\n\\[y_i = \\underbrace{g(\\vec{x}_i)}_{signal} + \\underbrace{e_i}_{noise}\\]\nLet’s use a concrete, simple example:\n\\[y_i = \\underbrace{x^2}_{signal} + \\underbrace{e_i}_{noise}\\]\nImagine we want to “learn” a model of the form:\n\\[y_i = \\sum_{k=1}^K b_k g_k(x_i) + e_i\\]\nThen, the model\nFits if \\(\\sum_{k=1}^K b_k g_k(x) \\approx x^2\\).\nOverfits if \\(\\sum_{k=1}^K b_k g_k(x) \\approx x^2 + e_i\\)\nUnderfits if \\(\\sum_{k=1}^K b_k g_k(x) \\neq x^2\\)\nLet’s simulate from the example above to give a better idea:\n\n\nlibrary(tibble)\nlibrary(splines)\nlibrary(modelr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n\n\n\ndat = tibble(x = rnorm(30),y=rnorm(30,mean=x^2))\nggplot(dat,aes(x=x,y=y)) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\nNow let’s fit three models to this data. One that underfits, one that fits, and one that overfits.\n\n\n# To make plotting easier later\ndat  = dat %>%\n  arrange(x)\n\nlm_under = lm(y ~ x,data=dat)\n# using natural splines to model a non-linear relationship with piecewise cubic polynomials\n# can specificy df (degrees of freedom) and then ns() chooses (df - 1) interior knots at certain quantiles of x\nlm_fits = lm(y ~ ns(x,df=2),data=dat) \nlm_over = lm(y ~ ns(x,df=10),data=dat)\n\ndat_pred = dat %>%\n    add_predictions(lm_fits,\"fits\") %>%\n    add_predictions(lm_under,\"under\") %>%\n    add_predictions(lm_over,\"over\")\n\ndat_pred  = dat_pred %>% \n  pivot_longer(-c(x,y), names_to = \"fit_type\", values_to = \"fit\")\n\nggplot(dat_pred,aes(x=x,y=y)) + \n  geom_point() + \n  geom_line(aes(x=x,y=fit,group=fit_type,color=fit_type)) + \n  theme_minimal()\n\n\n\n\nYou can kind of see that the blue line misses the signal, the red line fits pretty well, and the green line seems to capture a little too much of the noise. If we look at the errors of each approach we get:\n\n\ndat_pred %>% \n  mutate(res2 = (y - fit)^2) %>%\n    group_by(fit_type) %>%\n    summarise(rmse = sqrt(sum(res2)))\n\n\n# A tibble: 3 × 2\n  fit_type  rmse\n  <chr>    <dbl>\n1 fits      6.10\n2 over      4.58\n3 under     9.12\n\nIt looks like the overfitting approach was best, but we can probably guess that won’t work on a new data set:\n\n\ndat2 = tibble(x = rnorm(30),y=rnorm(30,mean=x^2)) \ndat2_pred = dat2 %>%\n    add_predictions(lm_fits,\"fits\") %>%\n    add_predictions(lm_under,\"under\") %>%\n    add_predictions(lm_over,\"over\")\n\ndat2_pred <-  \n  dat2_pred %>% \n  pivot_longer(-c(x,y), names_to = \"fit_type\", values_to = \"fit\")\n\nggplot(dat2_pred,aes(x=x,y=y)) + geom_point() + theme_minimal() + \n    geom_line(aes(x=x,y=fit,group=fit_type,color=fit_type))\n\n\n\n\nWhere the best model will be the one that captures the signal (which remains fixed) and not the noise (which changes).\n\n\ndat2_pred %>% mutate(res2 = (y - fit)^2) %>%\n    group_by(fit_type) %>%\n    summarise(rmse = sqrt(sum(res2)))\n\n\n# A tibble: 3 × 2\n  fit_type  rmse\n  <chr>    <dbl>\n1 fits      5.45\n2 over      6.03\n3 under     7.86\n\nBias variance tradeoff\n\nOverfitting is related to another general concept - the bias variance trade-off.\nIn general, the more predictors you have in a model the lower the bias but the higher the variance. This is called the “bias-variance trade-off”.\n\nTo see this, let’s fit these models in 100 simulated data sets and see what the models predict for a x value of 0 (the prediction should equal 0).\n\n\nover = under = fits = rep(0,100)\nex_dat = tibble(x=0)\nfor(i in 1:100){\n  new_dat = tibble(x = rnorm(30),y=rnorm(30,mean=x^2))   \n  lm_under = lm(y ~ x,data=new_dat)\n  lm_fits = lm(y ~ ns(x,df=2),data=new_dat)\n  lm_over = lm(y ~ ns(x,df=10),data=new_dat)\n  over[i] = predict(lm_over,ex_dat)\n  under[i] = predict(lm_under,ex_dat)\n  fits[i] = predict(lm_fits,ex_dat)\n}\n\nresults = tibble(over,under,fits) %>%\n    pivot_longer(names_to = \"type\", values_to = \"prediction\",c(over,under,fits))\n\n\n\nThe results show that when we fit the exact right model we do best (no surprise there!).\nWhen the model is too complex we get low bias (values predicted close to zero on average) but high variance.\nWhen the model is not complex enough we get high bias (values predicted away from zero) but low variance.\n\n\nresults %>% \n    ggplot(aes(y=prediction,group=type,fill=type)) +\n    geom_boxplot() +\n    theme_minimal()\n\n\n\n\nIn general you will not know the true model, so the goal is to try to pick a model that gives a happy medium on the bias-variance trade-off (of course depending on your goals).\nWhat do you do in training/tuning/testing\nImagine we have broken the data into three components: \\(X_{tr}, X_{tu}, X_{te}\\). Now we need to “fit” the model. Let’s briefly talk about what this means. A machine learning model has two parts:\nAn algorithm\nA set of parameters\nThe algorithm would be something like regression models with splines:\n\\[y_i = \\sum_{k=1}^K b_k g_k(x_i) + e_i\\]\nAnd the parameters would be the choices of \\(b_k\\), \\(g_k\\) and \\(K\\). These parameters are “fit” by estimating them from the data or fixing them in advance.\nTraining - In the training set, you try different algorithms, estimate their parameters, and see which one works best.\nTuning - Once you have settled on a single algorithm (or a small set of algorithms), you use the tuning set to estimate which parameters work best outside of the training sample you originally built on.\nTesting - Once your algorithm and all your parameters for your model are fixed, then you apply that fitted model just one time to the test set to evaluate the error rate for your model realistically.\nCross validation\nWithin the training set you are choosing between algorithms and parameters, but like we saw above, if you use the whole training set you may end up overfitting to the noise in the data. So when selecting algorithms and parameters, you need some way to make sure you don’t just pick the algorithm that is most overfit.\nThe typical way people do this is by cross-validation (figure borrowed from Rafa’s Data Science Book). K-fold cross validation just splits up the training set into K pieces. You build the model on part of the training data and apply it to the rest. This gives you a better evaluation of the out of sample error - so will allow you to rank models in a better way.\n\n\n\nYou can also use the bootstrap. But you need to adjust for the fact that the training and testing sets are random samples.\nBack to quick draw\nHere we are going to simply use training and testing using the createDataPartition() function in the caret package with the argument p being the percentages of data that goes into training:\n\n\ntrain_set = createDataPartition(y = img_final$type, p = 0.5,\n                                list=FALSE)\n\ntrain_dat = img_final[train_set,]\ntest_dat = img_final[-train_set,]\n\n\n\nWe leave the test set alone until the very end!\nModel selection and fitting\nA lot of machine learning is considering variations on the equation:\n\\[d(y,f(\\vec{x}))\\]\nwhere the things we are varying is the choice of distance metric \\(d()\\) which we have already discussed. We have talked a lot less about varying \\(f()\\) which is the most common topic for many machine learning books. Here we will briefly review a couple of the key algorithms.\nThe first thing to keep in mind is that with well engineered features, often simple algorithms due almost as well as more advanced algorithms.\nThe other thing to keep in mind with these types of algorithms is that there are often important tradeoffs.\n\n\n\nThe important tradeoffs are:\nInterpretability versus accuracy\nSpeed versus accuracy\nSimplicity versus accuracy\nScalability versus accuracy\nTypes of models\nThere are a few key ideas you should know about that define most regression models you will run into in practice.\nRegression\nTrees\nEnsembling\nNeural Networks\nToday we will briefly demonstrate trees.\nTrees\nClassification and regression trees are an extremely popular approach to prediction. The basic algorithm for a classification tree is the following:\nStart with all variables in one group\nFind the variable/split that best separates the outcomes\nDivide the data into two groups (“leaves”) on that split (“node”)\nWithin each split, find the best variable/split that separates the outcomes\nContinue until the groups are too small or sufficiently “pure”\nThis is an example tree:\n\n\n\nThe big question is how to define “best separates the outcomes” in Step 4 of the general algorithm. For continuous data you might minimize the residual sum of squares in each group. For binary data you might measure misclassification or information gain.\nStrengths - Trees are usually easy to understand and can be fit quickly.\nWeaknesses - Trees have a tendency not to be terribly accurate compared to some other methods and may overfit.\nOur example\nBased on these plots of a few of the features:\n\n\nggplot(train_dat, aes(x=type,y=pixel1)) + \n  geom_violin() + \n  theme_minimal() + \n  scale_y_log10()\n\n\n\n\n\n\nggplot(train_dat, aes(x=type,y=pixel2)) + \n  geom_violin() + \n  theme_minimal() + \n  scale_y_log10()\n\n\n\n\nWe should be able to do ok for this model fitting. We can fit models using the caret package. The caret package simplifies a lot of model fitting for machine learning. We can use the train command to do this in R.\n\n\nmod = train(as.factor(type) ~ ., data=train_dat,method=\"rpart\")\nmod\n\n\nCART \n\n100 samples\n257 predictors\n  2 classes: 'axe', 'cloud' \n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 100, 100, 100, 100, 100, 100, ... \nResampling results across tuning parameters:\n\n  cp    Accuracy   Kappa    \n  0.14  0.8385178  0.6751338\n  0.26  0.7900434  0.5798719\n  0.54  0.6274736  0.2868931\n\nAccuracy was used to select the optimal model using the\n largest value.\nThe final value used for the model was cp = 0.14.\n\nHere you can see we have reasonable accuracy, this accuracy is estimated using bootstrapping only the training set. We can look at the final model fit after model selection using the finalModel argument.\n\n\nmod$finalModel\n\n\nn= 100 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 100 50 axe (0.50000000 0.50000000)  \n  2) pixel246< 0.001953125 73 23 axe (0.68493151 0.31506849)  \n    4) pixel243< 0.001953125 58  9 axe (0.84482759 0.15517241) *\n    5) pixel243>=0.001953125 15  1 cloud (0.06666667 0.93333333) *\n  3) pixel246>=0.001953125 27  0 cloud (0.00000000 1.00000000) *\n\nYou can use the rpart.plot package to visualize what is actually going on here.\n\n\nrpart.plot(mod$finalModel)\n\n\n\n\nModel evaluation\nThe last step is model evaluation. A good model evaluation includes the components:\nEvaluation of the model predictions based on the goal you stated at the beginning of the problem.\nExploratory analysis of predictions to ensure that there aren’t obvious problems\nConsideration of the practical and ethical consequences of deploying your model.\n\nwhy is this important?\nA while back Amazon developed an AI algorithm for predicting who they should hire. They did a good job of evaluating criteria 1 - they knew they could accurately predict in their training set.\nHowever, they didn’t do exploratory analysis to identify what their model was using to make predictions. They also didn’t do a careful job of evaluating the implications of their model in terms of bias.\nThis led to some major problems.\n\nback to model evaluation\nBefore evaluating our model in the test set, we want to understand what is going on with our prediction. This is an active and open area of research: the interpretation of results from black box machine learning algorithms. We can do this in a couple of ways. One approach that has seen some traction is locally interpretable model agnostic explanations (lime). This approach fits very simple local models to approximate the complicated model in a local neighborhood.\n\n\n\nThen for each prediction you can see how much the features are positively or negatively correlated with the complex model near that prediction. To do this, you can use the lime package.\nOr you can start looking at the data for individual features.\n\n\nggplot(img_final,aes(x=type,y=pixel246)) + \n  geom_violin() + \n  theme_minimal() + \n  scale_y_log10()\n\n\n\n\nWe can also look at where this pixel would be in the image:\n\n\nexpand.grid(x=1:16,y=1:16)[246,]\n\n\n    x  y\n246 6 16\n\nAnd plot it\n\n\nX = matrix(0,nrow=16,ncol=16)\nX[6,16] = 1\npheatmap(X,cluster_cols=FALSE,cluster_rows=FALSE)\n\n\n\n\nWe can also figure out which of the images are misclassified and look at them\n\n\nmissed = which(predict(mod,train_dat) != train_dat$type)\nmissed_imgs = train_dat[missed,] %>% \n  select(type,drawing) \n\n\n\nLet’s look at one of the missed images\n\n\nmissed_imgs  = missed_imgs %>%\n  filter(type==\"axe\") \n\nrjson::fromJSON(axes_json[[missed_imgs$drawing[1]]]) %>% \n    parse_drawing() %>% \n    ggplot(aes(x, y)) +\n    geom_path(aes(group = line), lwd=1)+\n    scale_x_continuous(limits=c(0, 255))+\n    scale_y_reverse(limits=c(255, 0))+\n    theme_minimal()\n\n\n\n\nThe first is not clear why we missed this? Maybe just because the model is too sparse? This would be something we’d explore more carefully.\nThe last step is to apply the predictions in the test set. You only do this once, but it gives you the best estimate of the out of sample error rate you’d see in practice.\n\n\nconfusionMatrix(factor(test_dat$type), predict(mod,test_dat))\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction axe cloud\n     axe    45     5\n     cloud  13    37\n                                          \n               Accuracy : 0.82            \n                 95% CI : (0.7305, 0.8897)\n    No Information Rate : 0.58            \n    P-Value [Acc > NIR] : 2.857e-07       \n                                          \n                  Kappa : 0.64            \n                                          \n Mcnemar's Test P-Value : 0.09896         \n                                          \n            Sensitivity : 0.7759          \n            Specificity : 0.8810          \n         Pos Pred Value : 0.9000          \n         Neg Pred Value : 0.7400          \n             Prevalence : 0.5800          \n         Detection Rate : 0.4500          \n   Detection Prevalence : 0.5000          \n      Balanced Accuracy : 0.8284          \n                                          \n       'Positive' Class : axe             \n                                          \n\nThis accuracy is usually slightly lower than the accuracy in the training data.\nPost-lecture materials\nAdditional Resources\n\nhttps://rafalab.github.io/dsbook/introduction-to-machine-learning.html\n\n\n\n\n",
    "preview": "posts/2021-10-19-introduction-to-machine-learning/../../images/cdi1.png",
    "last_modified": "2021-10-19T13:15:28-04:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 600
  },
  {
    "path": "posts/2021-10-14-python-for-r-users/",
    "title": "Python for R users",
    "description": "Introduction to using Python in R and the reticulate package",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-10-14",
    "categories": [
      "module 2",
      "week 7",
      "python",
      "reticulate",
      "R Markdown",
      "R",
      "programming"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nPython in R Markdown\npython path\nCalling Python\n\nPython basics\nstart python\nobjects in python\nvariables\noperators\nformat operators\nfunctions\nnew functions\niteration\nmethods for each type of object (dot notation)\nData structures\n\nreticulate\nPython engine within R Markdown\nimport python modules\nCalling python scripts\nCalling the python repl\n\n\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://rstudio.github.io/reticulate\nhttps://py-pkgs.org/02-setup\nThe Python Tutorial\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rstudio.github.io/reticulate\nhttps://github.com/bcaffo/ds4ph-bme/blob/master/python.md\nLearning objectives\n\nAt the end of this lesson you will:\nInstall python on your local machine\nLearn about the reticulate package to work interoperability between Python and R\nRecognize some basics in the python language\nBe able to translate between R and Python objects\n\nPython in R Markdown\nFor this lesson, we will be using the reticulate R package, which provides a set of tools for interoperability between Python and R. The package includes facilities for:\nCalling Python from R in a variety of ways including R Markdown, sourcing Python scripts, importing Python modules, and using Python interactively within an R session.\nTranslation between R and Python objects (for example, between R and Pandas data frames, or between R matrices and NumPy arrays).\n\n\n\nFigure 1: reticulate R package logo\n\n\n\n[Source: Rstudio]\n\nInstalling python: If you would like recommendations on installing python, I like this resource: https://py-pkgs.org/02-setup#installing-python\nWhat’s happening under the hood?: reticulate embeds a Python session within your R session, enabling seamless, high-performance interoperability.\nIf you are an R developer that uses Python for some of your work or a member of data science team that uses both languages, reticulate can make your life better!\n\nLet’s try it out. Before we get started, you will need to install the packages, if not already:\n\n\ninstall.package(\"reticulate\")\n\n\n\nWe will also load the here and tidyverse packages for our lesson:\n\n\nlibrary(here)\nlibrary(tidyverse)\nlibrary(reticulate)\n\n\n\npython path\nBy default, reticulate uses the version of Python found on your PATH\n\n\nSys.which(\"python3.9\")\n\n\npython3.9 \n       \"\" \n\nThe use_python() function enables you to specify an alternate version, for example:\n\n\nuse_python(\"/usr/<new>/<path>/local/bin/python\")\n\n\n\nFor example, I can define the path explicitly:\n\n\nuse_python(\"/Users/shicks/opt/miniconda3/bin/python3.9\", required = TRUE)\n\n\n\nCalling Python\nThere are a variety of ways to integrate Python code into your R projects:\nPython in R Markdown — A new Python language engine for R Markdown that supports bi-directional communication between R and Python (R chunks can access Python objects and vice-versa).\nImporting Python modules — The import() function enables you to import any Python module and call its functions directly from R.\nSourcing Python scripts — The source_python() function enables you to source a Python script the same way you would source() an R script (Python functions and objects defined within the script become directly available to the R session).\nPython REPL — The repl_python() function creates an interactive Python console within R. Objects you create within Python are available to your R session (and vice-versa).\nBelow I will focus on introducing the first and last one. However, before we do that, let’s introduce a bit about python basics.\nPython basics\nPython is a high-level, object-oriented programming language useful to know for anyone analyzing data. The most important thing to know before learning Python, is that in Python, everything is an object. There is no compiling and no need to define the type of variables before using them. No need to allocate memory for variables. The code is very easy to learn and easy to read (syntax).\nThere is a large scientific community contributing to Python. Some of the most widely used libraries in Python are numpy, scipy, pandas, and matplotlib.\nstart python\nThere are two modes you can write Python code in: interactive mode or script mode. If you open up a UNIX command window and have a command-line interface, you can simply type python (or python3) in the shell:\n\npython3\n\nand the interactive mode will open up. You can write code in the interactive mode and Python will interpret the code using the python interpreter.\nAnother way to pass code to Python is to store code in a file ending in .py, and execute the file in the script mode using\n\npython3 myscript.py\n\nTo check what version of Python you are using, type the following in the shell:\n\npython3 --version\n\nobjects in python\nEverything in Python is an object. Think of an object as a data structure that contains both data as well as functions. These objects can be variables, functions, and modules which are all objects. We can operate on this objects with what are called operators (e.g. addition, subtraction, concatenation or other operations), define/apply functions, test/apply for conditionals statements, (e.g. if, else statements) or iterate over the objects.\nNot all objects are required to have attributes and methods to operate on the objects in Python, but everything is an object (i.e. all objects can be assigned to a variable or passed as an argument to a function). A user can work with built-in defined classes of objects or can create new classes of objects. Using these objects, a user can perform operations on the objects by modifying / interacting with them.\nvariables\nVariable names are case sensitive, can contain numbers and letters, can contain underscores, cannot begin with a number, cannot contain illegal characters and cannot be one of the 31 keywords in Python:\n\n“and, as, assert, break, class, continue, def, del, elif, else, except, exec, finally, for, from, global, if, import, in, is, lambda, not, or, pass, print, raise, return, try, while, with, yield”\n\noperators\nNumeric operators are +, -, *, /, ** (exponent), % (modulus if applied to integers)\nString and list operators: + and * .\nAssignment operator: =\nThe augmented assignment operator += (or -=) can be used like n += x which is equal to n = n + x\nBoolean relational operators: == (equal), != (not equal), >, <, >= (greater than or equal to), <= (less than or equal to)\nBoolean expressions will produce True or False\nLogical operators: and, or, and not. e.g. x > 1 and x <= 5\n\n2 ** 3\n8\nx = 3 \nx > 1 and x <= 5\nTrue\n\nformat operators\nIf % is applied to strings, this operator is the format operator. It tells Python how to format a list of values in a string. For example,\n%d says to format the value as an integer\n%g says to format the value as an float\n%s says to format the value as an string\n\nprint('In %d days, I have eaten %g %s.' % (5, 3.5, 'crabs'))\nIn 5 days, I have eaten 3.5 crabs.\n\nfunctions\nPython contains a small list of very useful built-in functions. All other functions need defined by the user or need to be imported from modules. For a more detailed list on the built-in functions in Python, see Built-in Python Functions.\nThe first function we will discuss, type(), reports the type of any object, which is very useful when handling multiple data types (remember, everything in Python is an object). Here are some the mains types you will encounter:\ninteger (int)\nfloating-point (float)\nstring (str)\nlist (list)\ndictionary (dict)\ntuple (tuple)\nfunction (function)\nmodule (module)\nboolean (bool): e.g. True, False\nenumerate (enumerate)\nIf we asked for the type of a string “Let’s go Ravens!”\n\ntype(\"Let's go Ravens!\")\n<class 'str'>\n\nThis would return the str type.\nYou have also seen how to use the print() function. The function print will accept an argument and print the argument to the screen. Print can be used in two ways:\n\n\nprint(\"Let's go Ravens!\")\n\n\n[1] \"Let's go Ravens!\"\n\nnew functions\nNew functions can be defined using one of the 31 keywords in Python def.\n\ndef new_world(): \n    return 'Hello world!'\n    \nprint(new_world())\nHello world!\n\nThe first line of the function (the header) must start with def, the name of the function (which can contain underscores), parentheses (with any arguments inside of it) and a colon. The arguments can be specified in any order.\nThe rest of the function (the body) always has an indentation of four spaces. If you define a function in the interactive mode, the interpreter will print ellipses (…) to let you know the function is not complete. To complete the function, enter an empty line (not necessary in a script).\nTo return a value from a function, use return. The function will immediately terminate and not run any code written past this point.\n\ndef squared(x):\n    \"\"\" Return the square of a  \n        value \"\"\"\n    return x ** 2\n\nprint(squared(4))\n16\n\n\nNote: python has its version of ... (also from docs.python.org)\n\ndef concat(*args, sep=\"/\"):\n return sep.join(args)  \n\nconcat(\"a\", \"b\", \"c\")\n'a/b/c'\n\n\niteration\nIterative loops can be written with the for, while and break statements.\nDefining a for loop is similar to defining a new function. The header ends with a colon and the body is indented. The function range(n) takes in an integer n and creates a set of values from 0 to n - 1. for loops are not just for counters, but they can iterate through many types of objects such as strings, lists and dictionaries.\n\nfor i in range(3):\n  print('Baby shark, doo doo doo doo doo doo!')\nBaby shark, doo doo doo doo doo doo!\nBaby shark, doo doo doo doo doo doo!\nBaby shark, doo doo doo doo doo doo!\nprint('Baby shark!')\nBaby shark!\n\nThe function len() can be used to:\nCalculate the length of a string\nCalculate the number of elements in a list\nCalculate the number of items (key-value pairs) in a dictionary\nCalculate the number elements in the tuple\n\nx = 'Baby shark!'\nlen(x)\n11\n\nmethods for each type of object (dot notation)\nFor strings, lists and dictionaries, there are set of methods you can use to manipulate the objects. In general, the notation for methods is the dot notation. The syntax is the name of the objects followed by a dot (or period) followed by the name of the method.\n\nx = \"Hello Baltimore!\"\nx.split()\n['Hello', 'Baltimore!']\n\nData structures\nWe have already seen lists. Python has other data structures built in.\nSets {\"a\", \"a\", \"a\", \"b\"} (unique elements)\nTuples (1, 2, 3) (a lot like lists but not mutable, i.e. need to create a new to modify)\nDictionaries\n\ndict = {\"a\" : 1, \"b\" : 2}\ndict['a']\n1\ndict['b']\n2\n\nMore about data structures can be founds at the python docs\nreticulate\nPython engine within R Markdown\nThe reticulate package includes a Python engine for R Markdown with the following features:\nRun Python chunks in a single Python session embedded within your R session (shared variables/state between Python chunks)\nPrinting of Python output, including graphical output from matplotlib.\nAccess to objects created within Python chunks from R using the py object (e.g. py$x would access an x variable created within Python from R).\nAccess to objects created within R chunks from Python using the r object (e.g. r.x would access to x variable created within R from Python)\nBuilt in conversion for many Python object types is provided, including NumPy arrays and Pandas data frames.\nFrom Python to R\nAs an example, you can use Pandas to read and manipulate data then easily plot the Pandas data frame using ggplot2:\nLet’s first create a flights.csv dataset in R:\n\n\nif(!file.exists(here(\"data\", \"flights.csv\"))){\n  readr::write_csv(nycflights13::flights, \n                   file = here(\"data\", \"flights.csv\"))\n}\n\n\n\nUse Python to read in the file and do some data wrangling\n\nimport pandas\nflights_path = \"/Users/shicks/Documents/github/teaching/jhustatcomputing2021/data/flights.csv\"\nflights = pandas.read_csv(flights_path)\nflights = flights[flights['dest'] == \"ORD\"]\nflights = flights[['carrier', 'dep_delay', 'arr_delay']]\nflights = flights.dropna()\nflights\n       carrier  dep_delay  arr_delay\n5           UA       -4.0       12.0\n9           AA       -2.0        8.0\n25          MQ        8.0       32.0\n38          AA       -1.0       14.0\n57          AA       -4.0        4.0\n...        ...        ...        ...\n336645      AA      -12.0      -37.0\n336669      UA       -7.0      -13.0\n336675      MQ       -7.0      -11.0\n336696      B6       -5.0      -23.0\n336709      AA      -13.0      -38.0\n\n[16566 rows x 3 columns]\n\n\n\nhead(py$flights)\n\n\n   carrier dep_delay arr_delay\n5       UA        -4        12\n9       AA        -2         8\n25      MQ         8        32\n38      AA        -1        14\n57      AA        -4         4\n70      UA         9        20\n\npy$flights_path \n\n\n[1] \"/Users/shicks/Documents/github/teaching/jhustatcomputing2021/data/flights.csv\"\n\n\n\nclass(py$flights)\n\n\n[1] \"data.frame\"\n\nclass(py$flights_path)\n\n\n[1] \"character\"\n\nNext, we can use R to visualize the Pandas DataFrame. The data frame is loaded in as an R object now stored in the variable py.\n\n\nggplot(py$flights, aes(x = carrier, y = arr_delay)) + \n  geom_point() + \n  geom_jitter()\n\n\n\n\nNote that the reticulate Python engine is enabled by default within R Markdown whenever reticulate is installed.\nFrom R to Python\nUse R to read and manipulate data\n\n\nlibrary(tidyverse)\nflights <- read_csv(here(\"data\",\"flights.csv\")) %>%\n  filter(dest == \"ORD\") %>%\n  select(carrier, dep_delay, arr_delay) %>%\n  na.omit()\n\nflights\n\n\n# A tibble: 16,566 × 3\n   carrier dep_delay arr_delay\n   <chr>       <dbl>     <dbl>\n 1 UA             -4        12\n 2 AA             -2         8\n 3 MQ              8        32\n 4 AA             -1        14\n 5 AA             -4         4\n 6 UA              9        20\n 7 UA              2        21\n 8 AA             -6       -12\n 9 MQ             39        49\n10 B6             -2        15\n# … with 16,556 more rows\n\nUse Python to print R dataframe\nIf you recall, we can access objects created within R chunks from Python using the r object (e.g. r.x would access to x variable created within R from Python). We can then ask for the first ten rows using the head() function in python.\n\nr.flights.head(10)\n  carrier  dep_delay  arr_delay\n0      UA       -4.0       12.0\n1      AA       -2.0        8.0\n2      MQ        8.0       32.0\n3      AA       -1.0       14.0\n4      AA       -4.0        4.0\n5      UA        9.0       20.0\n6      UA        2.0       21.0\n7      AA       -6.0      -12.0\n8      MQ       39.0       49.0\n9      B6       -2.0       15.0\n\nimport python modules\nYou can use the import() function to import any Python module and call it from R. For example, this code imports the Python os module in python and calls the listdir() function:\n\n\nos <- import(\"os\")\nos$listdir(\".\")\n\n\n[1] \"python-for-r-users_files\" \"python-for-r-users.Rmd\"  \n[3] \"python-for-r-users.html\" \n\nFunctions and other data within Python modules and classes can be accessed via the $ operator (analogous to the way you would interact with an R list, environment, or reference class).\nImported Python modules support code completion and inline help:\n\n\n\nFigure 2: Using reticulate tab completion\n\n\n\n[Source: Rstudio]\nSimilarly, we can import the pandas library:\n\n\npd <- import('pandas')\ntest <- pd$read_csv(here(\"data\",\"flights.csv\"))\nhead(test)\n\n\n  year month day dep_time sched_dep_time dep_delay arr_time\n1 2013     1   1      517            515         2      830\n2 2013     1   1      533            529         4      850\n3 2013     1   1      542            540         2      923\n4 2013     1   1      544            545        -1     1004\n5 2013     1   1      554            600        -6      812\n6 2013     1   1      554            558        -4      740\n  sched_arr_time arr_delay carrier flight tailnum origin dest\n1            819        11      UA   1545  N14228    EWR  IAH\n2            830        20      UA   1714  N24211    LGA  IAH\n3            850        33      AA   1141  N619AA    JFK  MIA\n4           1022       -18      B6    725  N804JB    JFK  BQN\n5            837       -25      DL    461  N668DN    LGA  ATL\n6            728        12      UA   1696  N39463    EWR  ORD\n  air_time distance hour minute            time_hour\n1      227     1400    5     15 2013-01-01T10:00:00Z\n2      227     1416    5     29 2013-01-01T10:00:00Z\n3      160     1089    5     40 2013-01-01T10:00:00Z\n4      183     1576    5     45 2013-01-01T10:00:00Z\n5      116      762    6      0 2013-01-01T11:00:00Z\n6      150      719    5     58 2013-01-01T10:00:00Z\n\nclass(test)\n\n\n[1] \"data.frame\"\n\nor the scikit-learn python library:\n\n\nskl_lr <- import(\"sklearn.linear_model\")\n\n\n\nCalling python scripts\n\n\nsource_python(\"secret_functions.py\")\nsubject_1 <- read_subject(\"secret_data.csv\")\n\n\n\nCalling the python repl\nIf you want to work with Python interactively you can call the repl_python() function, which provides a Python REPL embedded within your R session.\n\n\nrepl_python()\n\n\n\nObjects created within the Python REPL can be accessed from R using the py object exported from reticulate. For example:\n\n\n\nFigure 3: Using the repl_python() function\n\n\n\n[Source: Rstudio]\ni.e. objects do have permenancy in R after exiting the python repl.\nSo typing x = 4 in the repl will put py$x as 4 in R after you exit the repl.\nEnter exit within the Python REPL to return to the R prompt.\n\n\n\n",
    "preview": "https://rstudio.github.io/reticulate/images/reticulated_python.png",
    "last_modified": "2021-10-16T21:13:40-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-12-dealing-with-large-data/",
    "title": "Strategies to deal with large data in R",
    "description": "Introduction to dealing with large data in R.",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-10-12",
    "categories": [
      "module 2",
      "week 7",
      "large data",
      "programming",
      "R",
      "tidyverse",
      "SQL"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nAcknowledgements\n\nLearning objectives\nIntroduction\nData\nSQLite databases\n\nSample and Model\nAdvantages\nDisadvantages\nExample\n\nChunk and Pull\nAdvantages\nDisadvantages\nExample\n\nPush Compute to Data\nAdvantages\nDisadvantages\nExample\n\nSummary\n\n\nPre-lecture materials\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://jhu-advdatasci.github.io/2019/lectures/29-dealing-with-large-data-in-R.html\nA great blog post by Alex Gold from RStudio\nLearning objectives\n\nAt the end of this lesson you will:\nRecognize different file formats to work with large data not locally\nImplement three ways to work with large data: “sample and model”, “chunk and pull”, and “push compute to data”\n\nIntroduction\nFirst, we load a few R packages\n\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(dbplyr)\nlibrary(rsample)\n\n\n\nFor most data analyses in R, data you encounter can easily be read into memory in R (either locally or on a cluster of sorts) and analyzed in a standard way. However, if you do encounter data that is too big to be read into memory, you might start to search for strategies on how to deal with this data. For most of people, it might be obvious why you would want to use R with big data, but it not obvious how.\nNow, you might say advances in hardware make this less and less of a problem as most laptops come with >4-16Gb of memory and it is easy to get instances on cloud providers with terabytes of RAM.\nThat’s definitely true. But there might be some problems that you will run into.\nLoading data into memory\nLet’s say you are able load part of the data into the RAM on your machine (in-memory).\nIf you had something like a zipped .csv file, you could always try loading just the first few lines into memory (see n_max = 8 below) to see what is inside the files, but eventually you will likely need a different strategy.\n\n\nread_csv(readr_example(\"mtcars.csv.bz2\"), \n         skip = 0, n_max = 8, progress = show_progress())\n\n\n# A tibble: 8 × 11\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n\nMemory for calculations\nYou have to keep in mind that you will need to do something with the data too (typically need 2-3 times the RAM of the size of your data. This may or may not be a problem for your hardware that you are working with.\nTransfer speeds can be slow\nIf you are working with data on a server that needs to be transferred somewhere to do the processing or computation once the data has been transferred.\nFor example, the time it takes to make a call over the internet from San Francisco to New York City takes over 4 times longer than reading from a standard hard drive and over 200 times longer than reading from a solid state hard drive.\n\n\n\nThis is an especially big problem early in developing a model or performing a data analysis, when data might have to be pulled repeatedly.\n[image source]\nToday we are going to discuss some strategies (and R packages) for working with big data in R. We will also go through some examples of how to execute these strategies in R.\nData\nWe will use the nycflights13 data that we learned about previously.\nWhat’s in the data package?\n\n“This package contains information about all flights that departed from NYC (e.g. EWR, JFK and LGA) to destinations in the United States, Puerto Rico, and the American Virgin Islands) in 2013: 336,776 flights in total. To help understand what causes delays, it also includes a number of other useful datasets.”\n\nThis package provides the following data tables.\nflights: all flights that departed from NYC in 2013\nweather: hourly meterological data for each airport\nplanes: construction information about each plane\nairports: airport names and locations\nairlines: translation between two letter carrier codes and names\nHowever, this time we will cache the data from the nycflights13 package in a form we are already familiar with (SQLite databases). But there are many other data formats that you might encounter including:\n.sqlite (SQL database). Talk more about this in a bit.\n.csv (comma separated values). Good for storing rectangular data. However, can really slow to read and write, making them (often) unusable for large datasets.\n.json (JavaScript object notation). Key-value pairs in a partially structured format\n.parquet (Apache Parquet). Developed by Cloudera and Twitter to serve as a column-based storage format, optimized for work with multi-column datasets. Can be used for Spark data or other tools in the Hadoop ecosystem. When you store data in parquet format, you actually get a whole directory worth of files. The data is split across multiple .parquet files, allowing it to be easily stored on multiple machines, and there are some metadata files too, describing the contents of each column. Can use sparklyr to import .parquet files\n.avro (Apache Avro). Released by the Hadoop working group in 2009. It is a row-based format that is highly splittable. It is also described as a data serialization system similar to Java Serialization. The schema is stored in JSON format, while the data is stored in binary format, minimizing file size and maximizing efficiency. Can use sparkavro to import .avro files.\n.zarr (Zarr). Zarr files are a modern library and data format for storing chunked, compressed N-dimensional data in Python, but can work with these files using reticulate. Still very much in development though.\n.h5 (Hierarchical Data Format or HDF5). Mature (20 years old) library and data format which is also designed to handle chunked compressed N-dimensional data. Can use rhdf5 and HDF5Array to read and write .h5 files.\nSQLite databases\nOK so as mentioned above, let’s use the SQLite format to demonstrate the strategies for dealing with large data. However, they can easily transfer other data formats.\nReminder: There are several ways to query SQL or SQLite databases in R.\nOk, we will set up the SQLite database using the nycflights13_sqlite() function in the dbplyr package.\n\n\nlibrary(nycflights13)\nif(!file.exists(here(\"data\", \"nycflights13\", \"nycflights13.sqlite\"))){\n  dir.create(here(\"data\", \"nycflights13\"))\n  dbplyr::nycflights13_sqlite(path=here(\"data\", \"nycflights13\"))\n}\n\n\n\nWe can check to see what file has been created\n\n\nlist.files(here(\"data\", \"nycflights13\"))\n\n\n[1] \"nycflights13.sqlite\"\n\n\nQuestion: How can we use the DBI::dbConnect() function with RSQLite::SQLite() backend to connect to the SQLite database?\n\n\nlibrary(DBI)\n# try it yourself \n\n\n\n\nClick here for the answer.\n\n\nlibrary(DBI)\nconn <- DBI::dbConnect(RSQLite::SQLite(), \n                       here(\"data\", \"nycflights13\", \"nycflights13.sqlite\"))\nconn\n\n\n<SQLiteConnection>\n  Path: /Users/shicks/Documents/github/teaching/jhustatcomputing2021/data/nycflights13/nycflights13.sqlite\n  Extensions: TRUE\n\n\n\nQuestion: Next, let’s use the dplyr::tbl() function returns something that feels like a data frame with the flights dataset. Finally, show the first 10 rows of the data frame.\n\n\n# try it yourself \n\n\n\n\nClick here for the answer.\n\n\ntbl(conn, \"flights\") %>%\n  head(n=10)\n\n\n# Source:   lazy query [?? x 19]\n# Database: sqlite 3.36.0\n#   [/Users/shicks/Documents/github/teaching/jhustatcomputing2021/data/nycflights13/nycflights13.sqlite]\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     1     1      517            515         2      830\n 2  2013     1     1      533            529         4      850\n 3  2013     1     1      542            540         2      923\n 4  2013     1     1      544            545        -1     1004\n 5  2013     1     1      554            600        -6      812\n 6  2013     1     1      554            558        -4      740\n 7  2013     1     1      555            600        -5      913\n 8  2013     1     1      557            600        -3      709\n 9  2013     1     1      557            600        -3      838\n10  2013     1     1      558            600        -2      753\n# … with 12 more variables: sched_arr_time <int>, arr_delay <dbl>,\n#   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>,\n#   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dbl>\n\n\nBefore we jump into the next section, let’s save this data frame as flights_df and count the number of rows using dplyr::tally():\n\n\nflights_df <- dplyr::tbl(conn, \"flights\")\nflights_df %>% \n  tally()\n\n\n# Source:   lazy query [?? x 1]\n# Database: sqlite 3.36.0\n#   [/Users/shicks/Documents/github/teaching/jhustatcomputing2021/data/nycflights13/nycflights13.sqlite]\n       n\n   <int>\n1 336776\n\nEven though it only has a few hundred thousand rows, it is still useful to demonstrate some strategies for dealing with big data in R.\nSample and Model\nThe first strategy is to downsample your data to a size that can be downloaded (or if already downloaded, just loaded into memory) and perform your analysis on the downsampled data. This also allows models and methods to be run in a reasonable amount of time.\n\n\n\n[image source]\n\nNote: If maintaining class balance is necessary (or one class needs to be over/under-sampled), it is reasonably simple to stratify the data set during sampling.\n\nAdvantages\nSpeed. Relative to working on your entire data set, working on just a sample can drastically decrease run times and increase iteration speed.\nPrototyping. Even if you will eventually have to run your model on the entire data set, this can be a good way to refine hyperparameters and do feature engineering for your model.\nPackages. Since you are working on a regular, in-memory data set, you can use all your favorite R packages.\nDisadvantages\nSampling. Downsampling is not terribly difficult, but does need to be done with care to ensure that the sample is valid and that you have pulled enough points from the original data set.\nScaling. If you are using sample and model to prototype something that will later be run on the full data set, you will need to have a strategy (such as pushing compute to the data) for scaling your prototype version back to the full data set.\nTotals. Business Intelligence (BI) – or strategies and technologies used by enterprises for the data analysis of business information (e.g. data mining, reporting, predictive analytics, etc) – tasks frequently answer questions about totals, like the count of all sales in a month. One of the other strategies is usually a better fit in this case.\nExample\nLet’s say we want to model whether flights will be delayed or not. We will start with some minor cleaning of the data.\nFirst, we will create a is_delayed column in the database:\n\n\nflights_df <- flights_df %>%\n    dplyr::mutate(is_delayed = arr_delay > 0,\n                  hour = sched_dep_time / 100) %>% # Get just hour (currently formatted so 6 pm = 1800)\n  # Remove small carriers that make modeling difficult\n  dplyr::filter(!is.na(is_delayed) & !carrier %in% c(\"OO\", \"HA\"))\n\n\n\nHere are the total number of flights that were delayed or not:\n\n\nflights_df %>% \n  dplyr::count(is_delayed)\n\n\n# Source:   lazy query [?? x 2]\n# Database: sqlite 3.36.0\n#   [/Users/shicks/Documents/github/teaching/jhustatcomputing2021/data/nycflights13/nycflights13.sqlite]\n  is_delayed      n\n       <int>  <int>\n1          0 194078\n2          1 132897\n\nThese classes are reasonably well balanced, but we going to use logistic regression, so I will load a perfectly balanced sample of 40,000 data points.\nFor most databases, random sampling methods do not work smoothly with R.\n\n\nflights_df %>% \n  dplyr::sample_n(size = 1000)\n\n\nError: `tbl` must be a data frame, not a `tbl_SQLiteConnection/tbl_dbi/tbl_sql/tbl_lazy/tbl` object.\n\nSo it is not suggested to use dplyr::sample_n() or dplyr::sample_frac(). So we will have to be a little more manual.\n\n\nset.seed(1234)\n\n# Create a modeling data set \ndf_mod <- flights_df %>%\n  # Within each class\n  group_by(is_delayed) %>%\n  # Assign random rank\n  mutate(x = random() %>% row_number()) %>%\n  ungroup()\n\n\n\n\nNote: dplyr::collect() forces a computation of a database query and retrieves data into a local tibble\nSo, here, we take the first 20K for each class for training set:\n\n\ndf_train <- df_mod %>%\n  group_by(is_delayed) %>%\n  filter(x <= 20000) %>%\n  collect() \n\n\n\n\nThen, we take next 5K for test set:\n\n\ndf_test <- df_mod %>%\n  group_by(is_delayed) %>%\n  filter(x > 20000 & x <= 25000) %>%\n  collect() # again, this data is now loaded locally\n\n\n\n\n\n# Double check I sampled right\ncount(df_train, is_delayed)\n\n\n# A tibble: 2 × 2\n# Groups:   is_delayed [2]\n  is_delayed     n\n       <int> <int>\n1          0 20000\n2          1 20000\n\ncount(df_test, is_delayed)\n\n\n# A tibble: 2 × 2\n# Groups:   is_delayed [2]\n  is_delayed     n\n       <int> <int>\n1          0  5000\n2          1  5000\n\nNow let’s build a model – let’s see if we can predict whether there will be a delay or not by the combination of the carrier, and the month of the flight.\n\n\nSys.time()\n\n\n[1] \"2021-10-11 22:34:00 EDT\"\n\nmod <- glm(is_delayed ~ carrier + as.factor(month),\n           family = \"binomial\", data = df_train)\nSys.time()\n\n\n[1] \"2021-10-11 22:34:01 EDT\"\n\n\n\nsummary(mod)\n\n\n\nCall:\nglm(formula = is_delayed ~ carrier + as.factor(month), family = \"binomial\", \n    data = df_train)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-1.74540  -1.14986  -0.04264   1.13010   1.61318  \n\nCoefficients:\n                   Estimate Std. Error z value Pr(>|z|)    \n(Intercept)        -0.11861    0.05556  -2.135 0.032759 *  \ncarrierAA          -0.10929    0.05494  -1.989 0.046678 *  \ncarrierAS          -0.16211    0.21976  -0.738 0.460723    \ncarrierB6           0.28459    0.05055   5.630 1.80e-08 ***\ncarrierDL          -0.13092    0.05175  -2.530 0.011405 *  \ncarrierEV           0.54128    0.05099  10.616  < 2e-16 ***\ncarrierF9           1.02775    0.25953   3.960 7.49e-05 ***\ncarrierFL           0.92384    0.11058   8.355  < 2e-16 ***\ncarrierMQ           0.48352    0.05702   8.480  < 2e-16 ***\ncarrierUA           0.10446    0.05034   2.075 0.037989 *  \ncarrierUS           0.05058    0.06053   0.836 0.403343    \ncarrierVX          -0.04523    0.09541  -0.474 0.635491    \ncarrierWN           0.40543    0.06898   5.878 4.16e-09 ***\ncarrierYV           0.61160    0.27411   2.231 0.025667 *  \nas.factor(month)2   0.01977    0.05180   0.382 0.702633    \nas.factor(month)3  -0.17260    0.04971  -3.472 0.000516 ***\nas.factor(month)4   0.12777    0.04954   2.579 0.009901 ** \nas.factor(month)5  -0.25054    0.04983  -5.027 4.97e-07 ***\nas.factor(month)6   0.18434    0.04954   3.721 0.000198 ***\nas.factor(month)7   0.23093    0.04889   4.723 2.32e-06 ***\nas.factor(month)8  -0.13891    0.04934  -2.815 0.004874 ** \nas.factor(month)9  -0.73390    0.05214 -14.077  < 2e-16 ***\nas.factor(month)10 -0.32838    0.04946  -6.639 3.16e-11 ***\nas.factor(month)11 -0.28839    0.05022  -5.743 9.33e-09 ***\nas.factor(month)12  0.36816    0.04956   7.429 1.09e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 55452  on 39999  degrees of freedom\nResidual deviance: 54054  on 39975  degrees of freedom\nAIC: 54104\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n# Out-of-Sample AUROC\ndf_test$pred <- predict(mod, newdata = df_test)\nauc <- suppressMessages(pROC::auc(df_test$is_delayed, df_test$pred))\nauc\n\n\nArea under the curve: 0.6025\n\nAs you can see, this is not a great model, but that’s not the point here!\nInstead, we showed how to build a model on a small subset of a big data set. Including sampling time, this took my laptop a second to run, making it easy to iterate quickly as I want to improve the model. After I’m happy with this model, I could pull down a larger sample or even the entire data set if it is feasible, or do something with the model from the sample.\nChunk and Pull\nA second strategy to chunk the data into separable units and each chunk is pulled separately and operated on serially, in parallel, or after recombining. This strategy is conceptually similar to the MapReduce algorithm – or MapReduce is a framework using which we can write applications to process huge amounts of data, in parallel, on large clusters in a reliable manner – more here on MapReduce. Depending on the task at hand, the chunks might be time periods, geographic units, or logical like separate businesses, departments, products, or customer segments.\n\n\n\n[image source]\nAdvantages\nFull data set. The entire data set gets used.\nParallelization. If the chunks are run separately, the problem is easy to treat as embarassingly parallel and make use of parallelization to speed runtimes.\nDisadvantages\nNeed Chunks. Your data needs to have separable chunks for chunk and pull to be appropriate.\nPull All Data. Eventually have to pull in all data, which may still be very time and memory intensive.\nStale Data. The data may require periodic refreshes from the database to stay up-to-date since you’re saving a version on your local machine.\nExample\nIn this case, I want to build another model of on-time arrival, but I want to do it per-carrier. This is exactly the kind of use case that is ideal for chunk and pull.\nI am going to separately pull the data in by carrier and run the model on each carrier’s data.\nI am going to start by just getting the complete list of the carriers.\n\n\n# Get all unique carriers\ncarriers <- flights_df %>% \n  select(carrier) %>% \n  distinct() %>% \n  pull(carrier)\n\ncarriers\n\n\n [1] \"9E\" \"AA\" \"AS\" \"B6\" \"DL\" \"EV\" \"F9\" \"FL\" \"MQ\" \"UA\" \"US\" \"VX\" \"WN\"\n[14] \"YV\"\n\nNow, I will write a function that\ntakes the name of a carrier as input\npulls the data for that carrier into R\nsplits the data into training and test\ntrains the model\noutputs the out-of-sample AUROC (a common measure of model quality)\n\n\ncarrier_model <- function(carrier_name) {\n  # Pull a chunk of data\n  df_mod <- flights_df %>%\n    filter(carrier == carrier_name) %>%\n    collect()\n  \n  # Split into training and test\n  split <- df_mod %>%\n    rsample::initial_split(prop = 0.9, strata = \"is_delayed\") %>% \n    suppressMessages()\n  \n  # Get training data\n  df_train <- split %>% \n                rsample::training()\n  \n  # Train model\n  mod <- glm(is_delayed ~ as.factor(month),\n             family = \"binomial\", data = df_train)\n  \n  # Get out-of-sample AUROC\n  df_test <- split %>% \n                rsample::testing()\n  df_test$pred <- predict(mod, newdata = df_test)\n  suppressMessages(auc <- pROC::auc(df_test$is_delayed ~ df_test$pred))\n  \n  auc\n}\n\n\n\nNow, I am going to actually run the carrier model function across each of the carriers. This code runs pretty quickly, and so I do not think the overhead of parallelization would be worth it.\n\n\nset.seed(1234)\nmods <- lapply(carriers, carrier_model) %>%\n  suppressMessages()\n\nnames(mods) <- carriers\n\n\n\nLet’s look at the results.\n\n\nmods\n\n\n$`9E`\nArea under the curve: 0.5711\n\n$AA\nArea under the curve: 0.5731\n\n$AS\nArea under the curve: 0.5597\n\n$B6\nArea under the curve: 0.6208\n\n$DL\nArea under the curve: 0.5817\n\n$EV\nArea under the curve: 0.588\n\n$F9\nArea under the curve: 0.5134\n\n$FL\nArea under the curve: 0.5508\n\n$MQ\nArea under the curve: 0.572\n\n$UA\nArea under the curve: 0.6046\n\n$US\nArea under the curve: 0.5811\n\n$VX\nArea under the curve: 0.67\n\n$WN\nArea under the curve: 0.5607\n\n$YV\nArea under the curve: 0.6041\n\nSo these models (again) are a little better than random chance. The point was that we utilized the chunk and pull strategy to pull the data separately by logical units and building a model on each chunk.\nPush Compute to Data\nA third strategy is push some of the computing to where the data are stored before moving a subset of the data out of wherever it is stored and into R. Imagine the data is compressed on a database somwhere. It is often possible to obtain significant speedups simply by doing summarization or filtering in the database before pulling the data into R.\nSometimes, more complex operations are also possible, including computing histogram and raster maps with dbplot, building a model with modeldb, and generating predictions from machine learning models with tidypredict.\n\n\n\n[image source]\nAdvantages\nUse the Database. Takes advantage of what databases are often best at: quickly summarizing and filtering data based on a query.\nMore Info, Less Transfer. By compressing before pulling data back to R, the entire data set gets used, but transfer times are far less than moving the entire data set.\nDisadvantages\nDatabase Operations. Depending on what database you are using, some operations might not be supported.\nDatabase Speed. In some contexts, the limiting factor for data analysis is the speed of the database itself, and so pushing more work onto the database is the last thing analysts want to do.\nExample\nIn this case, I am doing a pretty simple BI task - plotting the proportion of flights that are late by the hour of departure and the airline.\nJust by way of comparison, let’s run this first the naive way -– pulling all the data to my system and then doing my data manipulation to plot.\n\n\nsystem.time(\n  df_plot <- flights_df %>%\n    collect() %>%\n    group_by(carrier, sched_dep_time) %>%\n    # Get proportion per carrier-time\n    summarize(delay_pct = mean(is_delayed, na.rm = TRUE)) %>%\n    ungroup() %>%\n    # Change string times into actual times\n    dplyr::mutate(sched_dep_time = \n                    stringr::str_pad(sched_dep_time, 4, \"left\", \"0\") %>% \n             strptime(\"%H%M\") %>%  # converts character class into POSIXlt class\n             as.POSIXct()) # converts POSIXlt class to POSIXct class\n  ) -> timing1\n\ntiming1\n\n\n   user  system elapsed \n  1.972   0.064   2.066 \n\nNow that wasn’t too bad, just 2.066 seconds on my laptop.\nBut let’s see how much of a speedup we can get from chunk and pull. The conceptual change here is significant - I’m doing as much work as possible in the SQLite server now instead of locally. But using dplyr means that the code change is minimal. The only difference in the code is that the collect() call got moved down by a few lines (to below ungroup()).\n\n\nsystem.time(\n  df_plot <- flights_df %>%\n    dplyr::group_by(carrier, sched_dep_time) %>%\n    # Get proportion per carrier-time\n    dplyr::summarize(delay_pct = mean(is_delayed, na.rm = TRUE)) %>%\n    dplyr::ungroup() %>%\n    dplyr::collect() %>%\n    # Change string times into actual times\n    dplyr::mutate(sched_dep_time = \n                    stringr::str_pad(sched_dep_time, 4, \"left\", \"0\") %>% \n             strptime(\"%H%M\") %>% \n             as.POSIXct())) -> timing2\n\ntiming2\n\n\n   user  system elapsed \n  0.431   0.089   0.531 \n\nIt might have taken you the same time to read this code as the last chunk, but this took only 0.531 seconds to run, almost an order of magnitude faster! That’s pretty good for just moving one line of code.\nNow that we have done a speed comparison, we can create the nice plot we all came for.\n\n\ndf_plot %>%\n  dplyr::mutate(carrier = paste0(\"Carrier: \", carrier)) %>%\n  ggplot(aes(x = sched_dep_time, y = delay_pct)) +\n    geom_line() +\n    facet_wrap(\"carrier\") +\n    ylab(\"Proportion of Flights Delayed\") +\n    xlab(\"Time of Day\") +\n    scale_y_continuous(labels = scales::percent) +\n    scale_x_datetime(date_breaks = \"4 hours\", \n                    date_labels = \"%H\")\n\n\n\n\nIt looks to me like flights later in the day might be a little more likely to experience delays, which we saw in our last class with this data. However, here we have learned how to work with data not necessarily loaded in memory.\nSummary\nThere are lots of ways you can work with large data in R. A few that we learned about today include\nSample and model\nChunk and pull\nPush compute to data\nHopefully this will help the next time you encounter a large dataset in R.\n\n\n\n",
    "preview": "https://blog.codinghorror.com/content/images/2014/May/internet-latency-usa.png",
    "last_modified": "2021-10-11T22:34:14-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-12-sql-basics/",
    "title": "SQL Basics",
    "description": "Introduction to SQL basics in R",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-10-12",
    "categories": [
      "module 2",
      "week 7",
      "large data",
      "programming",
      "R",
      "tidyverse",
      "SQL"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nRelational databases\nSQL basics\nReading SQL data\nConnect to the SQL database\nQuerying with dplyr syntax\n\ndata viz\nPost-lecture materials\nAdditional Resources\n\n\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://dbi.r-dbi.org\nhttps://db.rstudio.com/databases/sqlite/\nhttps://dbplyr.tidyverse.org\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://jhu-advdatasci.github.io/2019/lectures/04-gettingdata-api.html#reading_in_from_sqlite_database\nLearning objectives\n\nAt the end of this lesson you will:\nUnderstand what are relational databases with SQL as an example\nLearn about the DBI, RSQLite, dbplyr packages for interacting with SQL databses\n\nBefore we get started, you will need to install these packages, if not already:\n\n\ninstall.package(\"dbplyr\") # installed with the tidyverse\ninstall.package(\"RSQLite\") # note: the 'DBI' package is installed here\n\n\n\nWe will also load the tidyverse for our lesson:\n\n\nlibrary(tidyverse)\n\n\n\nRelational databases\nData live anywhere and everywhere. Data might be stored simply in a .csv or .txt file.\nData might be stored in an Excel or Google Spreadsheet. Data might be stored in large databases that require users to write special functions to interact with to extract the data they are interested in.\nA relational database is a digital database based on the relational model of data, as proposed by E. F. Codd in 1970.\n\n\n\nFigure 1: Relational model concepts\n\n\n\n[Source: Wikipedia]\nA system used to maintain relational databases is a relational database management system (RDBMS). Many relational database systems have an option of using the SQL (Structured Query Language) (or SQLite – very similar to SQL) for querying and maintaining the database.\nSQL basics\nReading SQL data\nThere are several ways to query databases in R.\nFirst, we will download a .sqlite database. This is a portable version of a SQL database. For our purposes, we will use the chinook sqlite database here. The database represents a “digital media store, including tables for artists, albums, media tracks, invoices and customers”.\nFrom the Readme.md file:\n\nSample Data\nMedia related data was created using real data from an iTunes Library. … Customer and employee information was manually created using fictitious names, addresses that can be located on Google maps, and other well formatted data (phone, fax, email, etc.). Sales information is auto generated using random data for a four year period.\n\nHere we download the data to our data folder:\n\n\nlibrary(here)\nif(!file.exists(here(\"data\", \"Chinook.sqlite\"))){\n  file_url <- paste0(\"https://github.com/lerocha/chinook-database/raw/master/ChinookDatabase/DataSources/Chinook_Sqlite.sqlite\")\n  download.file(file_url,\n                destfile=here(\"data\", \"Chinook.sqlite\"))\n}\n\n\n\nWe can list the files and see the .sqlite database:\n\n\nlist.files(here(\"data\"))\n\n\n [1] \"2016-07-19.csv.bz2\"       \"bmi_pm25_no2_sim.csv\"    \n [3] \"chicago.rds\"              \"Chinook.sqlite\"          \n [5] \"chopped.RDS\"              \"maacs_sim.csv\"           \n [7] \"nycflights13\"             \"storms_2004.csv.gz\"      \n [9] \"team_standings.csv\"       \"tuesdata_rainfall.RDS\"   \n[11] \"tuesdata_temperature.RDS\"\n\nConnect to the SQL database\nThe main workhorse packages that we will use are the DBI and dplyr packages. Let’s look at the DBI::dbConnect() help file\n\n\n?DBI::dbConnect\n\n\n\nSo we need a driver and one example is RSQLite::SQLite(). Let’s look at the help file\n\n\n?RSQLite::SQLite\n\n\n\nOk so with RSQLite::SQLite() and DBI::dbConnect() we can connect to a SQLite database.\nLet’s try that with our Chinook.sqlite file that we downloaded.\n\n\nlibrary(DBI)\nconn <- DBI::dbConnect(drv = RSQLite::SQLite(), \n                       dbname = here(\"data\", \"Chinook.sqlite\"))\nconn\n\n\n<SQLiteConnection>\n  Path: /Users/shicks/Documents/github/teaching/jhustatcomputing2021/data/Chinook.sqlite\n  Extensions: TRUE\n\nSo we have opened up a connection with the SQLite database. Next, we can see what tables are available in the database using the dbListTables() function:\n\n\ndbListTables(conn)\n\n\n [1] \"Album\"         \"Artist\"        \"Customer\"      \"Employee\"     \n [5] \"Genre\"         \"Invoice\"       \"InvoiceLine\"   \"MediaType\"    \n [9] \"Playlist\"      \"PlaylistTrack\" \"Track\"        \n\nFrom RStudio’s website, there are several ways to interact with SQL Databases. One of the simplest ways that we will use here is to leverage the dplyr framework.\n\n\"The dplyr package now has a generalized SQL backend for talking to databases, and the new dbplyr package translates R code into database-specific variants. As of this writing, SQL variants are supported for the following databases: Oracle, Microsoft SQL Server, PostgreSQL, Amazon Redshift, Apache Hive, and Apache Impala. More will follow over time.\n\nSo if we want to query a SQL databse with dplyr, the benefit of using dbplyr is:\n\n\"You can write your code in dplyr syntax, and dplyr will translate your code into SQL. There are several benefits to writing queries in dplyr syntax: you can keep the same consistent language both for R objects and database tables, no knowledge of SQL or the specific SQL variant is required, and you can take advantage of the fact that dplyr uses lazy evaluation.\n\nLet’s take a closer look at the conn database that we just connected to:\n\n\nlibrary(dbplyr)\nsrc_dbi(conn)\n\n\nsrc:  sqlite 3.36.0 [/Users/shicks/Documents/github/teaching/jhustatcomputing2021/data/Chinook.sqlite]\ntbls: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine,\n  MediaType, Playlist, PlaylistTrack, Track\n\nYou can think of the multiple tables similar to having multiple worksheets in a spreadsheet.\nLet’s try interacting with one.\nQuerying with dplyr syntax\nFirst, let’s look at the first ten rows in the Album table using the tbl() function from dplyr:\n\n\ntbl(conn, \"Album\") %>%\n  head(n=10)\n\n\n# Source:   lazy query [?? x 3]\n# Database: sqlite 3.36.0\n#   [/Users/shicks/Documents/github/teaching/jhustatcomputing2021/data/Chinook.sqlite]\n   AlbumId Title                                 ArtistId\n     <int> <chr>                                    <int>\n 1       1 For Those About To Rock We Salute You        1\n 2       2 Balls to the Wall                            2\n 3       3 Restless and Wild                            2\n 4       4 Let There Be Rock                            1\n 5       5 Big Ones                                     3\n 6       6 Jagged Little Pill                           4\n 7       7 Facelift                                     5\n 8       8 Warner 25 Anos                               6\n 9       9 Plays Metallica By Four Cellos               7\n10      10 Audioslave                                   8\n\nThe output looks just like a data.frame that we are familiar with. But it’s important to know that it’s not really a dataframe. For example, what about if we use the dim() function?\n\n\ntbl(conn, \"Album\") %>%\n  dim()\n\n\n[1] NA  3\n\nInteresting! We see that the number of rows returned is NA. This is because these functions are different than operating on datasets in memory (e.g. loading data into memory using read_csv()). Instead, dplyr communicates differently with a SQLite database.\nLet’s consider our example. If we were to use straight SQL, the following SQL query returns the first 10 rows from the Album table:\n\nSELECT *\nFROM `Album`\nLIMIT 10\n\nIn the background, dplyr does the following:\ntranslates your R code into SQL\nsubmits it to the database\ntranslates the database’s response into an R data frame\nTo better understand the dplyr code, we can use the show_query() function:\n\n\nAlbum <- tbl(conn, \"Album\")\nshow_query(head(Album, n = 10))\n\n\n<SQL>\nSELECT *\nFROM `Album`\nLIMIT 10\n\nThis is nice because instead of having to write the SQL query ourself, we can just use the dplyr and R syntax that we are used to.\nHowever, the downside is that dplyr never gets to see the full Album table. It only sends our query to the database, waits for a response and returns the query. However, in this way we can interact with large datasets!\nMany of the usual dplyr functions are available too:\nselect()\nfilter()\nsummarize()\nand many join functions.\nOk let’s try some of the functions out. First, let’s count how many albums each artist has made.\n\n\ntbl(conn, \"Album\") %>%\n  group_by(ArtistId) %>% \n  summarize(n = count(ArtistId)) %>% \n  head(n=10)\n\n\n# Source:   lazy query [?? x 2]\n# Database: sqlite 3.36.0\n#   [/Users/shicks/Documents/github/teaching/jhustatcomputing2021/data/Chinook.sqlite]\n   ArtistId     n\n      <int> <int>\n 1        1     2\n 2        2     2\n 3        3     1\n 4        4     1\n 5        5     1\n 6        6     2\n 7        7     1\n 8        8     3\n 9        9     1\n10       10     1\n\ndata viz\nNext, let’s plot it.\n\n\ntbl(conn, \"Album\") %>%\n  group_by(ArtistId) %>% \n  summarize(n = count(ArtistId)) %>% \n  arrange(desc(n)) %>% \n  ggplot(aes(x = ArtistId, y = n)) + \n  geom_bar(stat = \"identity\")\n\n\n\n\nLet’s also extract the first letter from each album and plot the frequency of each letter.\n\n\ntbl(conn, \"Album\") %>%\n  mutate(first_letter = str_sub(Title, end = 1)) %>% \n  ggplot(aes(first_letter)) + \n  geom_bar()\n\n\n\n\nPost-lecture materials\nAdditional Resources\n\nhttps://dbi.r-dbi.org\nhttps://db.rstudio.com/databases/sqlite/\nhttps://dbplyr.tidyverse.org\n\n\n\n\n",
    "preview": "https://upload.wikimedia.org/wikipedia/commons/8/8d/Relational_model_concepts.png",
    "last_modified": "2021-10-11T22:34:27-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-07-regular-expressions/",
    "title": "Regular expression",
    "description": "Introduction to working with character strings and regular expressions inR",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-10-07",
    "categories": [
      "module 2",
      "week 6",
      "tidyverse",
      "R",
      "programming",
      "strings and regex"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nIntroduction\nregex basics\nstring basics\n\ngrepl()\nmetacharacters\nrepetition\ncapture group\ncharacter sets\nbrackets\nranges\nbeginning and end\nOR metacharacter\n\nOther regex in base R\ngrep()\nsub()\ngsub()\nstrsplit()\n\nThe stringr package\nPost-lecture materials\nFinal Questions\nAdditional Resources\n\n\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://r4ds.had.co.nz/strings.html\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-regular-expressions.html\nhttps://r4ds.had.co.nz/strings.html\nLearning objectives\n\nAt the end of this lesson you will:\nUnderstand what is a regular expression and how to create one\nLearn the basics of searching for patterns in character strings in base R and the stringr R package in the tidyverse\nUse the built in character sets to search for patterns in strings including \"\\n\", \"\\t\", \"\\w\", \"\\d\", and \"\\s\"\n\nIntroduction\nregex basics\nNow that we have covered the basics of string manipulation in R, let’s discuss the more advanced topic of regular expressions.\nA regular expression (also known as a “regex” or “regexp”) is a concise language for describing patterns in character strings. For example, a regex are patterns that could be contained within another string. A regular expression can be used for e.g. \nsearching for a pattern or string within another string (e.g searching for the string “a” in the string “Maryland”)\nreplacing one part of a string with another string (e.g replacing the string “t” with “p” in the string “hot” where you are changing the string “hot” to “hop”)\nIf you have never worked with regular expressions, it can seem like maybe a baby hit the keys on your keyboard (complete gibberish), but it will slowly make sense once you learn the syntax. Soon you will be able create incredibly powerful regular expressions in your day-to-day work.\nstring basics\nIn R, you can create (character) strings with either single quotes ('hello!') or double quotes (\"hello!\") – no difference (not true for other languages).\nI recommend using the double quotes, unless you want to create a string with multiple \".\n\n\nstring1 <- \"This is a string\"\nstring2 <- 'If I want to include a \"quote\" inside a string, I use single quotes'\n\n\n\n\nPro tip: strings can be tricky when executing them. If you forget to close a quote, you’ll see +\n\n> \"This is a string without a closing quote\n+ \n+ \n+ HELP I'M STUCK\n\nIf this happen to you, press Escape and try again\n\nMultiple strings are often stored in a character vector, which you can create with c():\n\n\nc(\"one\", \"two\", \"three\")\n\n\n[1] \"one\"   \"two\"   \"three\"\n\ngrepl()\nOne of the most basic functions in R that uses regular expressions is the grepl(pattern, x) function, which takes two arguments: a regular expression (pattern) and a string to be searched (x).\nIt literally translates to “grep logical”.\nIf the string (x) contains the specified regular expression (pattern), then grepl() will return TRUE, otherwise it will return FALSE.\nLet’s take a look at one example:\n\n\nregular_expression <- \"a\"\nstring_to_search <- \"Maryland\"\n\ngrepl(regular_expression, string_to_search)\n\n\n[1] TRUE\n\nIn the example above, we specify the regular expression \"a\" and store it in a variable called regular_expression.\nRemember that regular expressions are just strings!\nWe also store the string \"Maryland\" in a variable called string_to_search. The regular expression \"a\" represents a single occurrence of the character \"a\". Since \"a\" is contained within \"Maryland\", grepl() returns the value TRUE.\nLet’s try another simple example:\n\n\nregular_expression <- \"u\"\nstring_to_search <- \"Maryland\"\n\ngrepl(regular_expression, string_to_search)\n\n\n[1] FALSE\n\nThe regular expression \"u\" represents a single occurrence of the character \"u\", which is not a sub-string of \"Maryland\", therefore grepl() returns the value FALSE.\nRegular expressions can be much longer than single characters. You could for example search for smaller strings inside of a larger string:\n\n\ngrepl(\"land\", \"Maryland\")\n\n\n[1] TRUE\n\ngrepl(\"ryla\", \"Maryland\")\n\n\n[1] TRUE\n\ngrepl(\"Marly\", \"Maryland\")\n\n\n[1] FALSE\n\ngrepl(\"dany\", \"Maryland\")\n\n\n[1] FALSE\n\nSince \"land\" and \"ryla\" are sub-strings of \"Maryland\", grepl() returns TRUE, however when a regular expression like \"Marly\" or \"dany\" is searched grepl() returns FALSE because neither are sub-strings of \"Maryland\".\nThere is a dataset that comes with R called state.name which is a vector of strings, one for each state in the United States of America.\nWe are going to use this vector in several of the following examples.\n\n\nhead(state.name)\n\n\n[1] \"Alabama\"    \"Alaska\"     \"Arizona\"    \"Arkansas\"   \"California\"\n[6] \"Colorado\"  \n\nlength(state.name)\n\n\n[1] 50\n\nLet’s build a regular expression for identifying several strings in this vector, specifically a regular expression that will match names of states that both start and end with a vowel.\nThe state name could start and end with any vowel, so we will not be able to match exact sub-strings like in the previous examples. Thankfully we can use metacharacters to look for vowels and other parts of strings.\nmetacharacters\nThe first metacharacter that we will discuss is \".\".\nThe metacharacter that only consists of a period represents any character other than a new line (we will discuss new lines soon).\nLet’s take a look at some examples using the period regex:\n\n\ngrepl(\".\", \"Maryland\")\n\n\n[1] TRUE\n\ngrepl(\".\", \"*&2[0+,%<@#~|}\")\n\n\n[1] TRUE\n\ngrepl(\".\", \"\")\n\n\n[1] FALSE\n\nAs you can see the period metacharacter is very liberal. This metacharacter is most useful when you do not care about a set of characters in a regular expression.\nFor example:\n\n\ngrepl(\"a.b\", c(\"aaa\", \"aab\", \"abb\", \"acadb\"))\n\n\n[1] FALSE  TRUE  TRUE  TRUE\n\nIn the case above, grepl() returns TRUE for all strings that contain an a followed by any other character followed by a b.\nrepetition\nYou can specify a regular expression that contains a certain number of characters or metacharacters using the enumeration metacharacters (or sometimes called quantifiers).\n+: indicates that one or more of the preceding expression should be present (or matches at least 1 time)\n*: indicates that zero or more of the preceding expression is present (or matches at least 0 times)\n?: indicates that zero or 1 of the preceding expression is not present or present at most 1 time (or matches between 0 and 1 times)\nLet’s take a look at some examples using these metacharacters:\n\n\n# Does \"Maryland\" contain one or more of \"a\" ?\ngrepl(\"a+\", \"Maryland\")\n\n\n[1] TRUE\n\n# Does \"Maryland\" contain one or more of \"x\" ?\ngrepl(\"x+\", \"Maryland\")\n\n\n[1] FALSE\n\n# Does \"Maryland\" contain zero or more of \"x\" ?\ngrepl(\"x*\", \"Maryland\")\n\n\n[1] TRUE\n\nYou can also specify exact numbers of expressions using curly brackets {}.\n{n}: exactly n\n{n,}: n or more\n{,m}: at most m\n{n,m}: between n and m\nFor example \"a{5}\" specifies “a exactly five times”, \"a{2,5}\" specifies “a between 2 and 5 times,” and \"a{2,}\" specifies “a at least 2 times.” Let’s take a look at some examples:\n\n\n# Does \"Mississippi\" contain exactly 2 adjacent \"s\" ?\ngrepl(\"s{2}\", \"Mississippi\")\n\n\n[1] TRUE\n\n# This is equivalent to the expression above:\ngrepl(\"ss\", \"Mississippi\")\n\n\n[1] TRUE\n\n# Does \"Mississippi\" contain between 1 and 3 adjacent \"s\" ?\ngrepl(\"s{1,3}\", \"Mississippi\")\n\n\n[1] TRUE\n\n# Does \"Mississippi\" contain between 2 and 3 adjacent \"i\" ?\ngrepl(\"i{2,3}\", \"Mississippi\")\n\n\n[1] FALSE\n\n# Does \"Mississippi\" contain between 2 adjacent \"iss\" ?\ngrepl(\"(iss){2}\", \"Mississippi\")\n\n\n[1] TRUE\n\n# Does \"Mississippi\" contain between 2 adjacent \"ss\" ?\ngrepl(\"(ss){2}\", \"Mississippi\")\n\n\n[1] FALSE\n\n# Does \"Mississippi\" contain the pattern of an \"i\" followed by \n# 2 of any character, with that pattern repeated three times adjacently?\ngrepl(\"(i.{2}){3}\", \"Mississippi\")\n\n\n[1] TRUE\n\ncapture group\nIn the last three examples, I used parentheses () to create a capturing group. A capturing group allows you to use quantifiers on other regular expressions.\nIn the last example, I first created the regex \"i.{2}\" which matches i followed by any two characters (“iss” or “ipp”). Then, I used a capture group to to wrap that regex, and to specify exactly three adjacent occurrences of that regex.\nYou can specify sets of characters (or character sets or character classes) with regular expressions, some of which come built in, but you can build your own character sets too.\ncharacter sets\nFirst, we will discuss the built in character sets:\nwords (\"\\\\w\") = Words specify any letter, digit, or a underscore.\ndigits (\"\\\\d\") = Digits specify the digits 0 through 9\nwhitespace characters (\"\\\\s\") = whitespace specifies line breaks, tabs, or spaces\nEach of these character sets have their own compliments:\nnot words (\"\\\\W\")\nnot digits (\"\\\\D\")\nnot whitespace characters (\"\\\\S\")\nEach specifies all of the characters not included in their corresponding character sets.\n\nInteresting fact: Technically, you are using the a character set \"\\d\" or \"\\s\" (with only one black slash), but because you are using this character set in a string, you need the second \\ to escape the string. So you will type \"\\\\d\" or \"\\\\s\".\nSo for example, to include a literal single or double quote in a string you can use \\ to “escape” the string and being able to include a single or double quote:\n\n\ndouble_quote <- \"\\\"\" # or '\"'\ndouble_quote\n\n\n[1] \"\\\"\"\n\nsingle_quote <- '\\'' # or \"'\"Copy\nsingle_quote\n\n\n[1] \"'\"\n\nThat means if you want to include a literal backslash, you will need to double it up: \"\\\\\".\n\nIn fact, putting two backslashes before any punctuation mark that is also a metacharacter indicates that you are looking for the symbol and not the metacharacter meaning. For example \"\\\\.\" indicates you are trying to match a period in a string. Let’s take a look at a few examples:\n\n\ngrepl(\"\\\\+\", \"tragedy + time = humor\")\n\n\n[1] TRUE\n\ngrepl(\"\\\\.\", \"https://publichealth.jhu.edu\")\n\n\n[1] TRUE\n\n\nBeware: the printed representation of a string is not the same as string itself, because the printed representation shows the escapes. To see the raw contents of the string, use writeLines():\n\n\nx <- c(\"\\'\", \"\\\"\", \"\\\\\")\nx\n\n\n[1] \"'\"  \"\\\"\" \"\\\\\"\n\nwriteLines(x)\n\n\n'\n\"\n\\\n\nThere are a handful of other special characters. The most common are “”, newline, and “, tab, but you can see the complete list by requesting help on”: ?‘“’, or ?”’\".\n\n\n?\"'\"\n\n\n\nYou will also sometimes see strings like “”, this is a way of writing non-English characters that works on all platforms:\n\n\nx <- c(\"\\\\t\", \"\\\\n\", \"\\u00b5\")\nx\n\n\n[1] \"\\\\t\" \"\\\\n\" \"µ\"  \n\nwriteLines(x)\n\n\n\\t\n\\n\nµ\n\n\nLet’s take a look at a few examples of built in character sets: \"\\w\", \"\\d\", \"\\s\".\n\n\ngrepl(\"\\\\w\", \"abcdefghijklmnopqrstuvwxyz0123456789\")\n\n\n[1] TRUE\n\ngrepl(\"\\\\d\", \"0123456789\")\n\n\n[1] TRUE\n\n# \"\\n\" is the metacharacter for a new line\n# \"\\t\" is the metacharacter for a tab\ngrepl(\"\\\\s\", \"\\n\\t   \")\n\n\n[1] TRUE\n\ngrepl(\"\\\\d\", \"abcdefghijklmnopqrstuvwxyz\")\n\n\n[1] FALSE\n\ngrepl(\"\\\\D\", \"abcdefghijklmnopqrstuvwxyz\")\n\n\n[1] TRUE\n\ngrepl(\"\\\\w\", \"\\n\\t   \")\n\n\n[1] FALSE\n\nbrackets\nYou can also specify specific character sets using straight brackets []. For example a character set of just the vowels would look like: \"[aeiou]\".\n\n\ngrepl(\"[aeiou]\", \"rhythms\")\n\n\n[1] FALSE\n\nYou can find the complement to a specific character by putting a carrot ^ after the first bracket. For example \"[^aeiou]\" matches all characters except the lowercase vowels.\n\n\ngrepl(\"[^aeiou]\", \"rhythms\")\n\n\n[1] TRUE\n\nranges\nYou can also specify ranges of characters using a hyphen - inside of the brackets. For example \"[a-m]\" matches all of the lowercase characters between a and m, while \"[5-8]\" matches any digit between 5 and 8 inclusive. Let’s take a look at some examples using custom character sets:\n\n\ngrepl(\"[a-m]\", \"xyz\")\n\n\n[1] FALSE\n\ngrepl(\"[a-m]\", \"ABC\")\n\n\n[1] FALSE\n\ngrepl(\"[a-mA-M]\", \"ABC\")\n\n\n[1] TRUE\n\nbeginning and end\nThere are also metacharacters for matching the beginning and the end of a string which are \"^\" and \"$\" respectively. Let’s take a look at a few examples:\n\n\ngrepl(\"^a\", c(\"bab\", \"aab\"))\n\n\n[1] FALSE  TRUE\n\ngrepl(\"b$\", c(\"bab\", \"aab\"))\n\n\n[1] TRUE TRUE\n\ngrepl(\"^[ab]*$\", c(\"bab\", \"aab\", \"abc\"))\n\n\n[1]  TRUE  TRUE FALSE\n\nOR metacharacter\nThe last metacharacter we will discuss is the OR metacharacter (\"|\"). The OR metacharacter matches either the regex on the left or the regex on the right side of this character. A few examples:\n\n\ngrepl(\"a|b\", c(\"abc\", \"bcd\", \"cde\"))\n\n\n[1]  TRUE  TRUE FALSE\n\ngrepl(\"North|South\", c(\"South Dakota\", \"North Carolina\", \"West Virginia\"))\n\n\n[1]  TRUE  TRUE FALSE\n\nFinally, we have learned enough to create a regular expression that matches all state names that both begin and end with a vowel:\nWe match the beginning of a string.\nWe create a character set of just capitalized vowels.\nWe specify one instance of that set.\nThen any number of characters until:\nA character set of just lowercase vowels.\nWe specify one instance of that set.\nWe match the end of a string.\n\n\nstart_end_vowel <- \"^[AEIOU]{1}.+[aeiou]{1}$\"\nvowel_state_lgl <- grepl(start_end_vowel, state.name)\nhead(vowel_state_lgl)\n\n\n[1]  TRUE  TRUE  TRUE FALSE FALSE FALSE\n\nstate.name[vowel_state_lgl]\n\n\n[1] \"Alabama\"  \"Alaska\"   \"Arizona\"  \"Idaho\"    \"Indiana\"  \"Iowa\"    \n[7] \"Ohio\"     \"Oklahoma\"\n\nBelow is a table of several important metacharacters:\n\nMetacharacter\nMeaning\n.\nAny Character\n\\w\nA Word\n\\W\nNot a Word\n\\d\nA Digit\n\\D\nNot a Digit\n\\s\nWhitespace\n\\S\nNot Whitespace\n[xyz]\nA Set of Characters\n[^xyz]\nNegation of Set\n[a-z]\nA Range of Characters\n^\nBeginning of String\n$\nEnd of String\n\\n\nNewline\n+\nOne or More of Previous\n*\nZero or More of Previous\n?\nZero or One of Previous\n|\nEither the Previous or the Following\n{5}\nExactly 5 of Previous\n{2, 5}\nBetween 2 and 5 or Previous\n{2, }\nMore than 2 of Previous\n\nOther regex in base R\nSo far we’ve been using grepl() to see if a regex matches a string. There are a few other built in regex functions you should be aware of.\nFirst, we will review our workhorse of this lesson, grepl(), which stands for “grep logical.”\n\n\ngrepl(\"[Ii]\", c(\"Hawaii\", \"Illinois\", \"Kentucky\"))\n\n\n[1]  TRUE  TRUE FALSE\n\ngrep()\nThen, there is old fashioned grep(pattern, x), which returns the indices of the vector that match the regex:\n\n\ngrep(\"[Ii]\", c(\"Hawaii\", \"Illinois\", \"Kentucky\"))\n\n\n[1] 1 2\n\nsub()\nThe sub(pattern, replacement, x) function takes as arguments a regex, a “replacement,” and a vector of strings. This function will replace the first instance of that regex found in each string.\n\n\nsub(\"[Ii]\", \"1\", c(\"Hawaii\", \"Illinois\", \"Kentucky\"))\n\n\n[1] \"Hawa1i\"   \"1llinois\" \"Kentucky\"\n\ngsub()\nThe gsub(pattern, replacement, x) function is nearly the same as sub() except it will replace every instance of the regex that is matched in each string.\n\n\ngsub(\"[Ii]\", \"1\", c(\"Hawaii\", \"Illinois\", \"Kentucky\"))\n\n\n[1] \"Hawa11\"   \"1ll1no1s\" \"Kentucky\"\n\nstrsplit()\nThe strsplit(x, split) function will split up strings (split) according to the provided regex (x) . If strsplit() is provided with a vector of strings it will return a list of string vectors.\n\n\ntwo_s <- state.name[grep(\"ss\", state.name)]\ntwo_s\n\n\n[1] \"Massachusetts\" \"Mississippi\"   \"Missouri\"      \"Tennessee\"    \n\nstrsplit(two_s, \"ss\")\n\n\n[[1]]\n[1] \"Ma\"        \"achusetts\"\n\n[[2]]\n[1] \"Mi\"   \"i\"    \"ippi\"\n\n[[3]]\n[1] \"Mi\"   \"ouri\"\n\n[[4]]\n[1] \"Tenne\" \"ee\"   \n\nThe stringr package\nThe stringr package, written by Hadley Wickham, is part of the Tidyverse group of R packages. This package takes a “data first” approach to functions involving regex, so usually the string is the first argument and the regex is the second argument. The majority of the function names in stringr begin with str_*().\n\n\n\nFigure 1: Cheatsheet for stringr R package\n\n\n\n[Source: https://stringr.tidyverse.org]\nThe str_extract(string, pattern) function returns the sub-string of a string (string) that matches the provided regular expression (pattern).\n\n\nlibrary(stringr)\nstate_tbl <- paste(state.name, state.area, state.abb)\nhead(state_tbl)\n\n\n[1] \"Alabama 51609 AL\"     \"Alaska 589757 AK\"    \n[3] \"Arizona 113909 AZ\"    \"Arkansas 53104 AR\"   \n[5] \"California 158693 CA\" \"Colorado 104247 CO\"  \n\nstr_extract(state_tbl, \"[0-9]+\")\n\n\n [1] \"51609\"  \"589757\" \"113909\" \"53104\"  \"158693\" \"104247\" \"5009\"  \n [8] \"2057\"   \"58560\"  \"58876\"  \"6450\"   \"83557\"  \"56400\"  \"36291\" \n[15] \"56290\"  \"82264\"  \"40395\"  \"48523\"  \"33215\"  \"10577\"  \"8257\"  \n[22] \"58216\"  \"84068\"  \"47716\"  \"69686\"  \"147138\" \"77227\"  \"110540\"\n[29] \"9304\"   \"7836\"   \"121666\" \"49576\"  \"52586\"  \"70665\"  \"41222\" \n[36] \"69919\"  \"96981\"  \"45333\"  \"1214\"   \"31055\"  \"77047\"  \"42244\" \n[43] \"267339\" \"84916\"  \"9609\"   \"40815\"  \"68192\"  \"24181\"  \"56154\" \n[50] \"97914\" \n\nThe str_order(x) function returns a numeric vector that corresponds to the alphabetical order of the strings in the provided vector (x).\n\n\nhead(state.name)\n\n\n[1] \"Alabama\"    \"Alaska\"     \"Arizona\"    \"Arkansas\"   \"California\"\n[6] \"Colorado\"  \n\nstr_order(state.name)\n\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22\n[23] 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44\n[45] 45 46 47 48 49 50\n\nhead(state.abb)\n\n\n[1] \"AL\" \"AK\" \"AZ\" \"AR\" \"CA\" \"CO\"\n\nstr_order(state.abb)\n\n\n [1]  2  1  4  3  5  6  7  8  9 10 11 15 12 13 14 16 17 18 21 20 19 22\n[23] 23 25 24 26 33 34 27 29 30 31 28 32 35 36 37 38 39 40 41 42 43 44\n[45] 46 45 47 49 48 50\n\nThe str_pad(string, width, side, pad) function pads strings (string) with other characters, which is often useful when the string is going to be eventually printed for a person to read.\n\n\nstr_pad(\"Thai\", width = 8, side = \"left\", pad = \"-\")\n\n\n[1] \"----Thai\"\n\nstr_pad(\"Thai\", width = 8, side = \"right\", pad = \"-\")\n\n\n[1] \"Thai----\"\n\nstr_pad(\"Thai\", width = 8, side = \"both\", pad = \"-\")\n\n\n[1] \"--Thai--\"\n\nThe str_to_title(string) function acts just like tolower() and toupper() except it puts strings into Title Case.\n\n\ncases <- c(\"CAPS\", \"low\", \"Title\")\nstr_to_title(cases)\n\n\n[1] \"Caps\"  \"Low\"   \"Title\"\n\nThe str_trim(string) function deletes white space from both sides of a string.\n\n\nto_trim <- c(\"   space\", \"the    \", \"    final frontier  \")\nstr_trim(to_trim)\n\n\n[1] \"space\"          \"the\"            \"final frontier\"\n\nThe str_wrap(string) function inserts newlines in strings so that when the string is printed each line’s length is limited.\n\n\npasted_states <- paste(state.name[1:20], collapse = \" \")\n\ncat(str_wrap(pasted_states, width = 80))\n\n\nAlabama Alaska Arizona Arkansas California Colorado Connecticut Delaware Florida\nGeorgia Hawaii Idaho Illinois Indiana Iowa Kansas Kentucky Louisiana Maine\nMaryland\n\ncat(str_wrap(pasted_states, width = 30))\n\n\nAlabama Alaska Arizona\nArkansas California Colorado\nConnecticut Delaware Florida\nGeorgia Hawaii Idaho Illinois\nIndiana Iowa Kansas Kentucky\nLouisiana Maine Maryland\n\nThe word() function allows you to index each word in a string as if it were a vector.\n\n\na_tale <- \"It was the best of times it was the worst of times it was the age of wisdom it was the age of foolishness\"\n\nword(a_tale, 2)\n\n\n[1] \"was\"\n\nword(a_tale, end = 3)\n\n\n[1] \"It was the\"\n\nword(a_tale, start = 11, end = 15)\n\n\n[1] \"of times it was the\"\n\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nThere is a corpus of common words here:\n\n\nhead(stringr::words)\n\n\n[1] \"a\"        \"able\"     \"about\"    \"absolute\" \"accept\"   \"account\" \n\nlength(stringr::words)\n\n\n[1] 980\n\nUsing stringr::words, create regular expressions that find all words that:\nStart with “y”.\nEnd with “x”\nAre exactly three letters long. (Don’t cheat by using str_length()!)\nHave seven letters or more.\nUsing the same stringr::words, create regular expressions to find all words that:\nStart with a vowel.\nThat only contain consonants. (Hint: thinking about matching “not”-vowels.)\nEnd with ed, but not with eed.\nEnd with ing or ise.\n\nAdditional Resources\n\nhttps://stringr.tidyverse.org\nhttps://rdpeng.github.io/Biostat776/lecture-regular-expressions.html\nhttps://r4ds.had.co.nz/strings.html\n\n\n\n\n",
    "preview": "https://raw.githubusercontent.com/rstudio/cheatsheets/master/pngs/thumbnails/strings-cheatsheet-thumbs.png",
    "last_modified": "2021-10-06T18:26:19-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-05-working-with-dates-and-times/",
    "title": "Working with dates and times",
    "description": "Introduction to lubridate for dates and times in R.",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-10-05",
    "categories": [
      "module 2",
      "week 6",
      "tidyverse",
      "R",
      "programming",
      "dates and times"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nIntroduction\nThe lubridate package\n\nCreating date/times\nFrom a string\nFrom individual date-time components\nFrom other types\n\nDate-Times in R\nFrom a string\nPOSIXct or the POSIXlt class\n\nTime Zones!\nOperations on Dates and Times\nArithmetic\nLeaps and Bounds\n\nExtracting Elements of Dates/Times\nDate Elements\nTime Elements\n\nVisualizing Dates\nReading in the Data\nHistograms of Dates/Times\nScatterplots of Dates/Times\n\nSummary\nPost-lecture materials\nFinal Questions\nAdditional Resources\n\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://r4ds.had.co.nz/dates-and-times.html\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://jhu-advdatasci.github.io/2018/lectures/09-dates-times.html\nhttps://r4ds.had.co.nz/dates-and-times.html\nLearning objectives\n\nAt the end of this lesson you will:\nRecognize the Date, POSIXct and POSIXlt class types in R to represent dates and times\nLearn how to create date and time objects in R using functions from the lubridate package\nLearn how dealing with time zones can be frustrating 🙀 but hopefully less so after today’s lecture 😺\nLearn how to perform arithmetic operations on dates and times\nLearn how plotting systems in R “know” about dates and times to appropriately handle axis labels\n\nIntroduction\nIn this lesson, we will learn how to work with dates and times in R. These may seem simple as you use them all of the time in your day-to-day life, but the more you work with them, the more complicated they seem to get.\nDates and times are hard because they have to reconcile two physical phenomena (the rotation of the Earth and its orbit around the sun) with a whole raft of geopolitical phenomena including months, time zones, and daylight savings time (DST).\nThis lesson will not teach you every last detail about dates and times, but it will give you a solid grounding of practical skills that will help you with common data analysis challenges.\nR has developed a special representation of dates and times\nDates are represented by the Date class\nTimes are represented by the POSIXct or the POSIXlt class\nDates are stored internally as the number of days since 1970-01-01\nTimes are stored internally as the number of seconds since 1970-01-01\nThe lubridate package\nHere, we will focus on the lubridate R package, which makes it easier to work with dates and times in R.\ncheck out the cheat sheet at https://lubridate.tidyverse.org\nA few things to note about it:\nIt largely replaces the default date/time functions in base R\nIt contains methods for date/time arithmetic\nIt handles time zones, leap year, leap seconds, etc.\nlubridate is installed when you install tidyverse, but it is not loaded when you load tidyverse. Alternatively, you can install it separately.\n\n\ninstall.packages(\"lubridate\") \n\n\n\n\n\nlibrary(tidyverse)\nlibrary(lubridate) \n\n\n\nCreating date/times\nThere are three types of date/time data that refer to an instant in time:\nA date. Tibbles print this as <date>.\nA time within a day. Tibbles print this as <time>.\nA date-time is a date plus a time: it uniquely identifies an instant in time (typically to the nearest second). Tibbles print this as <dttm>. Elsewhere in R these are called POSIXct.\nIn this lesson, we will focus on dates and date-times as R does not have a native class for storing times. If you need one, you can use the hms package.\nYou should always use the simplest possible data type that works for your needs. That means if you can use a date instead of a date-time, you should. Date-times are substantially more complicated because of the need to handle time zones, which we’ll come back to at the end of the chapter.\nTo get the current date or date-time you can use today() or now() from lubridate:\n\n\ntoday()\n\n\n[1] \"2021-10-05\"\n\nnow()\n\n\n[1] \"2021-10-05 15:01:20 EDT\"\n\nOtherwise, there are three ways you are likely to create a date/time:\nFrom a string.\nFrom individual date-time components.\nFrom an existing date/time object.\nThey work as follows.\nFrom a string\nDates are of the Date class.\n\n\nx <- today()\nclass(x)\n\n\n[1] \"Date\"\n\nDates can be coerced from a character strings using some helper functions from lubridate. They automatically work out the format once you specify the order of the component.\nTo use the helper functions, identify the order in which year, month, and day appear in your dates, then arrange “y”, “m”, and “d” in the same order.\nThat gives you the name of the lubridate function that will parse your date. For example:\n\n\nymd(\"1970-01-01\")\n\n\n[1] \"1970-01-01\"\n\nymd(\"2017-01-31\")\n\n\n[1] \"2017-01-31\"\n\nmdy(\"January 31st, 2017\")\n\n\n[1] \"2017-01-31\"\n\ndmy(\"31-Jan-2017\")\n\n\n[1] \"2017-01-31\"\n\n\nPro tips:\nWhen reading in data with read_csv(), you may need to read in as character first and then convert to date/time\nDate objects have their own special print() methods that will always format as “YYYY-MM-DD”\nThese functions also take unquoted numbers.\n\n\nymd(20170131)\n\n\n[1] \"2017-01-31\"\n\n\nAlternate Formulations\nDifferent locales have different ways of formatting dates\n\n\nymd(\"2016-09-13\")  ## International standard\n\n\n[1] \"2016-09-13\"\n\nymd(\"2016/09/13\")  ## Just figure it out\n\n\n[1] \"2016-09-13\"\n\nmdy(\"09-13-2016\")  ## Mostly U.S.\n\n\n[1] \"2016-09-13\"\n\ndmy(\"13-09-2016\")  ## Europe\n\n\n[1] \"2016-09-13\"\n\nAll of the above are valid and lead to the exact same object.\nEven if the individual dates are formatted differently, ymd() can usually figure it out.\n\n\nx <- c(\"2016-04-05\", \n       \"2016/05/06\",\n       \"2016,10,4\")\nymd(x)\n\n\n[1] \"2016-04-05\" \"2016-05-06\" \"2016-10-04\"\n\nFrom individual date-time components\nSometimes the date components will come across multiple columns in a dataset.\n\n\nlibrary(nycflights13)\n\nflights %>% \n  select(year, month, day)\n\n\n# A tibble: 336,776 × 3\n    year month   day\n   <int> <int> <int>\n 1  2013     1     1\n 2  2013     1     1\n 3  2013     1     1\n 4  2013     1     1\n 5  2013     1     1\n 6  2013     1     1\n 7  2013     1     1\n 8  2013     1     1\n 9  2013     1     1\n10  2013     1     1\n# … with 336,766 more rows\n\nTo create a date/time from this sort of input, use make_date() for dates, or make_datetime() for date-times:\n\n\nflights %>% \n  select(year, month, day) %>% \n  mutate(departure = make_date(year, month, day))\n\n\n# A tibble: 336,776 × 4\n    year month   day departure \n   <int> <int> <int> <date>    \n 1  2013     1     1 2013-01-01\n 2  2013     1     1 2013-01-01\n 3  2013     1     1 2013-01-01\n 4  2013     1     1 2013-01-01\n 5  2013     1     1 2013-01-01\n 6  2013     1     1 2013-01-01\n 7  2013     1     1 2013-01-01\n 8  2013     1     1 2013-01-01\n 9  2013     1     1 2013-01-01\n10  2013     1     1 2013-01-01\n# … with 336,766 more rows\n\n\nThe flights also contains a hour and minute column. Use make_datetime() to create a date-time column called departure:\n\n\n# try it yourself\n\n\n\n\nFrom other types\nYou may want to switch between a date-time and a date. That is the job of as_datetime() and as_date():\n\n\ntoday()\n\n\n[1] \"2021-10-05\"\n\nas_datetime(today())\n\n\n[1] \"2021-10-05 UTC\"\n\nnow()\n\n\n[1] \"2021-10-05 15:01:23 EDT\"\n\nas_date(now())\n\n\n[1] \"2021-10-05\"\n\nDate-Times in R\nFrom a string\nymd() and friends create dates. To create a date-time from a character string, add an underscore and one or more of “h”, “m”, and “s” to the name of the parsing function:\nTimes can be coerced from a character string with ymd_hms()\n\n\nymd_hms(\"2017-01-31 20:11:59\")\n\n\n[1] \"2017-01-31 20:11:59 UTC\"\n\nmdy_hm(\"01/31/2017 08:01\")\n\n\n[1] \"2017-01-31 08:01:00 UTC\"\n\nYou can also force the creation of a date-time from a date by supplying a timezone:\n\n\nymd_hms(\"2016-09-13 14:00:00\")\n\n\n[1] \"2016-09-13 14:00:00 UTC\"\n\nymd_hms(\"2016-09-13 14:00:00\", tz = \"America/New_York\")\n\n\n[1] \"2016-09-13 14:00:00 EDT\"\n\nymd_hms(\"2016-09-13 14:00:00\", tz = \"\")\n\n\n[1] \"2016-09-13 14:00:00 EDT\"\n\nPOSIXct or the POSIXlt class\nLet’s get into some hairy details about date-times. Date-times are represented using the POSIXct or the POSIXlt class in R. What are these things?\nPOSIXct\nPOSIXct is just a very large integer under the hood. It is a useful class when you want to store times in something like a data frame.\nTechnically, the POSIXct class represents the number of seconds since 1 January 1970. (In case you were wondering, “POSIXct” stands for “Portable Operating System Interface”, calendar time.)\n\n\nx <- ymd_hm(\"1970-01-01 01:00\")\nclass(x) \n\n\n[1] \"POSIXct\" \"POSIXt\" \n\nunclass(x)\n\n\n[1] 3600\nattr(,\"tzone\")\n[1] \"UTC\"\n\ntypeof(x)\n\n\n[1] \"double\"\n\nattributes(x)\n\n\n$class\n[1] \"POSIXct\" \"POSIXt\" \n\n$tzone\n[1] \"UTC\"\n\nPOSIXlt\nPOSIXlt is a list underneath and it stores a bunch of other useful information like the day of the week, day of the year, month, day of the month\n\n\ny <- as.POSIXlt(x)\n\ntypeof(y)\n\n\n[1] \"list\"\n\nattributes(y)\n\n\n$names\n[1] \"sec\"   \"min\"   \"hour\"  \"mday\"  \"mon\"   \"year\"  \"wday\"  \"yday\" \n[9] \"isdst\"\n\n$class\n[1] \"POSIXlt\" \"POSIXt\" \n\n$tzone\n[1] \"UTC\"\n\n\nPro tips: POSIXlts are rare inside the tidyverse. They do crop up in base R, because they are needed to extract specific components of a date, like the year or month.\nSince lubridate provides helpers for you to do this instead, you do not really need them imho.\nPOSIXct’s are always easier to work with, so if you find you have a POSIXlt, you should always convert it to a regular data time lubridate::as_datetime().\n\nTime Zones!\nTime zones were created to make your data analyses more difficult as a data analyst. Here are a few fun things to think about:\nymd_hms() function will by default use Coordinated Universal Time (UTC) as the time zone. UTC is the primary time standard by which the world regulates clocks and time.\nYou can go to Wikipedia to find the list of time zones\nSpecifying tz = \"\" in one of the ymd() and friends functions will use the local time zone\n\n\nx <- ymd_hm(\"1970-01-01 01:00\", tz = \"\")\nx\n\n\n[1] \"1970-01-01 01:00:00 EST\"\n\nattributes(x)\n\n\n$class\n[1] \"POSIXct\" \"POSIXt\" \n\n$tzone\n[1] \"\"\n\n\nPro tip: The tzone attribute is optional. It controls how the time is printed, not what absolute time it refers to.\n\n\nattr(x, \"tzone\") <- \"US/Pacific\"\nx\n\n\n[1] \"1969-12-31 22:00:00 PST\"\n\nattr(x, \"tzone\") <- \"US/Eastern\"\nx\n\n\n[1] \"1970-01-01 01:00:00 EST\"\n\n\nA few other fun things to think about related to time zones:\nAlmost always better to specify time zone when possible to avoid ambiguity\nDaylight savings time (DST)\nSome states are in two time zones\nSouthern hemisphere is opposite\nOperations on Dates and Times\nArithmetic\nYou can add and subtract dates and times. You can do comparisons too (i.e. ==, <=)\n\n\nx <- ymd(\"2012-01-01\", tz = \"\")  ## Midnight\ny <- dmy_hms(\"9 Jan 2011 11:34:21\", tz = \"\")\nx - y ## this works\n\n\nTime difference of 356.5178 days\n\nx < y ## this works\n\n\n[1] FALSE\n\nx > y ## this works\n\n\n[1] TRUE\n\nx == y ## this works\n\n\n[1] FALSE\n\nx + y ## what??? why does this not work? \n\n\nError in `+.POSIXt`(x, y): binary '+' is not defined for \"POSIXt\" objects\n\n\nRemember: POSIXct objects are a measure of seconds from an origin, usually the UNIX epoch (1st Jan 1970).\nJust add the requisite number of seconds to the object:\n\n\nx + 3*60*60 # add 3 hours\n\n\n[1] \"2012-01-01 03:00:00 EST\"\n\nx\n\n\n[1] \"2012-01-01 EST\"\n\n\nSame goes for days. For example, you can just keep the date portion using date():\n\n\ny <- date(y)\ny\n\n\n[1] \"2011-01-09\"\n\nAnd then add a number to the date (in this case 1 day)\n\n\ny + 1  \n\n\n[1] \"2011-01-10\"\n\nCool eh?\nLeaps and Bounds\nEven keeps track of leap years, leap seconds, daylight savings, and time zones.\nLeap years\n\n\nx <- ymd(\"2012-03-01\")\ny <- ymd(\"2012-02-28\")\nx - y\n\n\nTime difference of 2 days\n\nNot a leap year\n\n\nx <- ymd(\"2013-03-01\")\ny <- ymd(\"2013-02-28\")\nx - y\n\n\nTime difference of 1 days\n\nBUT beware of time zones!\n\n\nx <- ymd_hms(\"2012-10-25 01:00:00\", tz = \"\")\ny <- ymd_hms(\"2012-10-25 05:00:00\", tz = \"GMT\")\ny - x\n\n\nTime difference of 0 secs\n\nThere are also things called leap seconds.\n\n\n.leap.seconds\n\n\n [1] \"1972-07-01 GMT\" \"1973-01-01 GMT\" \"1974-01-01 GMT\"\n [4] \"1975-01-01 GMT\" \"1976-01-01 GMT\" \"1977-01-01 GMT\"\n [7] \"1978-01-01 GMT\" \"1979-01-01 GMT\" \"1980-01-01 GMT\"\n[10] \"1981-07-01 GMT\" \"1982-07-01 GMT\" \"1983-07-01 GMT\"\n[13] \"1985-07-01 GMT\" \"1988-01-01 GMT\" \"1990-01-01 GMT\"\n[16] \"1991-01-01 GMT\" \"1992-07-01 GMT\" \"1993-07-01 GMT\"\n[19] \"1994-07-01 GMT\" \"1996-01-01 GMT\" \"1997-07-01 GMT\"\n[22] \"1999-01-01 GMT\" \"2006-01-01 GMT\" \"2009-01-01 GMT\"\n[25] \"2012-07-01 GMT\" \"2015-07-01 GMT\" \"2017-01-01 GMT\"\n\nExtracting Elements of Dates/Times\nThere are a set of helper functions in lubridate that can extract sub-elements of dates/times\nDate Elements\n\n\nx <- ymd_hms(c(\"2012-10-25 01:13:46\",\n               \"2015-04-23 15:11:23\"), tz = \"\")\nyear(x)\n\n\n[1] 2012 2015\n\nmonth(x)\n\n\n[1] 10  4\n\nday(x)\n\n\n[1] 25 23\n\nweekdays(x)\n\n\n[1] \"Thursday\" \"Thursday\"\n\nTime Elements\n\n\nx <- ymd_hms(c(\"2012-10-25 01:13:46\",\n               \"2015-04-23 15:11:23\"), tz = \"\")\nminute(x)\n\n\n[1] 13 11\n\nsecond(x)\n\n\n[1] 46 23\n\nhour(x)\n\n\n[1]  1 15\n\nweek(x)\n\n\n[1] 43 17\n\nVisualizing Dates\nReading in the Data\n\n\nlibrary(here)\nlibrary(readr)\nstorm <- read_csv(here(\"data\", \"storms_2004.csv.gz\"), progress = FALSE)\nstorm\n\n\n# A tibble: 52,409 × 51\n   BEGIN_YEARMONTH BEGIN_DAY BEGIN_TIME END_YEARMONTH END_DAY END_TIME\n             <dbl>     <dbl>      <dbl>         <dbl>   <dbl>    <dbl>\n 1          200412        29       1800        200412      30     1200\n 2          200412        29       1800        200412      30     1200\n 3          200412         8       1800        200412       8     1800\n 4          200412        19       1500        200412      19     1700\n 5          200412        14        600        200412      14      800\n 6          200412        21        400        200412      21      800\n 7          200412        21        400        200412      21      800\n 8          200412        26       1500        200412      27      800\n 9          200412        26       1500        200412      27      800\n10          200412        11        800        200412      11     1300\n# … with 52,399 more rows, and 45 more variables: EPISODE_ID <dbl>,\n#   EVENT_ID <dbl>, STATE <chr>, STATE_FIPS <dbl>, YEAR <dbl>,\n#   MONTH_NAME <chr>, EVENT_TYPE <chr>, CZ_TYPE <chr>, CZ_FIPS <dbl>,\n#   CZ_NAME <chr>, WFO <chr>, BEGIN_DATE_TIME <chr>,\n#   CZ_TIMEZONE <chr>, END_DATE_TIME <chr>, INJURIES_DIRECT <dbl>,\n#   INJURIES_INDIRECT <dbl>, DEATHS_DIRECT <dbl>,\n#   DEATHS_INDIRECT <dbl>, DAMAGE_PROPERTY <chr>, …\n\n\n\nnames(storm)\n\n\n [1] \"BEGIN_YEARMONTH\"    \"BEGIN_DAY\"          \"BEGIN_TIME\"        \n [4] \"END_YEARMONTH\"      \"END_DAY\"            \"END_TIME\"          \n [7] \"EPISODE_ID\"         \"EVENT_ID\"           \"STATE\"             \n[10] \"STATE_FIPS\"         \"YEAR\"               \"MONTH_NAME\"        \n[13] \"EVENT_TYPE\"         \"CZ_TYPE\"            \"CZ_FIPS\"           \n[16] \"CZ_NAME\"            \"WFO\"                \"BEGIN_DATE_TIME\"   \n[19] \"CZ_TIMEZONE\"        \"END_DATE_TIME\"      \"INJURIES_DIRECT\"   \n[22] \"INJURIES_INDIRECT\"  \"DEATHS_DIRECT\"      \"DEATHS_INDIRECT\"   \n[25] \"DAMAGE_PROPERTY\"    \"DAMAGE_CROPS\"       \"SOURCE\"            \n[28] \"MAGNITUDE\"          \"MAGNITUDE_TYPE\"     \"FLOOD_CAUSE\"       \n[31] \"CATEGORY\"           \"TOR_F_SCALE\"        \"TOR_LENGTH\"        \n[34] \"TOR_WIDTH\"          \"TOR_OTHER_WFO\"      \"TOR_OTHER_CZ_STATE\"\n[37] \"TOR_OTHER_CZ_FIPS\"  \"TOR_OTHER_CZ_NAME\"  \"BEGIN_RANGE\"       \n[40] \"BEGIN_AZIMUTH\"      \"BEGIN_LOCATION\"     \"END_RANGE\"         \n[43] \"END_AZIMUTH\"        \"END_LOCATION\"       \"BEGIN_LAT\"         \n[46] \"BEGIN_LON\"          \"END_LAT\"            \"END_LON\"           \n[49] \"EPISODE_NARRATIVE\"  \"EVENT_NARRATIVE\"    \"DATA_SOURCE\"       \n\n\nLet’s take a look at the BEGIN_DATE_TIME, EVENT_TYPE, and DEATHS_DIRECT variables. Try to convert the BEGIN_DATE_TIME date/time column to a date/time R object.\n\n\nlibrary(dplyr)\nstorm %>% \n  select(BEGIN_DATE_TIME, EVENT_TYPE, DEATHS_DIRECT) %>% \n  mutate(date = dmy_hms(BEGIN_DATE_TIME))\n\n\n# A tibble: 52,409 × 4\n   BEGIN_DATE_TIME    EVENT_TYPE       DEATHS_DIRECT date               \n   <chr>              <chr>                    <dbl> <dttm>             \n 1 29-DEC-04 18:00:00 Heavy Snow                   0 2004-12-29 18:00:00\n 2 29-DEC-04 18:00:00 Heavy Snow                   0 2004-12-29 18:00:00\n 3 08-DEC-04 18:00:00 Winter Storm                 0 2004-12-08 18:00:00\n 4 19-DEC-04 15:00:00 High Wind                    0 2004-12-19 15:00:00\n 5 14-DEC-04 06:00:00 Winter Weather               0 2004-12-14 06:00:00\n 6 21-DEC-04 04:00:00 Winter Storm                 0 2004-12-21 04:00:00\n 7 21-DEC-04 04:00:00 Winter Storm                 0 2004-12-21 04:00:00\n 8 26-DEC-04 15:00:00 Winter Storm                 0 2004-12-26 15:00:00\n 9 26-DEC-04 15:00:00 Winter Storm                 0 2004-12-26 15:00:00\n10 11-DEC-04 08:00:00 Storm Surge/Tide             0 2004-12-11 08:00:00\n# … with 52,399 more rows\n\n# try it yourself\n\n\n\n\nNext, we do some wrangling to create a storm_sub data frame (code chunk set to echo=FALSE for the purposes of the lecture, but code is in the R Markdown).\n\n\n\n\n\nstorm_sub\n\n\n# A tibble: 52,409 × 3\n   begin               type             deaths\n   <dttm>              <chr>             <dbl>\n 1 2004-12-29 18:00:00 Heavy Snow            0\n 2 2004-12-29 18:00:00 Heavy Snow            0\n 3 2004-12-08 18:00:00 Winter Storm          0\n 4 2004-12-19 15:00:00 High Wind             0\n 5 2004-12-14 06:00:00 Winter Weather        0\n 6 2004-12-21 04:00:00 Winter Storm          0\n 7 2004-12-21 04:00:00 Winter Storm          0\n 8 2004-12-26 15:00:00 Winter Storm          0\n 9 2004-12-26 15:00:00 Winter Storm          0\n10 2004-12-11 08:00:00 Storm Surge/Tide      0\n# … with 52,399 more rows\n\nHistograms of Dates/Times\nWe can make a histogram of the dates/times to get a sense of when storm events occur.\n\n\nlibrary(ggplot2)\nstorm_sub %>%\n  ggplot(aes(x = begin)) + \n  geom_histogram(bins = 20) + \n  theme_bw()\n\n\n\n\nWe can group by event type too.\n\n\nlibrary(ggplot2)\nstorm_sub %>%\n  ggplot(aes(x = begin)) + \n  facet_wrap(~ type) + \n  geom_histogram(bins = 20) + \n  theme_bw() + \n  theme(axis.text.x.bottom = element_text(angle = 90))\n\n\n\n\nScatterplots of Dates/Times\n\n\nstorm_sub %>%\n  ggplot(aes(x = begin, y = deaths)) + \n  geom_point()\n\n\n\n\nIf we focus on a single month, the x-axis adapts.\n\n\nstorm_sub %>%\n  filter(month(begin) == 6) %>%\n  ggplot(aes(begin, deaths)) + \n  geom_point()\n\n\n\n\nSimilarly, we can focus on a single day.\n\n\nstorm_sub %>%\n  filter(month(begin) == 6, day(begin) == 16) %>%\n  ggplot(aes(begin, deaths)) + \n  geom_point()\n\n\n\n\nSummary\nDates and times have special classes in R that allow for numerical and statistical calculations\nDates use the Date class\nDate-Times (and Times) use the POSIXct and POSIXlt class\nCharacter strings can be coerced to Date/Time classes using the ymd() and ymd_hms() functions and friends.\nThe lubridate package is essential for manipulating date/time data\nBoth plot and ggplot “know” about dates and times and will handle axis labels appropriately.\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nWhat happens if you parse a string that contains invalid dates?\n\n\nymd(c(\"2010-10-10\", \"bananas\"))\n\n\n\nWhat does the tzone argument to today() do? Why is it important?\n\n\nunclass(today())\n\n\n[1] 18905\n\nUse the appropriate lubridate function to parse each of the following dates:\n\n\nd1 <- \"January 1, 2010\"\nd2 <- \"2015-Mar-07\"\nd3 <- \"06-Jun-2017\"\nd4 <- c(\"August 19 (2015)\", \"July 1 (2015)\")\nd5 <- \"12/30/14\" # Dec 30, 20\n\n\n\nUsing the flights dataset, how does the distribution of flight times within a day change over the course of the year?\nCompare dep_time, sched_dep_time and dep_delay. Are they consistent? Explain your findings.\n\nAdditional Resources\n\nhttps://lubridate.tidyverse.org\nlubridate cheat sheet: https://rawgit.com/rstudio/cheatsheets/master/lubridate.pdf\nhttps://jhu-advdatasci.github.io/2018/lectures/09-dates-times.html\nhttps://r4ds.had.co.nz/dates-and-times.html\n\n\n\n\n",
    "preview": "posts/2021-10-05-working-with-dates-and-times/working-with-dates-and-times_files/figure-html5/unnamed-chunk-34-1.png",
    "last_modified": "2021-10-05T15:01:41-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-09-30-object-oriented-programming/",
    "title": "Object Oriented Programming",
    "description": "Introduction to S3, S4, or reference class with generics and methods.",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-30",
    "categories": [
      "module 2",
      "week 5",
      "programming",
      "R",
      "functions"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nIntroduction\nOOP in R\nObject Oriented Principles\n\nS3\nBase types\nOO objects\nExample: S3 Class/Methods for Polygons\n\nS4\nReference Classes\nSummary\nPost-lecture materials\nAdditional Resources\n\n\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://adv-r.hadley.nz/oo.html\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-object-oriented-programming.html\nhttps://adv-r.hadley.nz/oo.html\nhttps://www.educative.io/blog/object-oriented-programming\nLearning objectives\n\nAt the end of this lesson you will:\nRecognize the primary object-oriented systems in R: S3, S4, and Reference Classes (RC).\nUnderstand the terminology of a class, object, method, constructor and generic.\nBe able to create a new S3, S4 or reference class with generics and methods\n\nIntroduction\nObject oriented programming is one of the most successful and widespread philosophies of programming and is a cornerstone of many programming languages including Java, Ruby, Python, and C++.\nAt it’s core, object oriented programming (OOP) is a paradigm that is made up of classes and objects.\nAt a high-level, we use OOP to structure software programs into small, reusable pieces of code blueprints (i.e. classes), which are used to create instances of concrete objects.\nThe blueprint (or class) typically represents broad categories, e.g. bus or car that share attributes (e.g. color). The classes specify what attributes you want, but not the actual values for a particular object. However, when you create instances with objects, you are specifying the attributes (e.g. a blue car, a red car, etc).\nIn addition, classes can also contain functions, called methods available only to objects of that type. These functions are defined within the class and perform some action helpful to that specific type of object. For example, our car class may have a method repaint that changes the color attribute of our car. This function is only helpful to objects of type car, so we declare it within the car class thus making it a method.\nOOP in R\nBase R has three object oriented systems because the roots of R date back to 1976, when the idea of object orientated programming was barely four years old.\nNew object oriented paradigms were added to R as they were invented, so some of the ideas in R about OOP have gone stale in the years since. It is still important to understand these older systems since a huge amount of R code is written with them, and they are still useful and interesting! Long time object oriented programmers reading this book may find these old ideas refreshing.\nThe two older object oriented systems in R are called S3 and S4, and the modern system is called RC which stands for “reference classes.” Programmers who are already familiar with object oriented programming will feel at home using RC.\nToday we are going to focus on S3 and S4, but we leave RC for you to review, if you wish.\nIn general, you should know and use S3 and this is also the OOP system that is most widely used in R. However, there are different R communities that prioritize other OOP systems (e.g. Bioconductor uses mostly S4).\nObject Oriented Principles\nOk let’s talk about some OOP principles. The first is is the idea of a class and an object.\nThe world is made up of physical objects - the chair you are sitting in, the clock next to your bed, the bus you ride every day, etc. Just like the world is full of physical objects, your programs can be made of objects as well.\nA class is a blueprint for an object: it describes the parts of an object, how to make an object, and what the object is able to do. If you were to think about a class for a bus (as in the public buses that roam the roads), this class would describe attributes for the bus like the number of seats on the bus, the number of windows, the color of the bus, the top speed of the bus, and the maximum distance the bus can drive on one tank of gas.\nBuses, in general, can perform the same actions, and these actions are also described in the class: a bus can open and close its doors, the bus can steer, and the accelerator or the brake can be used to slow down or speed up the bus. Each of these actions can be described as a method, which is a function that is associated with a particular class.\nWe will be using this class in order to create individual bus objects, so we should provide a constructor, which is a method where we can specify attributes of the bus as arguments. This constructor method will then return an individual bus object with the attributes that we specified.\nYou could also imagine that after making the bus class you might want to make a special kind of class for a party bus. Party buses have all of the same attributes and methods as our bus class, but they also have additional attributes and methods like the number of refrigerators, window blinds that can be opened and closed, and smoke machines that can be turned on and off.\nInstead of rewriting the entire bus class and then adding new attributes and methods, it is possible for the party bus class to inherit all of the attributes and methods from the bus class. In this framework of inheritance, we talk about the bus class as the super-class of the party bus, and the party bus is the sub-class of the bus. What this relationship means is that the party bus has all of the same attributes and methods as the bus class plus additional attributes and methods.\nS3\nConveniently everything in R is an object. By “everything” I mean every single “thing” in R including numbers, functions, strings, data frames, lists, etc.\nAnd while everything in R is an object, not everything is object-oriented.\nThis confusion arises because the base objects come from S, and were developed before anyone thought that S might need an OOP system. The tools and nomenclature evolved organically over many years without a single guiding principle.\nMost of the time, the distinction between objects and object-oriented objects is not important. But here we need to get into the nitty gritty details so we will use the terms base objects and OO objects to distinguish them.\n\n\n\nFigure 1: Everything in R is an object!\n\n\n\n[Source]\nTo tell the difference between a base and OO object, use is.object()\n\n\n# A base object:\nis.object(1:10)\n\n\n[1] FALSE\n\n# An OO object\nis.object(airquality)\n\n\n[1] TRUE\n\nTechnically, the difference between base and OO objects is that OO objects have a “class” attribute:\n\n\nattr(1:10, \"class\")\n\n\nNULL\n\nattr(airquality, \"class\")\n\n\n[1] \"data.frame\"\n\nThis can be slightly confusing, but important to note: you can find out the class of an object in R using the class() function, but this may or may not have a class attribute.\n\n\nclass(1:10)\n\n\n[1] \"integer\"\n\nclass(\"is in session.\")\n\n\n[1] \"character\"\n\nclass(class)\n\n\n[1] \"function\"\n\nBase types\nWhile only OO objects have a class attribute, every object has a base type:\n\n\ntypeof(1:10)\n\n\n[1] \"integer\"\n\ntypeof(mtcars)\n\n\n[1] \"list\"\n\nBase types do not form an OOP system because functions that behave differently for different base types are primarily written in C code that uses switch statements.\nThis means that only the R-core team can create new types, and creating a new type is a lot of work because every switch statement needs to be modified to handle a new case. As a consequence, new base types are rarely added. In total, there are 25 different base types.\nHere are two more base types we have already learned about:\n\n\ntypeof(1L)\n\n\n[1] \"integer\"\n\ntypeof(1i)\n\n\n[1] \"complex\"\n\nOO objects\nAt a high-level, an S3 object is a base type with at least a class attribute.\nFor example, take the factor. Its base type is the integer vector, it has a class attribute of “factor”, and a levels attribute that stores the possible levels:\n\n\nf <- factor(c(\"a\", \"b\", \"c\"))\n\ntypeof(f)\n\n\n[1] \"integer\"\n\nattributes(f)\n\n\n$levels\n[1] \"a\" \"b\" \"c\"\n\n$class\n[1] \"factor\"\n\nCool. Let’s try creating a new class in the S3 system.\nIn the S3 system you can arbitrarily assign a class to any object. Class assignments can be made using the structure() function, or you can assign the class using class() and <-:\n\n\nspecial_num_1 <- structure(1, class = \"special_number\")\nclass(special_num_1)\n\n\n[1] \"special_number\"\n\nspecial_num_2 <- 2\nclass(special_num_2)\n\n\n[1] \"numeric\"\n\nclass(special_num_2) <- \"special_number\"\nclass(special_num_2)\n\n\n[1] \"special_number\"\n\nAs crazy as this is, it is completely legal R code, but if you want to have a better behaved S3 class you should create a constructor which returns an S3 object. The shape_S3() function below is a constructor that returns a shape_S3 object:\n\n\nshape_s3 <- function(side_lengths){\n  structure(list(side_lengths = side_lengths), class = \"shape_S3\")\n}\n\nsquare_4 <- shape_s3(c(4, 4, 4, 4))\nclass(square_4)\n\n\n[1] \"shape_S3\"\n\ntriangle_3 <- shape_s3(c(3, 3, 3))\nclass(triangle_3)\n\n\n[1] \"shape_S3\"\n\nWe have now made two shape_S3 objects: square_4 and triangle_3, which are both instantiations of the shape_S3 class.\nImagine that you wanted to create a method (or function) that would return TRUE if a shape_S3 object was a square, FALSE if a shape_S3 object was not a square, and NA if the object provided as an argument to the method was not a shape_s3 object.\nThis can be achieved using R’s generic methods system. A generic method can return different values based depending on the class of its input.\nFor example mean() is a generic method that can find the average of a vector of number or it can find the “average day” from a vector of dates. The following snippet demonstrates this behavior:\n\n\nmean(c(2, 3, 7))\n\n\n[1] 4\n\nmean(c(as.Date(\"2016-09-01\"), as.Date(\"2016-09-03\")))\n\n\n[1] \"2016-09-02\"\n\nNow let’s create a generic method for identifying shape_S3 objects that are squares. The creation of every generic method uses the UseMethod() function in the following way with only slight variations:\n[name of method] <- function(x) UseMethod(\"[name of method]\")\nLet’s call this method is_square:\n\n\nis_square <- function(x) UseMethod(\"is_square\")\n\n\n\nNow we can add the actual function definition for detecting whether or not a shape is a square by specifying is_square.shape_S3. By putting a dot (.) and then the name of the class after is_square, we can create a method that associates is_square with the shape_S3 class:\n\n\nis_square.shape_S3 <- function(x){\n  length(x$side_lengths) == 4 &&\n    x$side_lengths[1] == x$side_lengths[2] &&\n    x$side_lengths[2] == x$side_lengths[3] &&\n    x$side_lengths[3] == x$side_lengths[4]\n}\n\nis_square(square_4)\n\n\n[1] TRUE\n\nis_square(triangle_3)\n\n\n[1] FALSE\n\nSeems to be working well! We also want is_square() to return NA when its argument is not a shape_S3. We can specify is_square.default as a last resort if there is not method associated with the object passed to is_square().\n\n\nis_square.default <- function(x){\n  NA\n}\n\nis_square(\"square\")\n\n\n[1] NA\n\nis_square(c(1, 1, 1, 1))\n\n\n[1] NA\n\nLet’s try printing square_4:\n\n\nprint(square_4)\n\n\n$side_lengths\n[1] 4 4 4 4\n\nattr(,\"class\")\n[1] \"shape_S3\"\n\nDoesn’t that look ugly? Lucky for us print() is a generic method, so we can specify a print method for the shape_S3 class:\n\n\nprint.shape_S3 <- function(x){\n  if(length(x$side_lengths) == 3){\n    paste(\"A triangle with side lengths of\", x$side_lengths[1], \n          x$side_lengths[2], \"and\", x$side_lengths[3])\n  } else if(length(x$side_lengths) == 4) {\n    if(is_square(x)){\n      paste(\"A square with four sides of length\", x$side_lengths[1])\n    } else {\n      paste(\"A quadrilateral with side lengths of\", x$side_lengths[1],\n            x$side_lengths[2], x$side_lengths[3], \"and\", x$side_lengths[4])\n    }\n  } else {\n    paste(\"A shape with\", length(x$side_lengths), \"sides.\")\n  }\n}\n\nprint(square_4)\n\n\n[1] \"A square with four sides of length 4\"\n\nprint(triangle_3)\n\n\n[1] \"A triangle with side lengths of 3 3 and 3\"\n\nprint(shape_s3(c(10, 10, 20, 20, 15)))\n\n\n[1] \"A shape with 5 sides.\"\n\nprint(shape_s3(c(2, 3, 4, 5)))\n\n\n[1] \"A quadrilateral with side lengths of 2 3 4 and 5\"\n\nSince printing an object to the console is one of the most common things to do in R, nearly every class has an associated print method! To see all of the methods associated with a generic like print() use the methods() function:\n\n\nhead(methods(print), 10)\n\n\n [1] \"print.acf\"     \"print.AES\"     \"print.anova\"   \"print.aov\"    \n [5] \"print.aovlist\" \"print.ar\"      \"print.Arima\"   \"print.arima0\" \n [9] \"print.AsIs\"    \"print.aspell\" \n\nOne last note on S3 with regard to inheritance. In the previous section we discussed how a sub-class can inherit attributes and methods from a super-class. Since you can assign any class to an object in S3, you can specify a super class for an object the same way you would specify a class for an object:\n\n\nclass(square_4)\n\n\n[1] \"shape_S3\"\n\nclass(square_4) <- c(\"shape_S3\", \"square\")\nclass(square_4)\n\n\n[1] \"shape_S3\" \"square\"  \n\nTo check if an object is a sub-class of a specified class you can use the inherits() function:\n\n\ninherits(square_4, \"square\")\n\n\n[1] TRUE\n\nExample: S3 Class/Methods for Polygons\nThe S3 system doesn’t have a formal way to define a class but typically, we use a list to define the class and elements of the list serve as data elements.\nHere is our definition of a polygon represented using Cartesian coordinates. The class contains an element called xcoord and ycoord for the x- and y-coordinates, respectively. The make_poly() function is the “constructor” function for polygon objects. It takes as arguments a numeric vector of x-coordinates and a corresponding numeric vector of y-coordinates.\n\n\n## Constructor function for polygon objects\n## x a numeric vector of x coordinates\n## y a numeric vector of y coordinates\nmake_poly <- function(x, y) {\n        if(length(x) != length(y))\n                stop(\"'x' and 'y' should be the same length\")\n        \n        ## Create the \"polygon\" object \n        object <- list(xcoord = x, ycoord = y)\n        \n        ## Set the class name\n        class(object) <- \"polygon\"\n        object\n}\n\n\n\nNow that we have a class definition, we can develop some methods for operating on objects from that class.\nThe first method that we will define is the print() method. The print() method should just show some simple information about the object and should not be too verbose—just enough information that the user knows what the object is.\nHere the print() method just shows the user how many vertices the polygon has. It is a convention for print() methods to return the object x invisibly using the invisible() function.\n\n\n## Print method for polygon objects\n## x an object of class \"polygon\"\n\nprint.polygon <- function(x, ...) {\n        cat(\"a polygon with\", length(x$xcoord), \n            \"vertices\\n\")\n        invisible(x)\n}\n\n\n\n\nPro tip: The invisible() function is useful when it is desired to have functions return values which can be assigned, but which do not print when they are not assigned.\n\n\n# These functions both return their argument\n\nf1 <- function(x) x\nf2 <- function(x) invisible(x)\n\nf1(1)  # prints\n\n\n[1] 1\n\nf2(1)  # does not\n\n\n\nHowever, when you assign the f2() function to an object, it does return the value\n\n\nz <- f2(1)\nz\n\n\n[1] 1\n\n\nNext is the summary() method. The summary() method typically shows a bit more information and may even do some calculations. This summary() method computes the ranges of the x- and y-coordinates.\nThe typical approach for summary() methods is to allow the summary method to compute something, but to not print something. The strategy is\nThe summary() method returns an object of class “summary_‘class name’”\nThere is a separate print() method for “summary_‘class name’” objects.\nFor example, here is the summary() method.\n\n\n## Summary method for polygon objects\n## object an object of class \"polygon\"\n\nsummary.polygon <- function(object, ...) {\n        object <- list(rng.x = range(object$xcoord),\n                       rng.y = range(object$ycoord))\n        class(object) <- \"summary_polygon\"\n        object\n}\n\n\n\nNote that it simply returns an object of class summary_polygon.\nNow the corresponding print() method:\n\n\n## Print method for summary.polygon objects\n## x an object of class \"summary_polygon\"\nprint.summary_polygon <- function(x, ...) {\n        cat(\"x:\", x$rng.x[1], \"-->\", x$rng.x[2], \"\\n\")\n        cat(\"y:\", x$rng.y[1], \"-->\", x$rng.y[2], \"\\n\")\n        invisible(x)\n}\n\n\n\nNow we can make use of our new class and methods.\n\n\n## Construct a new \"polygon\" object\nx <- make_poly(1:4, c(1, 5, 2, 1))\nattributes(x)\n\n\n$names\n[1] \"xcoord\" \"ycoord\"\n\n$class\n[1] \"polygon\"\n\nWe can use the print() to see what the object is.\n\n\nprint(x)\n\n\na polygon with 4 vertices\n\nAnd we can use the summary() method to get a bit more information about the object.\n\n\nout <- summary(x)\nclass(out)\n\n\n[1] \"summary_polygon\"\n\nprint(out)\n\n\nx: 1 --> 4 \ny: 1 --> 5 \n\nBecause of auto-printing we can just call the summary() method and let the results auto-print.\n\n\nsummary(x)\n\n\nx: 1 --> 4 \ny: 1 --> 5 \n\nFrom here, we could build other methods for interacting with our polygon object. For example, it may make sense to define a plot() method or maybe methods for intersecting two polygons together.\nS4\nThe S4 system is slightly more restrictive than S3, but it’s similar in many ways. To create a new class in S4 you need to use the setClass() function. You need to specify two or three arguments for this function: Class which is the name of the class as a string, slots, which is a named list of attributes for the class with the class of those attributes specified, and optionally contains which includes the super-class of they class you are specifying (if there is a super-class). Take look at the class definition for a bus_S4 and a party_bus_S4 below:\n\n\n\n\n\nsetClass(Class = \"bus_S4\",\n         slots = list(n_seats = \"numeric\", \n                      top_speed = \"numeric\",\n                      current_speed = \"numeric\",\n                      brand = \"character\"))\nsetClass(Class = \"party_bus_S4\",\n         slots = list(n_subwoofers = \"numeric\",\n                      smoke_machine_on = \"logical\"),\n         contains = \"bus_S4\")\n\n\n\nNow that we have created the bus_S4 and the party_bus_S4 classes we can create bus objects using the new() function. The new() function’s arguments are the name of the class and values for each “slot” in our S4 object.\n\n\nmy_bus <- new(\"bus_S4\", n_seats = 20, top_speed = 80, \n              current_speed = 0, brand = \"Volvo\")\nmy_bus\n\n\nAn object of class \"bus_S4\"\nSlot \"n_seats\":\n[1] 20\n\nSlot \"top_speed\":\n[1] 80\n\nSlot \"current_speed\":\n[1] 0\n\nSlot \"brand\":\n[1] \"Volvo\"\n\nmy_party_bus <- new(\"party_bus_S4\", n_seats = 10, top_speed = 100,\n                    current_speed = 0, brand = \"Mercedes-Benz\", \n                    n_subwoofers = 2, smoke_machine_on = FALSE)\nmy_party_bus\n\n\nAn object of class \"party_bus_S4\"\nSlot \"n_subwoofers\":\n[1] 2\n\nSlot \"smoke_machine_on\":\n[1] FALSE\n\nSlot \"n_seats\":\n[1] 10\n\nSlot \"top_speed\":\n[1] 100\n\nSlot \"current_speed\":\n[1] 0\n\nSlot \"brand\":\n[1] \"Mercedes-Benz\"\n\nYou can use the @ operator to access the slots of an S4 object:\n\n\nmy_bus@n_seats\n\n\n[1] 20\n\nmy_party_bus@top_speed\n\n\n[1] 100\n\nThis is essentially the same as using the $ operator with a list or an environment.\nS4 classes use a generic method system that is similar to S3 classes. In order to implement a new generic method you need to use the setGeneric() function and the standardGeneric() function in the following way:\nsetGeneric(\"new_generic\", function(x){\n  standardGeneric(\"new_generic\")\n})\nLet’s create a generic function called is_bus_moving() to see if a bus_S4 object is in motion:\n\n\nsetGeneric(\"is_bus_moving\", function(x){\n  standardGeneric(\"is_bus_moving\")\n})\n\n\n[1] \"is_bus_moving\"\n\nNow we need to actually define the function, which we can to with setMethod(). The setMethod() functions takes as arguments the name of the method as a string (or f), the method signature (signature), which specifies the class of each argument for the method, and then the function definition of the method:\n\n\nsetMethod(f = \"is_bus_moving\",\n          signature = c(x = \"bus_S4\"),\n          definition = function(x){\n                          x@current_speed > 0\n                      }\n          )\n\nis_bus_moving(my_bus)\n\n\n[1] FALSE\n\nmy_bus@current_speed <- 1\nis_bus_moving(my_bus)\n\n\n[1] TRUE\n\nIn addition to creating your own generic methods, you can also create a method for your new class from an existing generic.\nFirst, use the setGeneric() function with the name of the existing method you want to use with your class, and then use the setMethod() function like in the previous example. Let’s make a print() method for the bus_S4 class:\n\n\nsetGeneric(\"print\")\n\n\n[1] \"print\"\n\nsetMethod(f = \"print\",\n          signature = c(x = \"bus_S4\"),\n          definition = function(x){\n            paste(\"This\", x@brand, \"bus is traveling at a speed of\", x@current_speed)\n          })\n\nprint(my_bus)\n\n\n[1] \"This Volvo bus is traveling at a speed of 1\"\n\nprint(my_party_bus)\n\n\n[1] \"This Mercedes-Benz bus is traveling at a speed of 0\"\n\nReference Classes\n\nClick here to learn about reference classes (RC).\nWith reference classes we leave the world of R’s old object oriented systems and enter the philosophies of other prominent object oriented programming languages. We can use the setRefClass() function to define a class’ fields, methods, and super-classes. Let’s make a reference class that represents a student:\n\n\nStudent <- setRefClass(\"Student\",\n                      fields = list(name = \"character\",\n                                    grad_year = \"numeric\",\n                                    credits = \"numeric\",\n                                    id = \"character\",\n                                    courses = \"list\"),\n                      methods = list(\n                        hello = function(){\n                          paste(\"Hi! My name is\", name)\n                        },\n                        add_credits = function(n){\n                          credits <<- credits + n\n                        },\n                        get_email = function(){\n                          paste0(id, \"@jhu.edu\")\n                        }\n                      ))\n\n\n\nTo recap: we have created a class definition called Student, which defines the student class. This class has five fields and three methods. To create a Student object use the new() method:\n\n\nbrooke <- Student$new(name = \"Brooke\", grad_year = 2019, credits = 40,\n                    id = \"ba123\", courses = list(\"Ecology\", \"Calculus III\"))\nstephanie <- Student$new(name = \"Stephanie\", grad_year = 2021, credits = 10,\n                    id = \"shicks456\", courses = list(\"Puppetry\", \"Elementary Algebra\"))\n\n\n\nYou can access the fields and methods of each object using the $ operator:\n\n\nbrooke$credits\n\n\n[1] 40\n\nstephanie$hello()\n\n\n[1] \"Hi! My name is Stephanie\"\n\nstephanie$get_email()\n\n\n[1] \"shicks456@jhu.edu\"\n\nMethods can change the state of an object, for instance in the case of the add_credits() function:\n\n\nbrooke$credits\n\n\n[1] 40\n\nbrooke$add_credits(4)\nbrooke$credits\n\n\n[1] 44\n\nNotice that the add_credits() method uses the complex assignment operator (<<-). You need to use this operator if you want to modify one of the fields of an object with a method. You’ll learn more about this operator in the Expressions & Environments section.\nReference classes can inherit from other classes by specifying the contains argument when they’re defined. Let’s create a sub-class of Student called Grad_Student which includes a few extra features:\n\n\nGrad_Student <- setRefClass(\"Grad_Student\",\n                            contains = \"Student\",\n                            fields = list(thesis_topic = \"character\"),\n                            methods = list(\n                              defend = function(){\n                                paste0(thesis_topic, \". QED.\")\n                              }\n                            ))\n\njeff <- Grad_Student$new(name = \"Jeff\", grad_year = 2021, credits = 8,\n                    id = \"jl55\", courses = list(\"Fitbit Repair\", \n                                                \"Advanced Base Graphics\"),\n                    thesis_topic = \"Batch Effects\")\n\njeff$defend()\n\n\n[1] \"Batch Effects. QED.\"\n\nSummary\nR has three object oriented systems: S3, S4, and Reference Classes.\nReference Classes are the most similar to classes and objects in other programming languages.\nClasses are blueprints for an object.\nObjects are individual instances of a class.\nMethods are functions that are associated with a particular class.\nConstructors are methods that create objects.\nEverything in R is an object.\nS3 is a liberal object oriented system that allows you to assign a class to any object.\nS4 is a more strict object oriented system that build upon ideas in S3.\nReference Classes are a modern object oriented system that is similar to Java, C++, Python, or Ruby.\nPost-lecture materials\nAdditional Resources\n\nhttps://adv-r.hadley.nz/oo.html\nhttps://rdpeng.github.io/Biostat776/lecture-object-oriented-programming.html\n\n\n\n\n",
    "preview": "https://d33wubrfki0l68.cloudfront.net/5d6b4926530f3da70f16b54dcdf1a55eb8fa3d71/703e0/diagrams/oo-venn.png",
    "last_modified": "2021-09-30T12:49:07-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-28-building-r-packages/",
    "title": "Building R Packages",
    "description": "Introduction to building and documenting R packages.",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-28",
    "categories": [
      "module 2",
      "week 5",
      "programming",
      "R",
      "R package",
      "RStudio",
      "functions",
      "data viz"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nPrerequisites\nAcknowledgements\n\nLearning objectives\nIntroduction\nCreate a New R Package Project\nConfigure Build Tools\nR Package Files\n\nEdit main files\nAdd a R Script file\nDocumentation\nEditing the DESCRIPTION file\n\nBuild and Install\nWithin RStudio\nBuild Source Package\n\nPost-lecture materials\nAdditional Resources\n\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://r-pkgs.org\nhttps://stat545.com/package-overview.html\n\nPrerequisites\nBefore starting you must install two additional packages:\ndevtools - this provides many additional tools for building packages\nroxygen2 - this provides tools for writing documentation\nYou can do this by calling\n\n\ninstall.packages(c(\"devtools\", \"roxygen2\"))\n\n\n\nor use the “Install Packages…” option from the “Tools” menu in RStudio.\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-building-r-packages.html\nLearning objectives\n\nAt the end of this lesson you will:\nCreate an empty R package within RStudio\nDesign a R function and write documentation\nDescribe what a DESCRIPTION file is and what goes in it\nBe able to build and install a R package\n\nIntroduction\nThis lesson will cover how to build R packages using R and RStudio. Using RStudio for this lesson will be critical as RStudio includes a number of tools that make building R packages much simpler.\nFor the purposes of demonstration in this lesson, we will be building a package called greetings that has a single function called hello(). The hello() function takes a single argument called name (which is required) and makes a plot containing a message directed at name. For example,\n\n\nlibrary(greetings)\nhello(\"Stephanie\")\n\n\n\n\nAdmittedly, this is not a useful package, but it allows us to demonstrate all of the necessary ingredients for building a simple R package.\nCreate a New R Package Project\nCreating a new R packages begins with creating a new RStudio project.\n\n\n\nFigure 1: A screenshot of starting a New Project in R\n\n\n\nYou should choose New Directory as this will be a brand new project (not an existing project).\n\n\n\nFigure 2: A screenshot of starting a New Directory for a New Project in R\n\n\n\nNext, you should choose the “Project Type” as R Package using devtools (you may need to scroll down a little in that menu).\n\n\n\nFigure 3: Create a R package using devtools\n\n\n\nFinally, you should specify the name of your package. For this example, we will use greetings as the name of the package. Also you should double check the name of the sub-directory listed under “Create project as subdirectory of” is a directory that you can find.\n\nImportant!!: the name of this directory should not have any spaces in its name.\n\n\n\n\nFigure 4: Package name\n\n\n\nClick “Create Project” and allow R and RStudio to restart. You should get a brand new session. You will also see a window with a series of tabs. One of those tabs will be called Build and that will be important as we build our package.\n\n\n\nFigure 5: Build menu\n\n\n\nConfigure Build Tools\nThe next step after creating a new project is to configure your build tools. Click on the Build tab and then More and then Configure Build Tools….\n\n\n\nFigure 6: Configure Build tools\n\n\n\nIn the next screen, you should make sure that the check box for Generate documentation with Roxygen is checked. Then click the Configure… button.\n\n\n\nFigure 7: Click the Configure button\n\n\n\nIn the next menu, make sure to check the check box for Install and Restart.\n\n\n\nFigure 8: Check box for Install and Restart\n\n\n\nThen click “OK” and then “OK” again to exit the options menu.\nR Package Files\nIn this session, there will be the following files listed in the file browser.\n\n\n\nFigure 9: Files are listed in the File browser\n\n\n\nThe files we will focus on here are\nthe DESCRIPTION file; and\nany files in the R sub-directory. This package will only have one R script in the R sub-directory.\nThere is no need to worry about the other files for now.\nEdit main files\nNow, we need to write the R code and documentation for our one function in this package.\nAdd a R Script file\nFirst, create an R script in which the R code will go. You can do this by clicking on File > New File > R Script.\n\n\n\nFigure 10: Saving a new R Script file called hello.R\n\n\n\n\nNote: make sure that your R script is saved inside the R/ sub-directory.\n\n\n\n\nFigure 11: R Script file must be in the R/ sub-directory\n\n\n\nNext, once you have your R Script created, you can start to write the function and the documentation.\n\nNote: the idea is that when you write a function in a R package, just above the function is the documentation. The function is written in the usual way and the documentation is written using a special style.\n\nDocumentation\nLet’s start with the documentation. Here is the documentation for the hello() function.\n#' Print a Greeting\n#'\n#' Print a greeting for a custom name\n#'\n#' @details This function make a plot with a greeting to the name passed as an argument to the function\n#' \n#' @param name character, name of person to whom greeting should be directed\n#'\n#' @return nothing useful is returned.\n#'\n#' @import ggplot2\n#' @export\n#'\n#' @examples\n#' hello(\"Chris\")\n#'\nWe will take each line of documentation in order:\nThe first line is a short title for the function\nThe next line is the “description” line and should be a slightly longer description of what the function does. Generally, this line is one sentence.\nThis line contains the first Roxygen directive, which is @details. This directive indicates that the text that comes afterwards has detailed information about the function.\nThe next Roxygen directive is the @param directive. This indicates the name of the parameter that the function will accept. In this case, this is the name to which the greeting will be directed.\nThe @return directive indicates what the function returns to the user. This function does not return anything useful, but it is still useful to indicate that.\nThis function requires the ggplot() function and associated plotting functions. Therefore we need to use the @import directive to indicate that we need to import all of the functions in the ggplot2 package.\nWe want to indicate with the @export directive that this function should be visible to the user (i.e. we want the user to call this function). Therefore, the function should be exported to the user. More complex packages may have many functions and not all of them will be functions that the user will need to call. In addition, any function that is exported is required to have documentation.\nUnder the @examples directive, you can put R code that demonstrates how to call the function. Here, we provide a simple example of how to use the hello() function.\nOnce the documentation is written, we can write the code for the function itself. The complete R script file looks as follows.\n#' Print a Greeting\n#'\n#' Print a greeting for a custom name\n#'\n#' @details This function make a plot with a greeting to the name passed as an argument to the function\n\n#' @param name character, name of person to whom greeting should be directed\n#'\n#' @return nothing useful is returned.\n#'\n#' @import ggplot2\n#' @export\n#'\n#' @examples\n#' hello(\"Chris\")\n#'\nhello <- function(name) {\n        message <- paste0(\"Hello, \", name, \"!\")\n        ggplot() +\n                geom_text(aes(0, 0), label = message, size = 4) +\n                theme_minimal()\n\n}\n\nNote: in the function we do not actually plot any data. We just use the ggplot() function to setup a plot window so that we can add the message using geom_text().\n\nEditing the DESCRIPTION file\nAfter writing the code and documentation we need to edit the DESCRIPTION file for the package. This contains metadata about the package. Here is the final DESCRIPTION file for the package.\nPackage: greetings\nTitle: Displays a greeting plot\nVersion: 1.0\nAuthors@R: \n    person(given = \"Stephanie\",\n           family = \"Hicks\",\n           role = c(\"aut\", \"cre\"),\n           email = \"shicks19@jhu.edu\")\nDescription: This package displays a nice greeting for a custom name.\nImports: ggplot2\nLicense: GPL (>= 3)\nEncoding: UTF-8\nLazyData: true\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.1.1\nWe can go through each field one at time:\nPackage is just the name of the package. In this case it is greetings.\nTitle is a short description of the package.\nVersion is the version number. This is the first version so we use 1.0.\nAuthors@R indicates the author of the package (this is you!). This package only has one author but packages can have multiple authors. Look at the help file for person() to see how this is specified.\nDescription provides a multi-sentence description of what the package does.\nImports is only needed because the package imports the functions from the ggplot2 package. You will need to add this line explicitly to the DESCRIPTION file.\nLicense indicates the legal license for the package. This should be an open source license and we use the GNU General Public License Version 3 here. You can read more about R package licenses. Every R package must have a license.\nThe remaining fields are auto-generated by RStudio and you don’t need to worry about them for now.\nBuild and Install\nOnce you have the code, documentation, and DESCRIPTION file written, you can build the package and install it in order to try it out.\nWithin RStudio\nIn the Build tab, click the button labeled Install and Restart.\n\n\n\nFigure 12: Click the Build tab to install and restart\n\n\n\nClicking this button will\nBuild the R package\nInstall the R package on your system\nRestart the R session\nLoad your package using the library() function.\nOnce this is done, you can call the hello() function and see the results.\n\n\n\nFigure 13: Now you can load and use the hello() function\n\n\n\nBuild Source Package\nOnce the package is completed, you must build a source package so that it can be distributed to others. This can be done in the Build menu and clicking Build source package.\n\n\n\nFigure 14: Build the source package\n\n\n\nThis will produce a file with a .tar.gz extension. This is the package source file.\nYou should see a screen that looks something like this.\n\n\n\nFigure 15: Source file is built (file ends in a .tar.gz)\n\n\n\nOnce your package is built, you can send to others and they will be able to install it. The package source file would also be the file that would be uploaded to CRAN if you were submitting a package to CRAN.\n\nPro tip: if you are interested, you can also use the usethis package to create, build, document, and intall a R package:\nhttps://usethis.r-lib.org\nFWIW, this is how I create R packages.\n\nPost-lecture materials\nAdditional Resources\n\nhttps://rdpeng.github.io/Biostat776/lecture-building-r-packages.html\nhttps://r-pkgs.org\nhttps://stat545.com/package-overview.html\nhttps://usethis.r-lib.org\n\n\n\n\n",
    "preview": "posts/2021-09-28-building-r-packages/building-r-packages_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-09-26T22:48:15-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-09-23-debugging-r-code/",
    "title": "Debugging R Code",
    "description": "Help! What's wrong with my code???",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-23",
    "categories": [
      "module 2",
      "week 4",
      "programming",
      "debugging"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nDebugging R Code\nOverall approach\nSomething’s Wrong!\n\nDebugging Tools in R\nUsing traceback()\nUsing debug()\nUsing recover()\n\nSummary\nPost-lecture materials\nFinal Questions\nAdditional Resources\n\n\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://adv-r.hadley.nz/debugging.html\nhttps://rstats.wtf/debugging-r-code.html\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-debugging-r-code.html\nhttps://adv-r.hadley.nz/debugging.html\nLearning objectives\n\nAt the end of this lesson you will:\nDiscuss an overall approach to debugging code in R\nRecognize the three main indications of a problem/condition (message, warning, error) and a fatal problem (error)\nUnderstand the importance of reproducing the problem when debugging a function or piece of code\nLearn how to use interactive debugging tools traceback, debug, recover, browser, and trace can be used to find problematic code in functions\n\nDebugging R Code\nOverall approach\nFinding the root cause of a problem is always challenging. Most bugs are subtle and hard to find because if they were obvious, you would have avoided them in the first place. A good strategy helps. Below I outline a four step process that I have found useful:\nGoogle!\nWhenever you see an error message, start by googling it. If you are lucky, you will discover that it’s a common error with a known solution. When googling, improve your chances of a good match by removing any variable names or values that are specific to your problem.\nMake it repeatable\nTo find the root cause of an error, you are going to need to execute the code many times as you consider and reject hypotheses. To make that iteration as quick possible, it’s worth some upfront investment to make the problem both easy and fast to reproduce.\nStart by creating a reproducible example (reprex). This will help others help you, and often leads to a solution without asking others, because in the course of making the problem reproducible you often figure out the root cause. Next, make the example minimal by removing code and simplifying data. As you do this, you may discover inputs that do not trigger the error. Make note of them: they will be helpful when diagnosing the root cause.\nFigure out where it is\nIt’s a great idea to adopt the scientific method here. Generate hypotheses, design experiments to test them, and record your results. This may seem like a lot of work, but a systematic approach will end up saving you time. Often a lot of time can be wasted relying on my intuition to solve a bug (“oh, it must be an off-by-one error, so I’ll just subtract 1 here”), when I would have been better off taking a systematic approach.\nIf this fails, you might need to ask help from someone else. If you have followed the previous step, you will have a small example that is easy to share with others. That makes it much easier for other people to look at the problem, and more likely to help you find a solution.\nFix it and test it\nOnce you have found the bug, you need to figure out how to fix it and to check that the fix actually worked. Again, it is very useful to have automated tests in place. Not only does this help to ensure that you have actually fixed the bug, it also helps to ensure you have not introduced any new bugs in the process. In the absence of automated tests, make sure to carefully record the correct output, and check against the inputs that previously failed.\nSomething’s Wrong!\nOnce you have made the error repeatable, the next step is to figure out where it comes from.\nR has a number of ways to indicate to you that something is not right. There are different levels of indication that can be used, ranging from mere notification to fatal error. Executing any function in R may result in the following conditions.\nmessage: A generic notification/diagnostic message produced by the message() function; execution of the function continues\nwarning: An indication that something is wrong but not necessarily fatal; execution of the function continues. Warnings are generated by the warning() function\nerror: An indication that a fatal problem has occurred and execution of the function stops. Errors are produced by the stop() function.\ncondition: A generic concept for indicating that something unexpected has occurred; programmers can create their own custom conditions if they want.\nHere is an example of a warning that you might receive in the course of using R.\n\n\nlog(-1)\n\n\nWarning in log(-1): NaNs produced\n[1] NaN\n\nThis warning lets you know that taking the log of a negative number results in a NaN value because you can’t take the log of negative numbers. Nevertheless, R doesn’t give an error, because it has a useful value that it can return, the NaN value. The warning is just there to let you know that something unexpected happen. Depending on what you are programming, you may have intentionally taken the log of a negative number in order to move on to another section of code.\nHere is another function that is designed to print a message to the console depending on the nature of its input.\n\n\nprintmessage <- function(x) {\n        if(x > 0)\n                print(\"x is greater than zero\")\n        else\n                print(\"x is less than or equal to zero\")\n        invisible(x)        \n}\n\n\n\nThis function is simple—it prints a message telling you whether x is greater than zero or less than or equal to zero. It also returns its input invisibly, which is a common practice with “print” functions. Returning an object invisibly means that the return value does not get auto-printed when the function is called.\nTake a hard look at the function above and see if you can identify any bugs or problems.\nWe can execute the function as follows.\n\n\nprintmessage(1)\n\n\n[1] \"x is greater than zero\"\n\nThe function seems to work fine at this point. No errors, warnings, or messages.\n\n\nprintmessage(NA)\n\n\nError in if (x > 0) print(\"x is greater than zero\") else print(\"x is less than or equal to zero\"): missing value where TRUE/FALSE needed\n\nWhat happened?\nWell, the first thing the function does is test if x > 0. But you can’t do that test if x is a NA or NaN value. R doesn’t know what to do in this case so it stops with a fatal error.\nWe can fix this problem by anticipating the possibility of NA values and checking to see if the input is NA with the is.na() function.\n\n\nprintmessage2 <- function(x) {\n        if(is.na(x))\n                print(\"x is a missing value!\")\n        else if(x > 0)\n                print(\"x is greater than zero\")\n        else\n                print(\"x is less than or equal to zero\")\n        invisible(x)\n}\n\n\n\nNow we can run the following.\n\n\nprintmessage2(NA)\n\n\n[1] \"x is a missing value!\"\n\nAnd all is fine.\nNow what about the following situation.\n\n\nx <- log(c(-1, 2))\n\n\nWarning in log(c(-1, 2)): NaNs produced\n\nprintmessage2(x)\n\n\nWarning in if (is.na(x)) print(\"x is a missing value!\") else if (x >\n0) print(\"x is greater than zero\") else print(\"x is less than or equal\nto zero\"): the condition has length > 1 and only the first element\nwill be used\n[1] \"x is a missing value!\"\n\nNow what?? Why are we getting this warning? The warning says “the condition has length > 1 and only the first element will be used”.\nThe problem here is that I passed printmessage2() a vector x that was of length 2 rather then length 1. Inside the body of printmessage2() the expression is.na(x) returns a vector that is tested in the if statement. However, if cannot take vector arguments so you get a warning. The fundamental problem here is that printmessage2() is not vectorized.\nWe can solve this problem two ways. One is by simply not allowing vector arguments. The other way is to vectorize the printmessage2() function to allow it to take vector arguments.\nFor the first way, we simply need to check the length of the input.\n\n\nprintmessage3 <- function(x) {\n        if(length(x) > 1L)\n                stop(\"'x' has length > 1\")\n        if(is.na(x))\n                print(\"x is a missing value!\")\n        else if(x > 0)\n                print(\"x is greater than zero\")\n        else\n                print(\"x is less than or equal to zero\")\n        invisible(x)\n}\n\n\n\nNow when we pass printmessage3() a vector we should get an error.\n\n\nprintmessage3(1:2)\n\n\nError in printmessage3(1:2): 'x' has length > 1\n\nVectorizing the function can be accomplished easily with the Vectorize() function.\n\n\nprintmessage4 <- Vectorize(printmessage2)\nout <- printmessage4(c(-1, 2))\n\n\n[1] \"x is less than or equal to zero\"\n[1] \"x is greater than zero\"\n\nYou can see now that the correct messages are printed without any warning or error. Note that I stored the return value of printmessage4() in a separate R object called out. This is because when I use the Vectorize() function it no longer preserves the invisibility of the return value.\n\nHelpful tips\nThe primary task of debugging any R code is correctly diagnosing what the problem is. When diagnosing a problem with your code (or somebody else’s), it’s important first understand what you were expecting to occur. Then you need to idenfity what did occur and how did it deviate from your expectations. Some basic questions you need to ask are\nWhat was your input? How did you call the function?\nWhat were you expecting? Output, messages, other results?\nWhat did you get?\nHow does what you get differ from what you were expecting?\nWere your expectations correct in the first place?\nCan you reproduce the problem (exactly)?\nBeing able to answer these questions is important not just for your own sake, but in situations where you may need to ask someone else for help with debugging the problem. Seasoned programmers will be asking you these exact questions.\n\nDebugging Tools in R\nR provides a number of tools to help you with debugging your code. The primary tools for debugging functions in R are\ntraceback(): prints out the function call stack after an error occurs; does nothing if there’s no error\ndebug(): flags a function for “debug” mode which allows you to step through execution of a function one line at a time\nbrowser(): suspends the execution of a function wherever it is called and puts the function in debug mode\ntrace(): allows you to insert debugging code into a function at specific places\nrecover(): allows you to modify the error behavior so that you can browse the function call stack\nThese functions are interactive tools specifically designed to allow you to pick through a function. There is also the more blunt technique of inserting print() or cat() statements in the function.\nUsing traceback()\nThe traceback() function prints out the function call stack after an error has occurred. The function call stack is the sequence of functions that was called before the error occurred.\nFor example, you may have a function a() which subsequently calls function b() which calls c() and then d(). If an error occurs, it may not be immediately clear in which function the error occurred. The traceback() function shows you how many levels deep you were when the error occurred.\n> mean(x)\nError in mean(x) : object 'x' not found\n> traceback()\n1: mean(x)\nHere, it’s clear that the error occurred inside the mean() function because the object x does not exist.\nThe traceback() function must be called immediately after an error occurs. Once another function is called, you lose the traceback.\nHere is a slightly more complicated example using the lm() function for linear modeling.\n> lm(y ~ x)\nError in eval(expr, envir, enclos) : object ’y’ not found\n> traceback()\n7: eval(expr, envir, enclos)\n6: eval(predvars, data, env)\n5: model.frame.default(formula = y ~ x, drop.unused.levels = TRUE)\n4: model.frame(formula = y ~ x, drop.unused.levels = TRUE)\n3: eval(expr, envir, enclos)\n2: eval(mf, parent.frame())\n1: lm(y ~ x)\nYou can see now that the error did not get thrown until the 7th level of the function call stack, in which case the eval() function tried to evaluate the formula y ~ x and realized the object y did not exist.\nLooking at the traceback is useful for figuring out roughly where an error occurred but it’s not useful for more detailed debugging. For that you might turn to the debug() function.\nUsing debug()\n\nClick here for how to use debug() with an interactive browser.\nThe debug() function initiates an interactive debugger (also known as the “browser” in R) for a function. With the debugger, you can step through an R function one expression at a time to pinpoint exactly where an error occurs.\nThe debug() function takes a function as its first argument. Here is an example of debugging the lm() function.\n> debug(lm)      ## Flag the 'lm()' function for interactive debugging\n> lm(y ~ x)\ndebugging in: lm(y ~ x)\ndebug: {\n    ret.x <- x\n    ret.y <- y\n    cl <- match.call()\n    ...\n    if (!qr)\n        z$qr <- NULL \n    z\n} \nBrowse[2]>\nNow, every time you call the lm() function it will launch the interactive debugger. To turn this behavior off you need to call the undebug() function.\nThe debugger calls the browser at the very top level of the function body. From there you can step through each expression in the body. There are a few special commands you can call in the browser:\nn executes the current expression and moves to the next expression\nc continues execution of the function and does not stop until either an error or the function exits\nQ quits the browser\nHere’s an example of a browser session with the lm() function.\nBrowse[2]> n   ## Evalute this expression and move to the next one\ndebug: ret.x <- x\nBrowse[2]> n\ndebug: ret.y <- y\nBrowse[2]> n\ndebug: cl <- match.call()\nBrowse[2]> n\ndebug: mf <- match.call(expand.dots = FALSE)\nBrowse[2]> n\ndebug: m <- match(c(\"formula\", \"data\", \"subset\", \"weights\", \"na.action\",\n    \"offset\"), names(mf), 0L)\nWhile you are in the browser you can execute any other R function that might be available to you in a regular session. In particular, you can use ls() to see what is in your current environment (the function environment) and print() to print out the values of R objects in the function environment.\nYou can turn off interactive debugging with the undebug() function.\nundebug(lm)    ## Unflag the 'lm()' function for debugging\nUsing recover()\n\nClick here for how to use recover() with an interactive browser.\nThe recover() function can be used to modify the error behavior of R when an error occurs. Normally, when an error occurs in a function, R will print out an error message, exit out of the function, and return you to your workspace to await further commands.\nWith recover() you can tell R that when an error occurs, it should halt execution at the exact point at which the error occurred. That can give you the opportunity to poke around in the environment in which the error occurred. This can be useful to see if there are any R objects or data that have been corrupted or mistakenly modified.\n> options(error = recover)    ## Change default R error behavior\n> read.csv(\"nosuchfile\")      ## This code doesn't work\nError in file(file, \"rt\") : cannot open the connection\nIn addition: Warning message:\nIn file(file, \"rt\") :\n  cannot open file ’nosuchfile’: No such file or directory\n  \nEnter a frame number, or 0 to exit\n\n1: read.csv(\"nosuchfile\")\n2: read.table(file = file, header = header, sep = sep, quote = quote, dec =\n3: file(file, \"rt\")\n\nSelection:\nThe recover() function will first print out the function call stack when an error occurrs. Then, you can choose to jump around the call stack and investigate the problem. When you choose a frame number, you will be put in the browser (just like the interactive debugger triggered with debug()) and will have the ability to poke around.\nSummary\nThere are three main indications of a problem/condition: message, warning, error; only an error is fatal\nWhen analyzing a function with a problem, make sure you can reproduce the problem, clearly state your expectations and how the output differs from your expectation\nInteractive debugging tools traceback, debug, recover, browser, and trace can be used to find problematic code in functions\nDebugging tools are not a substitute for thinking!\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nTry using traceback() to debug this piece of code:\n\n\nf <- function(a) g(a)\ng <- function(b) h(b)\nh <- function(c) i(c)\ni <- function(d) {\n  if (!is.numeric(d)) {\n    stop(\"`d` must be numeric\", call. = FALSE)\n  }\n  d + 10\n}\nf(\"a\")\n\n\nError: `d` must be numeric\n\nDescribe in words what is happening above?\n\nAdditional Resources\n\nhttps://rdpeng.github.io/Biostat776/lecture-debugging-r-code.html\nhttps://adv-r.hadley.nz/debugging.html\nhttps://rstats.wtf/debugging-r-code.html\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-28T12:45:20-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-23-error-handling-and-generation/",
    "title": "Error Handling and Generation",
    "description": "Implement exception handling routines in R functions",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-23",
    "categories": [
      "module 2",
      "week 4",
      "programming",
      "debugging"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nError Handling and Generation\nWhat is an error?\nGenerating Errors\nWhen to generate errors or warnings\nHow should errors be handled?\n\nSummary\nPost-lecture materials\nAdditional Resources\n\n\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://adv-r.hadley.nz/debugging.html\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-error-handling-and-generation.html\nhttps://adv-r.hadley.nz/debugging.html\nLearning objectives\n\nAt the end of this lesson you will:\nCreate errors, warnings, and messages in R functions using the functions stop, stopifnot, warning, and message.\nUnderstand the importance of providing useful error messaging to improve user experience with functions. However, these can also slow down code substantially.\n\nError Handling and Generation\nWhat is an error?\nErrors most often occur when code is used in a way that it is not intended to be used. For example adding two strings together produces the following error:\n\n\n\"hello\" + \"world\"\n\n\nError in \"hello\" + \"world\": non-numeric argument to binary operator\n\nThe + operator is essentially a function that takes two numbers as arguments and finds their sum. Since neither \"hello\" nor \"world\" are numbers, the R interpreter produces an error. Errors will stop the execution of your program, and they will (hopefully) print an error message to the R console.\nIn R there are two other constructs which are related to errors: warnings and messages. Warnings are meant to indicate that something seems to have gone wrong in your program that should be inspected. Here’s a simple example of a warning being generated:\n\n\nas.numeric(c(\"5\", \"6\", \"seven\"))\n\n\nWarning: NAs introduced by coercion\n[1]  5  6 NA\n\nThe as.numeric() function attempts to convert each string in c(\"5\", \"6\", \"seven\") into a number, however it is impossible to convert \"seven\", so a warning is generated. Execution of the code is not halted, and an NA is produced for \"seven\" instead of a number.\nMessages simply print to the R console, though they are generated by an underlying mechanism that is similar to how errors and warning are generated. Here’s a small function that will generate a message:\n\n\nf <- function(){\n  message(\"This is a message.\")\n}\n\nf()\n\n\nThis is a message.\n\nGenerating Errors\nThere are a few essential functions for generating errors, warnings, and messages in R. The stop() function will generate an error. Let’s generate an error:\n\n\nstop(\"Something erroneous has occurred!\")\n\n\n\n\nError: Something erroneous has occurred!\n\nIf an error occurs inside of a function then the name of that function will appear in the error message:\n\n\nname_of_function <- function(){\n  stop(\"Something bad happened.\")\n}\n\nname_of_function()\n\n\nError in name_of_function(): Something bad happened.\n\nThe stopifnot() function takes a series of logical expressions as arguments and if any of them are false an error is generated specifying which expression is false. Let’s take a look at an example:\n\n\nerror_if_n_is_greater_than_zero <- function(n){\n  stopifnot(n <= 0)\n  n\n}\n\nerror_if_n_is_greater_than_zero(5)\n\n\nError in error_if_n_is_greater_than_zero(5): n <= 0 is not TRUE\n\nThe warning() function creates a warning, and the function itself is very similar to the stop() function. Remember that a warning does not stop the execution of a program (unlike an error.)\n\n\nwarning(\"Consider yourself warned!\")\n\n\nWarning: Consider yourself warned!\n\nJust like errors, a warning generated inside of a function will include the name of the function in which it was generated:\n\n\nmake_NA <- function(x){\n  warning(\"Generating an NA.\")\n  NA\n}\n\nmake_NA(\"Sodium\")\n\n\nWarning in make_NA(\"Sodium\"): Generating an NA.\n[1] NA\n\nMessages are simpler than errors or warnings; they just print strings to the R console. You can issue a message with the message() function:\n\n\nmessage(\"In a bottle.\")\n\n\nIn a bottle.\n\nWhen to generate errors or warnings\nStopping the execution of your program with stop() should only happen in the event of a catastrophe - meaning only if it is impossible for your program to continue. If there are conditions that you can anticipate that would cause your program to create an error, then you should document those conditions so whoever uses your software is aware. An example includes:\nProviding invalid arguments to a function. You could check this at the beginning of your program using stopifnot() so that the user can quickly realize something has gone wrong.\nYou can think of a function as kind of contract between you and the user:\nif the user provides specified arguments, your program will provide predictable results. Of course it’s impossible for you to anticipate all of the potential uses of your program. It’s appropriate to create a warning when this contract between you and the user is violated.\nA perfect example of this situation is the result of\n\n\nas.numeric(c(\"5\", \"6\", \"seven\"))\n\n\nWarning: NAs introduced by coercion\n[1]  5  6 NA\n\nThe user expects a vector of numbers to be returned as the result of as.numeric() but \"seven\" is coerced into being NA, which is not completely intuitive.\nR has largely been developed according to the Unix Philosophy, which generally discourages printing text to the console unless something unexpected has occurred. Languages that commonly run on Unix systems like C and C++ are rarely used interactively, meaning that they usually underpin computer infrastructure (computers “talking” to other computers). Messages printed to the console are therefore not very useful since nobody will ever read them and it’s not straightforward for other programs to capture and interpret them.\nIn contrast, R code is frequently executed by human beings in the R console, which serves as an interactive environment between the computer and person at the keyboard. If you think your program should produce a message, make sure that the output of the message is primarily meant for a human to read. You should avoid signaling a condition or the result of your program to another program by creating a message.\nHow should errors be handled?\nImagine writing a program that will take a long time to complete because of a complex calculation or because you’re handling a large amount of data. If an error occurs during this computation then you’re liable to lose all of the results that were calculated before the error, or your program may not finish a critical task that a program further down your pipeline is depending on. If you anticipate the possibility of errors occurring during the execution of your program, then you can design your program to handle them appropriately.\nThe tryCatch() function is the workhorse of handling errors and warnings in R. The first argument of this function is any R expression, followed by conditions which specify how to handle an error or a warning. The last argument, finally, specifies a function or expression that will be executed after the expression no matter what, even in the event of an error or a warning.\nLet’s construct a simple function I’m going to call beera that catches errors and warnings gracefully.\n\n\nbeera <- function(expr){\n  tryCatch(expr,\n         error = function(e){\n           message(\"An error occurred:\\n\", e)\n         },\n         warning = function(w){\n           message(\"A warning occured:\\n\", w)\n         },\n         finally = {\n           message(\"Finally done!\")\n         })\n}\n\n\n\nThis function takes an expression as an argument and tries to evaluate it. If the expression can be evaluated without any errors or warnings then the result of the expression is returned and the message Finally done! is printed to the R console. If an error or warning is generated, then the functions that are provided to the error or warning arguments are printed. Let’s try this function out with a few examples.\n\n\nbeera({\n  2 + 2\n})\n\n\nFinally done!\n[1] 4\n\nbeera({\n  \"two\" + 2\n})\n\n\nAn error occurred:\nError in \"two\" + 2: non-numeric argument to binary operator\n\nFinally done!\n\nbeera({\n  as.numeric(c(1, \"two\", 3))\n})\n\n\nA warning occured:\nsimpleWarning in doTryCatch(return(expr), name, parentenv, handler): NAs introduced by coercion\n\nFinally done!\n\nNotice that we’ve effectively transformed errors and warnings into messages.\nNow that you know the basics of generating and catching errors you’ll need to decide when your program should generate an error. My advice to you is to limit the number of errors your program generates as much as possible. Even if you design your program so that it’s able to catch and handle errors, the error handling process slows down your program by orders of magnitude. Imagine you wanted to write a simple function that checks if an argument is an even number. You might write the following:\n\n\nis_even <- function(n){\n  n %% 2 == 0\n}\n\nis_even(768)\n\n\n[1] TRUE\n\nis_even(\"two\")\n\n\nError in n%%2: non-numeric argument to binary operator\n\nYou can see that providing a string causes this function to raise an error. You could imagine though that you want to use this function across a list of different data types, and you only want to know which elements of that list are even numbers. You might think to write the following:\n\n\nis_even_error <- function(n){\n  tryCatch(n %% 2 == 0,\n           error = function(e){\n             FALSE\n           })\n}\n\nis_even_error(714)\n\n\n[1] TRUE\n\nis_even_error(\"eight\")\n\n\n[1] FALSE\n\nThis appears to be working the way you intended, however when applied to more data this function will be seriously slow compared to alternatives. For example I could check that n is numeric before treating n like a number:\n\n\nis_even_check <- function(n){\n  is.numeric(n) && n %% 2 == 0\n}\n\nis_even_check(1876)\n\n\n[1] TRUE\n\nis_even_check(\"twelve\")\n\n\n[1] FALSE\n\n\nNotice that by using is.numeric() before the “AND” operator (&&), the expression n %% 2 == 0 is never evaluated. This is a programming language design feature called “short circuiting.” The expression can never evaluate to TRUE if the left hand side of && evaluates to FALSE, so the right hand side is ignored.\n\nTo demonstrate the difference in the speed of the code, we will use the microbenchmark package to measure how long it takes for each function to be applied to the same data.\n\n\nlibrary(microbenchmark)\nmicrobenchmark(sapply(letters, is_even_check))\n\n\n\nUnit: microseconds\n                           expr    min      lq     mean  median      uq     max neval\n sapply(letters, is_even_check) 46.224 47.7975 61.43616 48.6445 58.4755 167.091   100\n\n\nmicrobenchmark(sapply(letters, is_even_error))\n\n\n\nUnit: microseconds\n                           expr     min       lq     mean   median       uq      max neval\n sapply(letters, is_even_error) 640.067 678.0285 906.3037 784.4315 1044.501 2308.931   100\nThe error catching approach is nearly 15 times slower!\nProper error handling is an essential tool for any software developer so that you can design programs that are error tolerant. Creating clear and informative error messages is essential for building quality software.\n\nOne closing tip I recommend is to put documentation for your software online, including the meaning of the errors that your software can potentially throw. Often a user’s first instinct when encountering an error is to search online for that error message, which should lead them to your documentation!\n\nSummary\nErrors, warnings, and messages can be generated within R code using the functions stop, stopifnot, warning, and message.\nCatching errors, and providing useful error messaging, can improve user experience with functions but can also slow down code substantially.\nPost-lecture materials\nAdditional Resources\n\nhttps://rdpeng.github.io/Biostat776/lecture-error-handling-and-generation.html\nhttps://adv-r.hadley.nz/debugging.html\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-22T21:13:01-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-23-profiling-r-code/",
    "title": "Profiling R Code",
    "description": "Introduction to understand how much time is spent on different parts of your R code (i.e. profiling).",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-23",
    "categories": [
      "module 2",
      "week 4",
      "programming",
      "profiling"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nProfiling R Code\nProfiling in R\nUsing system.time()\nTiming Longer Expressions\nThe R Profiler\nUsing summaryRprof()\n\nSummary\nPost-lecture materials\nFinal Questions\nAdditional Resources\n\n\n\nLet’s solve the problem but let’s not make it worse by guessing. —Gene Kranz, Apollo 13 Lead Flight Director\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://adv-r.hadley.nz/perf-measure.html\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-profiling-r-code.html\nLearning objectives\n\nAt the end of this lesson you will:\nKnow how to profile R code with the Rprof() function\nKnow how to summarize profiled R code using summaryRprof()\n\nProfiling R Code\nBefore you can make your code faster, you first need to figure out what is making it slow. This sounds easy, but it is not. Even experienced programmers have a hard time identifying bottlenecks in their code. So instead of relying on your intuition, you should profile your code: measure the run-time of each line of code using realistic inputs.\nOnce you have identified bottlenecks you will need to carefully experiment with alternatives to find faster code that is still equivalent.\nProfiling in R\nR comes with a profiler to help you optimize your code and improve its performance. In general, it is usually a bad idea to focus on optimizing your code at the very beginning of development. Rather, in the beginning it is better to focus on translating your ideas into code and writing code that is coherent and readable. The problem is that heavily optimized code tends to be obscure and difficult to read, making it harder to debug and revise. Better to get all the bugs out first, then focus on optimizing.\nOf course, when it comes to optimizing code, the question is\n\nWhat should I optimize?\n\nWell, clearly should optimize the parts of your code that are running slowly, but how do we know what parts those are?\nThis is what the profiler is for. Profiling is a systematic way to examine how much time is spent in different parts of a program.\nSometimes profiling becomes necessary as a project grows and layers of code are placed on top of each other. Often you might write some code that runs fine once. But then later, you might put that same code in a big loop that runs 1,000 times. Now, the original code that took 1 second to run is taking 1,000 seconds to run! Getting that little piece of original code to run faster will help the entire loop.\nIt’s tempting to think you just know where the bottlenecks in your code are. I mean, after all, you wrote it! But trust me, I can’t tell you how many times I have been surprised at where exactly my code is spending all its time. The reality is that profiling is better than guessing. Better to collect some data than to go on hunches alone. Ultimately, getting the biggest impact on speeding up code depends on knowing where the code spends most of its time. This cannot be done without some sort of rigorous performance analysis or profiling.\n\nWe should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil —Donald Knuth\n\nThe basic principles of optimizing your code are:\nDesign first, then optimize\nRemember: Premature optimization (spending a lot of time on something that you may not actually need) is the root of all evil\nMeasure (collect data), do not guess.\nIf you are going to be scientist, you need to apply the same principles here!\nUsing system.time()\nThey system.time() function takes an arbitrary R expression as input (can be wrapped in curly braces) and returns the amount of time taken to evaluate the expression. The system.time() function computes the time (in seconds) needed to execute an expression and if there is an error, gives the time until the error occurred. The function returns an object of class proc_time which contains two useful bits of information:\nuser time: time charged to the CPU(s) for this expression\nelapsed time: “wall clock” time, the amount of time that passes for you as you are sitting there at your computer\nUsually, the user time and elapsed time are relatively close, for straight computing tasks. But there are a few situations where the two can diverge, sometimes dramatically.\nThe elapsed time may be greater than the user time if the CPU spends a lot of time waiting around. This commonly happens if your R expression involves some input or output, which depends on the activity of the file system and the disk (or the Internet, if using a network connection).\nThe elapsed time may be smaller than the user time if your machine has multiple cores/processors (and is capable of using them). For example, multi-threaded BLAS libraries (vecLib/Accelerate, ATLAS, ACML, MKL) can greatly speed up linear algebra calculations and are commonly installed on even desktop systems these days. Also, parallel processing done via something like the parallel package can make the elapsed time smaller than the user time. When you have multiple processors/cores/machines working in parallel, the amount of time that the collection of CPUs spends working on a problem is the same as with a single CPU, but because they are operating in parallel, there is a savings in elapsed time.\nHere’s an example of where the elapsed time is greater than the user time.\n## Elapsed time > user time\n> system.time(readLines(\"https://publichealth.jhu.edu\"))\n   user  system elapsed \n  0.024   0.025   0.366 \nMost of the time in this expression is spent waiting for the connection to the web server and waiting for the data to travel back to my computer. This does not involve the CPU and so the CPU simply waits around for things to get done. Hence, the user time is small.\nIn this example, the elapsed time is smaller than the user time.\n## Elapsed time < user time\n> hilbert <- function(n) { \n+         i <- 1:n\n+         1 / outer(i - 1, i, \"+\")\n+ }\n> x <- hilbert(1000)\n> system.time(svd(x))\n   user  system elapsed \n  1.035   0.255   0.462 \nIn this case I ran a singular value decomposition on the matrix in x, which is a common linear algebra procedure. Because my computer is able to split the work across multiple processors, the elapsed time is about half the user time.\nTiming Longer Expressions\nYou can time longer expressions by wrapping them in curly braces within the call to system.time().\n\n\nsystem.time({\n        n <- 1000\n        r <- numeric(n)\n        for(i in 1:n) {\n                x <- rnorm(n)\n                r[i] <- mean(x)\n        }\n})\n\n\n   user  system elapsed \n  0.088   0.005   0.095 \n\nIf your expression is getting pretty long (more than 2 or 3 lines), it might be better to either break it into smaller pieces or to use the profiler. The problem is that if the expression is too long, you will not be able to identify which part of the code is causing the bottleneck.\nThe R Profiler\nUsing system.time() allows you to test certain functions or code blocks to see if they are taking excessive amounts of time. However, this approach assumes that you already know where the problem is and can call system.time() on it that piece of code. What if you don’t know where to start?\nThis is where the profiler comes in handy. The Rprof() function starts the profiler in R. Note that R must be compiled with profiler support (but this is usually the case). In conjunction with Rprof(), we will use the summaryRprof() function, which summarizes the output from Rprof() (otherwise it’s not really readable).\nRprof() keeps track of the function call stack at regularly sampled intervals and tabulates how much time is spent inside each function. By default, the profiler samples the function call stack every 0.02 seconds. This means that if your code runs very quickly (say, under 0.02 seconds), the profiler is not useful. But if your code runs that fast, you probably don’t need the profiler.\nThe profiler is started by calling the Rprof() function.\n\n\nRprof()    ## Turn on the profiler\n\n\n\nYou do not need any other arguments. By default, it will write its output to a file called Rprof.out. You can specify the name of the output file if you do not want to use this default.\nOnce you call the Rprof() function, everything that you do from then on will be measured by the profiler. Therefore, you usually only want to run a single R function or expression once you turn on the profiler and then immediately turn it off. The reason is that if you mix too many function calls together when running the profiler, all of the results will be mixed together and you will not be able to sort out where the bottlenecks are. In reality, I usually only run a single function with the profiler on.\nThe profiler can be turned off by passing NULL to Rprof().\n\n\nRprof(NULL)    ## Turn off the profiler\n\n\n\nThe raw output from the profiler looks something like this. Here I’m calling the lm() function on some data with the profiler running.\n## lm(y ~ x)\n\nsample.interval=10000\n\"list\" \"eval\" \"eval\" \"model.frame.default\" \"model.frame\" \"eval\" \"eval\" \"lm\" \n\"list\" \"eval\" \"eval\" \"model.frame.default\" \"model.frame\" \"eval\" \"eval\" \"lm\" \n\"list\" \"eval\" \"eval\" \"model.frame.default\" \"model.frame\" \"eval\" \"eval\" \"lm\" \n\"list\" \"eval\" \"eval\" \"model.frame.default\" \"model.frame\" \"eval\" \"eval\" \"lm\" \n\"na.omit\" \"model.frame.default\" \"model.frame\" \"eval\" \"eval\" \"lm\" \n\"na.omit\" \"model.frame.default\" \"model.frame\" \"eval\" \"eval\" \"lm\" \n\"na.omit\" \"model.frame.default\" \"model.frame\" \"eval\" \"eval\" \"lm\" \n\"na.omit\" \"model.frame.default\" \"model.frame\" \"eval\" \"eval\" \"lm\" \n\"na.omit\" \"model.frame.default\" \"model.frame\" \"eval\" \"eval\" \"lm\" \n\"na.omit\" \"model.frame.default\" \"model.frame\" \"eval\" \"eval\" \"lm\" \n\"na.omit\" \"model.frame.default\" \"model.frame\" \"eval\" \"eval\" \"lm\" \n\"lm.fit\" \"lm\" \n\"lm.fit\" \"lm\" \n\"lm.fit\" \"lm\" \nAt each line of the output, the profiler writes out the function call stack. For example, on the very first line of the output you can see that the code is 8 levels deep in the call stack. This is where you need the summaryRprof() function to help you interpret this data.\nUsing summaryRprof()\nThe summaryRprof() function tabulates the R profiler output and calculates how much time is spent in which function. There are two methods for normalizing the data.\n“by.total” divides the time spend in each function by the total run time\n“by.self” does the same as “by.total” but first subtracts out time spent in functions above the current function in the call stack. I personally find this output to be much more useful.\nHere is what summaryRprof() reports in the “by.total” output.\n$by.total\n                        total.time total.pct self.time self.pct\n\"lm\"                          7.41    100.00      0.30     4.05\n\"lm.fit\"                      3.50     47.23      2.99    40.35\n\"model.frame.default\"         2.24     30.23      0.12     1.62\n\"eval\"                        2.24     30.23      0.00     0.00\n\"model.frame\"                 2.24     30.23      0.00     0.00\n\"na.omit\"                     1.54     20.78      0.24     3.24\n\"na.omit.data.frame\"          1.30     17.54      0.49     6.61\n\"lapply\"                      1.04     14.04      0.00     0.00\n\"[.data.frame\"                1.03     13.90      0.79    10.66\n\"[\"                           1.03     13.90      0.00     0.00\n\"as.list.data.frame\"          0.82     11.07      0.82    11.07\n\"as.list\"                     0.82     11.07      0.00     0.00\nBecause lm() is the function that I called from the command line, of course 100% of the time is spent somewhere in that function. However, what this doesn’t show is that if lm() immediately calls another function (like lm.fit(), which does most of the heavy lifting), then in reality, most of the time is spent in that function, rather than in the top-level lm() function.\nThe “by.self” output corrects for this discrepancy.\n$by.self\n                        self.time self.pct total.time total.pct\n\"lm.fit\"                     2.99    40.35       3.50     47.23\n\"as.list.data.frame\"         0.82    11.07       0.82     11.07\n\"[.data.frame\"               0.79    10.66       1.03     13.90\n\"structure\"                  0.73     9.85       0.73      9.85\n\"na.omit.data.frame\"         0.49     6.61       1.30     17.54\n\"list\"                       0.46     6.21       0.46      6.21\n\"lm\"                         0.30     4.05       7.41    100.00\n\"model.matrix.default\"       0.27     3.64       0.79     10.66\n\"na.omit\"                    0.24     3.24       1.54     20.78\n\"as.character\"               0.18     2.43       0.18      2.43\n\"model.frame.default\"        0.12     1.62       2.24     30.23\n\"anyDuplicated.default\"      0.02     0.27       0.02      0.27\nNow you can see that only about 4% of the runtime is spent in the actual lm() function, whereas over 40% of the time is spent in lm.fit(). In this case, this is no surprise since the lm.fit() function is the function that actually fits the linear model.\nYou can see that a reasonable amount of time is spent in functions not necessarily associated with linear modeling (i.e. as.list.data.frame, [.data.frame). This is because the lm() function does a bit of pre-processing and checking before it actually fits the model. This is common with modeling functions—the preprocessing and checking is useful to see if there are any errors. But those two functions take up over 1.5 seconds of runtime. What if you want to fit this model 10,000 times? You’re going to be spending a lot of time in preprocessing and checking.\nThe final bit of output that summaryRprof() provides is the sampling interval and the total runtime.\n$sample.interval\n[1] 0.02\n\n$sampling.time\n[1] 7.41\nSummary\nRprof() runs the profiler for performance of analysis of R code\nsummaryRprof() summarizes the output of Rprof() and gives percent of time spent in each function (with two types of normalization)\nGood to break your code into functions so that the profiler can give useful information about where time is being spent\nC or Fortran code is not profiled\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nWhy is it a bad idea to use system.time() and Rprof() together?\n\nAdditional Resources\n\nhttps://rdpeng.github.io/Biostat776/lecture-profiling-r-code.html\nhttps://adv-r.hadley.nz/perf-measure.html\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-28T12:45:47-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-21-functions/",
    "title": "Functions",
    "description": "Introduction to writing functions in R.",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-21",
    "categories": [
      "module 2",
      "week 4",
      "programming",
      "functions"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nIntroduction\nFunctions in R\nYour First Function\n\nArguments\nNamed arguments\nArgument matching\nLazy Evaluation\nThe ... Argument\nArguments Coming After the ... Argument\n\nFunctions are for humans and computers\nEnvironment\nSummary\nPost-lecture materials\nFinal Questions\nAdditional Resources\n\n\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://r4ds.had.co.nz/functions.html\nhttps://adv-r.hadley.nz/functions.html?#functions\nhttps://swcarpentry.github.io/r-novice-inflammation/02-func-R/\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-functions.html\nhttps://rdpeng.github.io/Biostat776/lecture-scoping-rules-of-r.html\nhttps://r4ds.had.co.nz/functions.html\nhttps://r4ds.had.co.nz/functions.html#environment\nLearning objectives\n\nAt the end of this lesson you will:\nKnow how to create a function using function() in R\nKnow how to define named arguments inside a function with default values\nBe able to use named matching or positional matching in the argument list\nUnderstand what is lazy evaluation\nUnderstand the the special ... argument in a function definition\n\nIntroduction\nWriting functions is a core activity of an R programmer. It represents the key step of the transition from a mere “user” to a developer who creates new functionality for R. Functions are often used to encapsulate a sequence of expressions that need to be executed numerous times, perhaps under slightly different conditions. Functions are also often written when code must be shared with others or the public.\nThe writing of a function allows a developer to create an interface to the code, that is explicitly specified with a set of arguments (or parameters). This interface provides an abstraction of the code to potential users. This abstraction simplifies the users’ lives because it relieves them from having to know every detail of how the code operates. In addition, the creation of an interface allows the developer to communicate to the user the aspects of the code that are important or are most relevant.\nFunctions in R\nFunctions in R are “first class objects”, which means that they can be treated much like any other R object.\nImportant facts about R functions:\nFunctions can be passed as arguments to other functions. This is very handy for the various apply functions, like lapply() and sapply().\nFunctions can be nested, so that you can define a function inside of another function\nIf you are familiar with common language like C, these features might appear a bit strange. However, they are really important in R and can be useful for data analysis.\nYour First Function\nFunctions are defined using the function() directive and are stored as R objects just like anything else. In particular, they are R objects of class “function”.\nHere’s a simple function that takes no arguments and does nothing.\n\n\nf <- function() {\n        ## This is an empty function\n}\n## Functions have their own class\nclass(f)  \n\n\n[1] \"function\"\n\n## Execute this function\nf()       \n\n\nNULL\n\nNot very interesting, but it is a start. The next thing we can do is create a function that actually has a non-trivial function body.\n\n\nf <- function() {\n        # this is the function body\n        cat(\"Hello, world!\\n\") \n}\nf()\n\n\nHello, world!\n\nThe last aspect of a basic function is the function arguments. These are the options that you can specify to the user that the user may explicitly set. For this basic function, we can add an argument that determines how many times “Hello, world!” is printed to the console.\n\n\nf <- function(num) {\n        for(i in seq_len(num)) {\n                cat(\"Hello, world!\\n\")\n        }\n}\nf(3)\n\n\nHello, world!\nHello, world!\nHello, world!\n\nObviously, we could have just cut-and-pasted the cat(\"Hello, world!\\n\") code three times to achieve the same effect, but then we wouldn’t be programming, would we? Also, it would be un-neighborly of you to give your code to someone else and force them to cut-and-paste the code however many times the need to see “Hello, world!”.\n\nPro tip: if you find yourself doing a lot of cutting and pasting, that’s usually a good sign that you might need to write a function.\n\nFinally, the function above doesn’t return anything. It just prints “Hello, world!” to the console num number of times and then exits. But often it is useful if a function returns something that perhaps can be fed into another section of code.\nThis next function returns the total number of characters printed to the console.\n\n\nf <- function(num) {\n        hello <- \"Hello, world!\\n\"\n        for(i in seq_len(num)) {\n                cat(hello)\n        }\n        chars <- nchar(hello) * num\n        chars\n}\nmeaningoflife <- f(3)\n\n\nHello, world!\nHello, world!\nHello, world!\n\nprint(meaningoflife)\n\n\n[1] 42\n\nIn the above function, we did not have to indicate anything special in order for the function to return the number of characters. In R, the return value of a function is always the very last expression that is evaluated. Because the chars variable is the last expression that is evaluated in this function, that becomes the return value of the function.\nNote that there is a return() function that can be used to return an explicitly value from a function, but it is rarely used in R (we will discuss it a bit later in this lesson).\nFinally, in the above function, the user must specify the value of the argument num. If it is not specified by the user, R will throw an error.\n\n\nf()\n\n\nError in f(): argument \"num\" is missing, with no default\n\nWe can modify this behavior by setting a default value for the argument num. Any function argument can have a default value, if you wish to specify it. Sometimes, argument values are rarely modified (except in special cases) and it makes sense to set a default value for that argument. This relieves the user from having to specify the value of that argument every single time the function is called.\nHere, for example, we could set the default value for num to be 1, so that if the function is called without the num argument being explicitly specified, then it will print “Hello, world!” to the console once.\n\n\nf <- function(num = 1) {\n        hello <- \"Hello, world!\\n\"\n        for(i in seq_len(num)) {\n                cat(hello)\n        }\n        chars <- nchar(hello) * num\n        chars\n}\nf()    ## Use default value for 'num'\n\n\nHello, world!\n[1] 14\n\nf(2)   ## Use user-specified value\n\n\nHello, world!\nHello, world!\n[1] 28\n\nRemember that the function still returns the number of characters printed to the console.\n\nPro tip: The formals() function returns a list of all the formal arguments of a function\n\n\nformals(f)\n\n\n$num\n[1] 1\n\n\nSummary\nWe have written a function that\nhas one formal argument named num with a default value of 1. The formal arguments are the arguments included in the function definition.\nprints the message “Hello, world!” to the console a number of times indicated by the argument num\nreturns the number of characters printed to the console\nArguments\nNamed arguments\nFunctions have named arguments, which can optionally have default values. Because all function arguments have names, they can be specified using their name.\n\n\nf(num = 2)\n\n\nHello, world!\nHello, world!\n[1] 28\n\nSpecifying an argument by its name is sometimes useful if a function has many arguments and it may not always be clear which argument is being specified. Here, our function only has one argument so there’s no confusion.\nArgument matching\nCalling an R function with arguments can be done in a variety of ways. This may be confusing at first, but it’s really handy when doing interactive work at the command line. R functions arguments can be matched positionally or by name. Positional matching just means that R assigns the first value to the first argument, the second value to second argument, etc. So in the following call to rnorm()\n\n\nstr(rnorm)\n\n\nfunction (n, mean = 0, sd = 1)  \n\nmydata <- rnorm(100, 2, 1)              ## Generate some data\n\n\n\n100 is assigned to the n argument, 2 is assigned to the mean argument, and 1 is assigned to the sd argument, all by positional matching.\nThe following calls to the sd() function (which computes the empirical standard deviation of a vector of numbers) are all equivalent. Note that sd() has two arguments: x indicates the vector of numbers and na.rm is a logical indicating whether missing values should be removed or not.\n\n\n## Positional match first argument, default for 'na.rm'\nsd(mydata)                     \n\n\n[1] 1.029314\n\n## Specify 'x' argument by name, default for 'na.rm'\nsd(x = mydata)                 \n\n\n[1] 1.029314\n\n## Specify both arguments by name\nsd(x = mydata, na.rm = FALSE)  \n\n\n[1] 1.029314\n\nWhen specifying the function arguments by name, it doesn’t matter in what order you specify them. In the example below, we specify the na.rm argument first, followed by x, even though x is the first argument defined in the function definition.\n\n\n## Specify both arguments by name\nsd(na.rm = FALSE, x = mydata)     \n\n\n[1] 1.029314\n\nYou can mix positional matching with matching by name. When an argument is matched by name, it is “taken out” of the argument list and the remaining unnamed arguments are matched in the order that they are listed in the function definition.\n\n\nsd(na.rm = FALSE, mydata)\n\n\n[1] 1.029314\n\nHere, the mydata object is assigned to the x argument, because it’s the only argument not yet specified.\n\nPro tip: The args() function displays the argument names and corresponding default values of a function\n\n\nargs(f)\n\n\nfunction (num = 1) \nNULL\n\n\nBelow is the argument list for the lm() function, which fits linear models to a dataset.\n\n\nargs(lm)\n\n\nfunction (formula, data, subset, weights, na.action, method = \"qr\", \n    model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, \n    contrasts = NULL, offset, ...) \nNULL\n\nThe following two calls are equivalent.\nlm(data = mydata, y ~ x, model = FALSE, 1:100)\nlm(y ~ x, mydata, 1:100, model = FALSE)\nEven though it’s legal, I don’t recommend messing around with the order of the arguments too much, since it can lead to some confusion.\nMost of the time, named arguments are useful on the command line when you have a long argument list and you want to use the defaults for everything except for an argument near the end of the list. Named arguments also help if you can remember the name of the argument and not its position on the argument list. For example, plotting functions often have a lot of options to allow for customization, but this makes it difficult to remember exactly the position of every argument on the argument list.\nFunction arguments can also be partially matched, which is useful for interactive work. The order of operations when given an argument is\nCheck for exact match for a named argument\nCheck for a partial match\nCheck for a positional match\nPartial matching should be avoided when writing longer code or programs, because it may lead to confusion if someone is reading the code. However, partial matching is very useful when calling functions interactively that have very long argument names.\nLazy Evaluation\nArguments to functions are evaluated lazily, so they are evaluated only as needed in the body of the function.\nIn this example, the function f() has two arguments: a and b.\n\n\nf <- function(a, b) {\n        a^2\n} \nf(2)\n\n\n[1] 4\n\nThis function never actually uses the argument b, so calling f(2) will not produce an error because the 2 gets positionally matched to a. This behavior can be good or bad. It’s common to write a function that doesn’t use an argument and not notice it simply because R never throws an error.\nThis example also shows lazy evaluation at work, but does eventually result in an error.\n\n\nf <- function(a, b) {\n        print(a)\n        print(b)\n}\nf(45)\n\n\n[1] 45\nError in print(b): argument \"b\" is missing, with no default\n\nNotice that “45” got printed first before the error was triggered. This is because b did not have to be evaluated until after print(a). Once the function tried to evaluate print(b) the function had to throw an error.\nThe ... Argument\nThere is a special argument in R known as the ... argument, which indicates a variable number of arguments that are usually passed on to other functions. The ... argument is often used when extending another function and you don’t want to copy the entire argument list of the original function\nFor example, a custom plotting function may want to make use of the default plot() function along with its entire argument list. The function below changes the default for the type argument to the value type = \"l\" (the original default was type = \"p\").\nmyplot <- function(x, y, type = \"l\", ...) {\n        plot(x, y, type = type, ...)         ## Pass '...' to 'plot' function\n}\nGeneric functions use ... so that extra arguments can be passed to methods.\n\n\nmean\n\n\nfunction (x, ...) \nUseMethod(\"mean\")\n<bytecode: 0x7f8692d35048>\n<environment: namespace:base>\n\nThe ... argument is necessary when the number of arguments passed to the function cannot be known in advance. This is clear in functions like paste() and cat().\n\n\nargs(paste)\n\n\nfunction (..., sep = \" \", collapse = NULL, recycle0 = FALSE) \nNULL\n\nargs(cat)\n\n\nfunction (..., file = \"\", sep = \" \", fill = FALSE, labels = NULL, \n    append = FALSE) \nNULL\n\nBecause both paste() and cat() print out text to the console by combining multiple character vectors together, it is impossible for those functions to know in advance how many character vectors will be passed to the function by the user. So the first argument to either function is ....\nArguments Coming After the ... Argument\nOne catch with ... is that any arguments that appear after ... on the argument list must be named explicitly and cannot be partially matched or matched positionally.\nTake a look at the arguments to the paste() function.\n\n\nargs(paste)\n\n\nfunction (..., sep = \" \", collapse = NULL, recycle0 = FALSE) \nNULL\n\nWith the paste() function, the arguments sep and collapse must be named explicitly and in full if the default values are not going to be used.\nHere I specify that I want “a” and “b” to be pasted together and separated by a colon.\n\n\npaste(\"a\", \"b\", sep = \":\")\n\n\n[1] \"a:b\"\n\nIf I don’t specify the sep argument in full and attempt to rely on partial matching, I don’t get the expected result.\n\n\npaste(\"a\", \"b\", se = \":\")\n\n\n[1] \"a b :\"\n\nFunctions are for humans and computers\nAs you start to write your own functions, it’s important to keep in mind that functions are not just for the computer, but are also for humans. Technically, R does not care what your function is called, or what comments it contains, but these are important for human readers. This section discusses some things that you should bear in mind when writing functions that humans can understand.\nThe name of a function is important. In an ideal world, you want the name of your function to be short but clearly describe what the function does. This is not always easy, but here are some tips.\nThe function names should be verbs, and arguments should be nouns.\nThere are some exceptions: nouns are ok if the function computes a very well known noun (i.e. mean() is better than compute_mean()). A good sign that a noun might be a better choice is if you are using a very broad verb like “get”, “compute”, “calculate”, or “determine”. Use your best judgement and do not be afraid to rename a function if you figure out a better name later.\n\n\n# Too short\nf()\n\n# Not a verb, or descriptive\nmy_awesome_function()\n\n# Long, but clear\nimpute_missing()\ncollapse_years()\n\n\n\nIf your function name is composed of multiple words, use “snake_case”, where each lowercase word is separated by an underscore. “camelCase” is a popular alternative. It does not really matter which one you pick, the important thing is to be consistent: pick one or the other and stick with it. R itself is not very consistent, but there is nothing you can do about that. Make sure you do not fall into the same trap by making your code as consistent as possible.\n\n\n# Never do this!\ncol_mins <- function(x, y) {}\nrowMaxes <- function(y, x) {}\n\n\n\nIf you have a family of functions that do similar things, make sure they have consistent names and arguments. Use a common prefix to indicate that they are connected. That is better than a common suffix because autocomplete allows you to type the prefix and see all the members of the family.\n\n\n# Good\ninput_select()\ninput_checkbox()\ninput_text()\n\n# Not so good\nselect_input()\ncheckbox_input()\ntext_input()\n\n\n\nWhere possible, avoid overriding existing functions and variables. It is impossible to do in general because so many good names are already taken by other packages, but avoiding the most common names from base R will avoid confusion.\n\n\n# Don't do this!\nT <- FALSE\nc <- 10\nmean <- function(x) sum(x)\n\n\n\nUse comments, lines starting with #, to explain the “why” of your code. You generally should avoid comments that explain the “what” or the “how”. If you can’t understand what the code does from reading it, you should think about how to rewrite it to be more clear.\nDo you need to add some intermediate variables with useful names? Do you need to break out a subcomponent of a large function so you can name it? However, your code can never capture the reasoning behind your decisions: why did you choose this approach instead of an alternative? What else did you try that didn’t work? It’s a great idea to capture that sort of thinking in a comment.\nEnvironment\nThe last component of a function is its environment. This is not something you need to understand deeply when you first start writing functions. However, it’s important to know a little bit about environments because they are crucial to how functions work.\nThe environment of a function controls how R finds the value associated with a name.\nFor example, take this function:\n\n\nf <- function(x) {\n  x + y\n} \n\n\n\nIn many programming languages, this would be an error, because 1y1 is not defined inside the function. In R, this is valid code because R uses rules called lexical scoping to find the value associated with a name. Since y is not defined inside the function, R will look in the environment where the function was defined:\n\n\ny <- 100\nf(10)\n\n\n[1] 110\n\n\ny <- 1000\nf(10)\n\n\n[1] 1010\n\nThis behavior seems like a recipe for bugs, and indeed you should avoid creating functions like this deliberately, but by and large it does not cause too many problems (especially if you regularly restart R to get to a clean slate).\nThe advantage of this behavior is that from a language standpoint it allows R to be very consistent. Every name is looked up using the same set of rules. For f() that includes the behavior of two things that you might not expect: { and +. This allows you to do devious things like:\n\n\n`+` <- function(x, y) {\n  if (runif(1) < 0.1) {\n    sum(x, y)\n  } else {\n    sum(x, y) * 1.1\n  }\n}\ntable(replicate(1000, 1 + 2))\n\n\n\n  3 3.3 \n104 896 \n\n\n\nrm(`+`)\n\n\n\nThis is a common phenomenon in R. R places few limits on your power. You can do many things that you can’t do in other programming languages. You can do many things that 99% of the time are extremely ill-advised (like overriding how addition works!). But this power and flexibility is what makes tools like ggplot2 and dplyr possible.\n\nPro tip: If you are interested in learning more about scoping, check out\nhttps://adv-r.hadley.nz/functions.html?#lexical-scoping\nhttps://rdpeng.github.io/Biostat776/lecture-scoping-rules-of-r.html\n\nSummary\nFunctions can be defined using the function() directive and are assigned to R objects just like any other R object\nFunctions have can be defined with named arguments; these function arguments can have default values\nFunctions arguments can be specified by name or by position in the argument list\nFunctions always return the last expression evaluated in the function body\nA variable number of arguments can be specified using the special ... argument in a function definition.\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nPractice turning the following code snippets into functions. Think about what each function does. What would you call it? How many arguments does it need? Can you rewrite it to be more expressive or less duplicative?\n\n\nmean(is.na(x))\n\nx / sum(x, na.rm = TRUE)\n\n\n\nRead the complete lyrics to “Little Bunny Foo Foo”. There is a lot of duplication in this song. Extend the initial piping example to recreate the complete song, and use functions to reduce the duplication.\nTake a function that you’ve written recently and spend 5 minutes brainstorming a better name for it and its arguments.\nWhat does the trim argument to mean() do? When might you use it?\nThe default value for the method argument to cor() is c(\"pearson\", \"kendall\", \"spearman\"). What does that mean? What value is used by default?\n\nAdditional Resources\n\nhttps://rdpeng.github.io/Biostat776/lecture-functions.html\nhttps://rdpeng.github.io/Biostat776/lecture-scoping-rules-of-r.html\nhttps://r4ds.had.co.nz/functions.html\nhttps://r4ds.had.co.nz/functions.html#environment\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-21T13:11:40-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-21-loop-functions/",
    "title": "Vectorization and loop functionals",
    "description": "Introduction to vectorization and loop functionals.",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-21",
    "categories": [
      "module 2",
      "week 4",
      "programming",
      "functions"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nVectorization\nVector arithmetics\n\nFunctional loops\nlapply()\nsapply()\nsplit()\nSplitting a Data Frame\ntapply\napply()\nmapply()\nVectorizing a Function\n\nSummary\nPost-lecture materials\nFinal Questions\nAdditional Resources\n\n\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://rafalab.github.io/dsbook/programming-basics.html#vectorization\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-loop-functions.html\nhttps://rafalab.github.io/dsbook/programming-basics.html#vectorization\nLearning objectives\n\nAt the end of this lesson you will:\nUnderstand how to perform vector arithmetics in R\nImplement the 5 functional loops in R (vs e.g. for loops) in R\n\nVectorization\nWriting for and while loops are useful and easy to understand, but in R we rarely use them. As you learn more R, you will realize that vectorization is preferred over for-loops since it results in shorter and clearer code.\nVector arithmetics\nRescaling a vector\nIn R, arithmetic operations on vectors occur element-wise. For a quick example, suppose we have height in inches:\n\n\ninches <- c(69, 62, 66, 70, 70, 73, 67, 73, 67, 70)\n\n\n\nand want to convert to centimeters. Notice what happens when we multiply inches by 2.54:\n\n\ninches * 2.54\n\n\n [1] 175.26 157.48 167.64 177.80 177.80 185.42 170.18 185.42 170.18\n[10] 177.80\n\nIn the line above, we multiplied each element by 2.54. Similarly, if for each entry we want to compute how many inches taller or shorter than 69 inches (the average height for males), we can subtract it from every entry like this:\n\n\ninches - 69\n\n\n [1]  0 -7 -3  1  1  4 -2  4 -2  1\n\nTwo vectors\nIf we have two vectors of the same length, and we sum them in R, they will be added entry by entry as follows:\n\n\nx <- 1:10\ny <- 1:10 \nx + y\n\n\n [1]  2  4  6  8 10 12 14 16 18 20\n\nThe same holds for other mathematical operations, such as -, * and /.\n\n\nx <- 1:10\nsqrt(x)\n\n\n [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751\n [8] 2.828427 3.000000 3.162278\n\n\n\ny <- 1:10\nx*y\n\n\n [1]   1   4   9  16  25  36  49  64  81 100\n\nFunctional loops\nWhile for loops are perfectly valid, when you use vectorization in an element-wise fashion, there is no need for for loops because we can apply what are called functional loops.\nFunctional loops are functions that help us apply the same function to each entry in a vector, matrix, data frame, or list. Here are a list of them:\nlapply(): Loop over a list and evaluate a function on each element\nsapply(): Same as lapply but try to simplify the result\napply(): Apply a function over the margins of an array\ntapply(): Apply a function over subsets of a vector\nmapply(): Multivariate version of lapply\nAn auxiliary function split() is also useful, particularly in conjunction with lapply().\nlapply()\nThe lapply() function does the following simple series of operations:\nit loops over a list, iterating over each element in that list\nit applies a function to each element of the list (a function that you specify)\nand returns a list (the l in lapply() is for “list”).\nThis function takes three arguments: (1) a list X; (2) a function (or the name of a function) FUN; (3) other arguments via its ... argument. If X is not a list, it will be coerced to a list using as.list().\nThe body of the lapply() function can be seen here.\n\n\nlapply\n\n\nfunction (X, FUN, ...) \n{\n    FUN <- match.fun(FUN)\n    if (!is.vector(X) || is.object(X)) \n        X <- as.list(X)\n    .Internal(lapply(X, FUN))\n}\n<bytecode: 0x7f9a1c0461a0>\n<environment: namespace:base>\n\nNote: the actual looping is done internally in C code for efficiency reasons.\nIt is important to remember that lapply() always returns a list, regardless of the class of the input.\nHere’s an example of applying the mean() function to all elements of a list. If the original list has names, the the names will be preserved in the output.\n\n\nx <- list(a = 1:5, b = rnorm(10))\nlapply(x, mean)\n\n\n$a\n[1] 3\n\n$b\n[1] 0.1322028\n\nNotice that here we are passing the mean() function as an argument to the lapply() function. Functions in R can be used this way and can be passed back and forth as arguments just like any other object. When you pass a function to another function, you do not need to include the open and closed parentheses () like you do when you are calling a function.\nHere is another example of using lapply().\n\n\nx <- list(a = 1:4, b = rnorm(10), c = rnorm(20, 1), d = rnorm(100, 5))\nlapply(x, mean)\n\n\n$a\n[1] 2.5\n\n$b\n[1] 0.248845\n\n$c\n[1] 0.9935285\n\n$d\n[1] 5.051388\n\nYou can use lapply() to evaluate a function multiple times each with a different argument. Below, is an example where I call the runif() function (to generate uniformly distributed random variables) four times, each time generating a different number of random numbers.\n\n\nx <- 1:4\nlapply(x, runif)\n\n\n[[1]]\n[1] 0.02778712\n\n[[2]]\n[1] 0.5273108 0.8803191\n\n[[3]]\n[1] 0.37306337 0.04795913 0.13862825\n\n[[4]]\n[1] 0.3214921 0.1548316 0.1322282 0.2213059\n\nWhen you pass a function to lapply(), lapply() takes elements of the list and passes them as the first argument of the function you are applying. In the above example, the first argument of runif() is n, and so the elements of the sequence 1:4 all got passed to the n argument of runif().\nFunctions that you pass to lapply() may have other arguments. For example, the runif() function has a min and max argument too. In the example above I used the default values for min and max. How would you be able to specify different values for that in the context of lapply()?\nHere is where the ... argument to lapply() comes into play. Any arguments that you place in the ... argument will get passed down to the function being applied to the elements of the list.\nHere, the min = 0 and max = 10 arguments are passed down to runif() every time it gets called.\n\n\nx <- 1:4\nlapply(x, runif, min = 0, max = 10)\n\n\n[[1]]\n[1] 2.263808\n\n[[2]]\n[1] 1.314165 9.815635\n\n[[3]]\n[1] 3.270137 5.069395 6.814425\n\n[[4]]\n[1] 0.9916910 1.1890256 0.5043966 9.2925392\n\nSo now, instead of the random numbers being between 0 and 1 (the default), the are all between 0 and 10.\nThe lapply() function (and its friends) makes heavy use of anonymous functions. Anonymous functions are like members of Project Mayhem—they have no names. These functions are generated “on the fly” as you are using lapply(). Once the call to lapply() is finished, the function disappears and does not appear in the workspace.\nHere I am creating a list that contains two matrices.\n\n\nx <- list(a = matrix(1:4, 2, 2), b = matrix(1:6, 3, 2)) \nx\n\n\n$a\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n$b\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\nSuppose I wanted to extract the first column of each matrix in the list. I could write an anonymous function for extracting the first column of each matrix.\n\n\nlapply(x, function(elt) { elt[,1] })\n\n\n$a\n[1] 1 2\n\n$b\n[1] 1 2 3\n\nNotice that I put the function() definition right in the call to lapply(). This is perfectly legal and acceptable. You can put an arbitrarily complicated function definition inside lapply(), but if it’s going to be more complicated, it’s probably a better idea to define the function separately.\nFor example, I could have done the following.\n\n\nf <- function(elt) {\n        elt[, 1]\n}\nlapply(x, f)\n\n\n$a\n[1] 1 2\n\n$b\n[1] 1 2 3\n\nNow the function is no longer anonymous; its name is f. Whether you use an anonymous function or you define a function first depends on your context. If you think the function f is something you are going to need a lot in other parts of your code, you might want to define it separately. But if you are just going to use it for this call to lapply(), then it is probably simpler to use an anonymous function.\nsapply()\nThe sapply() function behaves similarly to lapply(); the only real difference is in the return value. sapply() will try to simplify the result of lapply() if possible. Essentially, sapply() calls lapply() on its input and then applies the following algorithm:\nIf the result is a list where every element is length 1, then a vector is returned\nIf the result is a list where every element is a vector of the same length (> 1), a matrix is returned.\nIf it can’t figure things out, a list is returned\nHere’s the result of calling lapply().\n\n\nx <- list(a = 1:4, b = rnorm(10), c = rnorm(20, 1), d = rnorm(100, 5))\nlapply(x, mean)\n\n\n$a\n[1] 2.5\n\n$b\n[1] -0.251483\n\n$c\n[1] 1.481246\n\n$d\n[1] 4.968715\n\nNotice that lapply() returns a list (as usual), but that each element of the list has length 1.\nHere’s the result of calling sapply() on the same list.\n\n\nsapply(x, mean) \n\n\n        a         b         c         d \n 2.500000 -0.251483  1.481246  4.968715 \n\nBecause the result of lapply() was a list where each element had length 1, sapply() collapsed the output into a numeric vector, which is often more useful than a list.\nsplit()\nThe split() function takes a vector or other objects and splits it into groups determined by a factor or list of factors.\nThe arguments to split() are\n\n\nstr(split)\n\n\nfunction (x, f, drop = FALSE, ...)  \n\nwhere\nx is a vector (or list) or data frame\nf is a factor (or coerced to one) or a list of factors\ndrop indicates whether empty factors levels should be dropped\nThe combination of split() and a function like lapply() or sapply() is a common paradigm in R. The basic idea is that you can take a data structure, split it into subsets defined by another variable, and apply a function over those subsets. The results of applying that function over the subsets are then collated and returned as an object. This sequence of operations is sometimes referred to as “map-reduce” in other contexts.\nHere we simulate some data and split it according to a factor variable. Note that we use the gl() function to “generate levels” in a factor variable.\n\n\nx <- c(rnorm(10), runif(10), rnorm(10, 1))\nf <- gl(3, 10) # generate factor levels\nsplit(x, f)\n\n\n$`1`\n [1]  0.3981302 -0.4075286  1.3242586 -0.7012317 -0.5806143 -1.0010722\n [7] -0.6681786  0.9451850  0.4337021  1.0051592\n\n$`2`\n [1] 0.34822440 0.94893818 0.64667919 0.03527777 0.59644846 0.41531800\n [7] 0.07689704 0.52804888 0.96233331 0.70874005\n\n$`3`\n [1]  1.13444766  1.76559900  1.95513668  0.94943430  0.69418458\n [6]  1.89367370 -0.04729815  2.97133739  0.61636789  2.65414530\n\nA common idiom is split followed by an lapply.\n\n\nlapply(split(x, f), mean)\n\n\n$`1`\n[1] 0.07478098\n\n$`2`\n[1] 0.5266905\n\n$`3`\n[1] 1.458703\n\nSplitting a Data Frame\n\n\nlibrary(datasets)\nhead(airquality)\n\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\nWe can split the airquality data frame by the Month variable so that we have separate sub-data frames for each month.\n\n\ns <- split(airquality, airquality$Month)\nstr(s)\n\n\nList of 5\n $ 5:'data.frame':  31 obs. of  6 variables:\n  ..$ Ozone  : int [1:31] 41 36 12 18 NA 28 23 19 8 NA ...\n  ..$ Solar.R: int [1:31] 190 118 149 313 NA NA 299 99 19 194 ...\n  ..$ Wind   : num [1:31] 7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n  ..$ Temp   : int [1:31] 67 72 74 62 56 66 65 59 61 69 ...\n  ..$ Month  : int [1:31] 5 5 5 5 5 5 5 5 5 5 ...\n  ..$ Day    : int [1:31] 1 2 3 4 5 6 7 8 9 10 ...\n $ 6:'data.frame':  30 obs. of  6 variables:\n  ..$ Ozone  : int [1:30] NA NA NA NA NA NA 29 NA 71 39 ...\n  ..$ Solar.R: int [1:30] 286 287 242 186 220 264 127 273 291 323 ...\n  ..$ Wind   : num [1:30] 8.6 9.7 16.1 9.2 8.6 14.3 9.7 6.9 13.8 11.5 ...\n  ..$ Temp   : int [1:30] 78 74 67 84 85 79 82 87 90 87 ...\n  ..$ Month  : int [1:30] 6 6 6 6 6 6 6 6 6 6 ...\n  ..$ Day    : int [1:30] 1 2 3 4 5 6 7 8 9 10 ...\n $ 7:'data.frame':  31 obs. of  6 variables:\n  ..$ Ozone  : int [1:31] 135 49 32 NA 64 40 77 97 97 85 ...\n  ..$ Solar.R: int [1:31] 269 248 236 101 175 314 276 267 272 175 ...\n  ..$ Wind   : num [1:31] 4.1 9.2 9.2 10.9 4.6 10.9 5.1 6.3 5.7 7.4 ...\n  ..$ Temp   : int [1:31] 84 85 81 84 83 83 88 92 92 89 ...\n  ..$ Month  : int [1:31] 7 7 7 7 7 7 7 7 7 7 ...\n  ..$ Day    : int [1:31] 1 2 3 4 5 6 7 8 9 10 ...\n $ 8:'data.frame':  31 obs. of  6 variables:\n  ..$ Ozone  : int [1:31] 39 9 16 78 35 66 122 89 110 NA ...\n  ..$ Solar.R: int [1:31] 83 24 77 NA NA NA 255 229 207 222 ...\n  ..$ Wind   : num [1:31] 6.9 13.8 7.4 6.9 7.4 4.6 4 10.3 8 8.6 ...\n  ..$ Temp   : int [1:31] 81 81 82 86 85 87 89 90 90 92 ...\n  ..$ Month  : int [1:31] 8 8 8 8 8 8 8 8 8 8 ...\n  ..$ Day    : int [1:31] 1 2 3 4 5 6 7 8 9 10 ...\n $ 9:'data.frame':  30 obs. of  6 variables:\n  ..$ Ozone  : int [1:30] 96 78 73 91 47 32 20 23 21 24 ...\n  ..$ Solar.R: int [1:30] 167 197 183 189 95 92 252 220 230 259 ...\n  ..$ Wind   : num [1:30] 6.9 5.1 2.8 4.6 7.4 15.5 10.9 10.3 10.9 9.7 ...\n  ..$ Temp   : int [1:30] 91 92 93 93 87 84 80 78 75 73 ...\n  ..$ Month  : int [1:30] 9 9 9 9 9 9 9 9 9 9 ...\n  ..$ Day    : int [1:30] 1 2 3 4 5 6 7 8 9 10 ...\n\nThen we can take the column means for Ozone, Solar.R, and Wind for each sub-data frame.\n\n\nlapply(s, function(x) {\n        colMeans(x[, c(\"Ozone\", \"Solar.R\", \"Wind\")])\n})\n\n\n$`5`\n   Ozone  Solar.R     Wind \n      NA       NA 11.62258 \n\n$`6`\n    Ozone   Solar.R      Wind \n       NA 190.16667  10.26667 \n\n$`7`\n     Ozone    Solar.R       Wind \n        NA 216.483871   8.941935 \n\n$`8`\n   Ozone  Solar.R     Wind \n      NA       NA 8.793548 \n\n$`9`\n   Ozone  Solar.R     Wind \n      NA 167.4333  10.1800 \n\nUsing sapply() might be better here for a more readable output.\n\n\nsapply(s, function(x) {\n        colMeans(x[, c(\"Ozone\", \"Solar.R\", \"Wind\")])\n})\n\n\n               5         6          7        8        9\nOzone         NA        NA         NA       NA       NA\nSolar.R       NA 190.16667 216.483871       NA 167.4333\nWind    11.62258  10.26667   8.941935 8.793548  10.1800\n\nUnfortunately, there are NAs in the data so we cannot simply take the means of those variables. However, we can tell the colMeans function to remove the NAs before computing the mean.\n\n\nsapply(s, function(x) {\n        colMeans(x[, c(\"Ozone\", \"Solar.R\", \"Wind\")], \n                 na.rm = TRUE)\n})\n\n\n                5         6          7          8         9\nOzone    23.61538  29.44444  59.115385  59.961538  31.44828\nSolar.R 181.29630 190.16667 216.483871 171.857143 167.43333\nWind     11.62258  10.26667   8.941935   8.793548  10.18000\n\nOccasionally, we may want to split an R object according to levels defined in more than one variable. We can do this by creating an interaction of the variables with the interaction() function.\n\n\nx <- rnorm(10)\nf1 <- gl(2, 5)\nf2 <- gl(5, 2)\nf1\n\n\n [1] 1 1 1 1 1 2 2 2 2 2\nLevels: 1 2\n\nf2\n\n\n [1] 1 1 2 2 3 3 4 4 5 5\nLevels: 1 2 3 4 5\n\n## Create interaction of two factors\ninteraction(f1, f2)\n\n\n [1] 1.1 1.1 1.2 1.2 1.3 2.3 2.4 2.4 2.5 2.5\nLevels: 1.1 2.1 1.2 2.2 1.3 2.3 1.4 2.4 1.5 2.5\n\nWith multiple factors and many levels, creating an interaction can result in many levels that are empty.\n\n\nstr(split(x, list(f1, f2)))\n\n\nList of 10\n $ 1.1: num [1:2] 1.512 0.083\n $ 2.1: num(0) \n $ 1.2: num [1:2] 0.567 -1.025\n $ 2.2: num(0) \n $ 1.3: num 0.323\n $ 2.3: num 1.04\n $ 1.4: num(0) \n $ 2.4: num [1:2] 0.0991 -0.4541\n $ 1.5: num(0) \n $ 2.5: num [1:2] -0.6558 -0.0359\n\nNotice that there are 4 categories with no data. But we can drop empty levels when we call the split() function.\n\n\nstr(split(x, list(f1, f2), drop = TRUE))\n\n\nList of 6\n $ 1.1: num [1:2] 1.512 0.083\n $ 1.2: num [1:2] 0.567 -1.025\n $ 1.3: num 0.323\n $ 2.3: num 1.04\n $ 2.4: num [1:2] 0.0991 -0.4541\n $ 2.5: num [1:2] -0.6558 -0.0359\n\ntapply\ntapply() is used to apply a function over subsets of a vector. It can be thought of as a combination of split() and sapply() for vectors only. I’ve been told that the “t” in tapply() refers to “table”, but that is unconfirmed.\n\n\nstr(tapply)\n\n\nfunction (X, INDEX, FUN = NULL, ..., default = NA, simplify = TRUE)  \n\nThe arguments to tapply() are as follows:\nX is a vector\nINDEX is a factor or a list of factors (or else they are coerced to factors)\nFUN is a function to be applied\n… contains other arguments to be passed FUN\nsimplify, should we simplify the result?\nGiven a vector of numbers, one simple operation is to take group means.\n\n\n## Simulate some data\nx <- c(rnorm(10), runif(10), rnorm(10, 1))\n## Define some groups with a factor variable\nf <- gl(3, 10)   \nf\n\n\n [1] 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3\nLevels: 1 2 3\n\ntapply(x, f, mean)\n\n\n        1         2         3 \n0.1896235 0.5336667 0.9568236 \n\nWe can also take the group means without simplifying the result, which will give us a list. For functions that return a single value, usually, this is not what we want, but it can be done.\n\n\ntapply(x, f, mean, simplify = FALSE)\n\n\n$`1`\n[1] 0.1896235\n\n$`2`\n[1] 0.5336667\n\n$`3`\n[1] 0.9568236\n\nWe can also apply functions that return more than a single value. In this case, tapply() will not simplify the result and will return a list. Here’s an example of finding the range of each sub-group.\n\n\ntapply(x, f, range)\n\n\n$`1`\n[1] -1.869789  1.497041\n\n$`2`\n[1] 0.09515213 0.86723879\n\n$`3`\n[1] -0.5690822  2.3644349\n\napply()\nThe apply() function is used to a evaluate a function (often an anonymous one) over the margins of an array. It is most often used to apply a function to the rows or columns of a matrix (which is just a 2-dimensional array). However, it can be used with general arrays, for example, to take the average of an array of matrices. Using apply() is not really faster than writing a loop, but it works in one line and is highly compact.\n\n\nstr(apply)\n\n\nfunction (X, MARGIN, FUN, ..., simplify = TRUE)  \n\nThe arguments to apply() are\nX is an array\nMARGIN is an integer vector indicating which margins should be “retained”.\nFUN is a function to be applied\n... is for other arguments to be passed to FUN\nHere I create a 20 by 10 matrix of Normal random numbers. I then compute the mean of each column.\n\n\nx <- matrix(rnorm(200), 20, 10)\napply(x, 2, mean)  ## Take the mean of each column\n\n\n [1]  0.02218266 -0.15932850  0.09021391  0.14723035 -0.22431309\n [6] -0.49657847  0.30095015  0.07703985 -0.20818099  0.06809774\n\nI can also compute the sum of each row.\n\n\napply(x, 1, sum)   ## Take the mean of each row\n\n\n [1] -0.48483448  5.33222301 -3.33862932 -1.39998450  2.37859098\n [6]  0.01082604 -6.29457190 -0.26287700  0.71133578 -3.38125293\n[11] -4.67522818  3.01900232 -2.39466347 -2.16004389  5.33063755\n[16] -2.92024635  3.52026401 -1.84880901 -4.10213912  5.30667310\n\nNote that in both calls to apply(), the return value was a vector of numbers.\nYou’ve probably noticed that the second argument is either a 1 or a 2, depending on whether we want row statistics or column statistics. What exactly is the second argument to apply()?\nThe MARGIN argument essentially indicates to apply() which dimension of the array you want to preserve or retain. So when taking the mean of each column, I specify\n\n\napply(x, 2, mean)\n\n\n\nbecause I want to collapse the first dimension (the rows) by taking the mean and I want to preserve the number of columns. Similarly, when I want the row sums, I run\n\n\napply(x, 1, mean)\n\n\n\nbecause I want to collapse the columns (the second dimension) and preserve the number of rows (the first dimension).\nCol/Row Sums and Means\nFor the special case of column/row sums and column/row means of matrices, we have some useful shortcuts.\nrowSums = apply(x, 1, sum)\nrowMeans = apply(x, 1, mean)\ncolSums = apply(x, 2, sum)\ncolMeans = apply(x, 2, mean)\nThe shortcut functions are heavily optimized and hence are much faster, but you probably won’t notice unless you’re using a large matrix. Another nice aspect of these functions is that they are a bit more descriptive. It’s arguably more clear to write colMeans(x) in your code than apply(x, 2, mean).\nOther Ways to Apply\nYou can do more than take sums and means with the apply() function. For example, you can compute quantiles of the rows of a matrix using the quantile() function.\n\n\nx <- matrix(rnorm(200), 20, 10)\n## Get row quantiles\napply(x, 1, quantile, probs = c(0.25, 0.75))    \n\n\n          [,1]       [,2]      [,3]       [,4]       [,5]        [,6]\n25% -1.0884151 -0.6693040 0.2908481 -0.4602083 -1.0432010 -1.12773555\n75%  0.1843547  0.8210295 1.3667301  0.4424153  0.3571219  0.03653687\n          [,7]       [,8]       [,9]     [,10]      [,11]      [,12]\n25% -1.4571706 -0.2406991 -0.3226845 -0.329898 -0.8677524 -0.2023664\n75% -0.1705336  0.6504486  1.1460854  1.247092  0.4138139  0.9145331\n         [,13]      [,14]      [,15]        [,16]      [,17]\n25% -0.9796050 -1.3551031 -0.1823252 -1.260911898 -0.9954289\n75%  0.5448777 -0.5396766  0.7795571  0.002908451  0.4323192\n         [,18]      [,19]      [,20]\n25% -0.3767354 -0.8557544 -0.7000363\n75%  0.7542638  0.5440158  0.5432995\n\nNotice that I had to pass the probs = c(0.25, 0.75) argument to quantile() via the ... argument to apply().\nFor a higher dimensional example, I can create an array of \\(2\\times2\\) matrices and then compute the average of the matrices in the array.\n\n\na <- array(rnorm(2 * 2 * 10), c(2, 2, 10))\napply(a, c(1, 2), mean)\n\n\n          [,1]       [,2]\n[1,] 0.1681387 -0.1039673\n[2,] 0.3519741 -0.4029737\n\nIn the call to apply() here, I indicated via the MARGIN argument that I wanted to preserve the first and second dimensions and to collapse the third dimension by taking the mean.\nThere is a faster way to do this specific operation via the colMeans() function.\n\n\nrowMeans(a, dims = 2)    ## Faster\n\n\n          [,1]       [,2]\n[1,] 0.1681387 -0.1039673\n[2,] 0.3519741 -0.4029737\n\nIn this situation, I might argue that the use of rowMeans() is less readable, but it is substantially faster with large arrays.\nmapply()\nThe mapply() function is a multivariate apply of sorts which applies a function in parallel over a set of arguments. Recall that lapply() and friends only iterate over a single R object. What if you want to iterate over multiple R objects in parallel? This is what mapply() is for.\n\n\nstr(mapply)\n\n\nfunction (FUN, ..., MoreArgs = NULL, SIMPLIFY = TRUE, USE.NAMES = TRUE)  \n\nThe arguments to mapply() are\nFUN is a function to apply\n... contains R objects to apply over\nMoreArgs is a list of other arguments to FUN.\nSIMPLIFY indicates whether the result should be simplified\nThe mapply() function has a different argument order from lapply() because the function to apply comes first rather than the object to iterate over. The R objects over which we apply the function are given in the ... argument because we can apply over an arbitrary number of R objects.\nFor example, the following is tedious to type\nlist(rep(1, 4), rep(2, 3), rep(3, 2), rep(4, 1))\nWith mapply(), instead we can do\n\n\n mapply(rep, 1:4, 4:1)\n\n\n[[1]]\n[1] 1 1 1 1\n\n[[2]]\n[1] 2 2 2\n\n[[3]]\n[1] 3 3\n\n[[4]]\n[1] 4\n\nThis passes the sequence 1:4 to the first argument of rep() and the sequence 4:1 to the second argument.\nHere’s another example for simulating random Normal variables.\n\n\nnoise <- function(n, mean, sd) {\n      rnorm(n, mean, sd)\n}\n## Simulate 5 random numbers\nnoise(5, 1, 2)        \n\n\n[1] -0.5196913  3.2979182 -0.6849525  1.7828267  2.7827545\n\n\n## This only simulates 1 set of numbers, not 5\nnoise(1:5, 1:5, 2)    \n\n\n[1] -1.670517  2.796247  2.776826  5.351488  3.422804\n\nHere we can use mapply() to pass the sequence 1:5 separately to the noise() function so that we can get 5 sets of random numbers, each with a different length and mean.\n\n\nmapply(noise, 1:5, 1:5, 2)\n\n\n[[1]]\n[1] 0.8260273\n\n[[2]]\n[1] 4.764568 2.336980\n\n[[3]]\n[1] 4.6463819 2.5582108 0.9412167\n\n[[4]]\n[1]  3.978149  1.550018 -1.192223  6.338245\n\n[[5]]\n[1] 2.826182 1.347834 6.990564 4.976276 3.800743\n\nThe above call to mapply() is the same as\n\n\nlist(noise(1, 1, 2), noise(2, 2, 2),\n     noise(3, 3, 2), noise(4, 4, 2),\n     noise(5, 5, 2))\n\n\n[[1]]\n[1] 0.644104\n\n[[2]]\n[1] 1.148037 3.993318\n\n[[3]]\n[1]  4.4553214 -0.4532612  3.7067970\n\n[[4]]\n[1]  5.4536273  5.3365220 -0.8486346  3.5292851\n\n[[5]]\n[1] 8.959267 6.593589 1.581448 1.672663 5.982219\n\nVectorizing a Function\nThe mapply() function can be use to automatically “vectorize” a function. What this means is that it can be used to take a function that typically only takes single arguments and create a new function that can take vector arguments. This is often needed when you want to plot functions.\nHere’s an example of a function that computes the sum of squares given some data, a mean parameter and a standard deviation. The formula is \\(\\sum_{i=1}^n(x_i-\\mu)^2/\\sigma^2\\).\n\n\nsumsq <- function(mu, sigma, x) {\n        sum(((x - mu) / sigma)^2)\n}\n\n\n\nThis function takes a mean mu, a standard deviation sigma, and some data in a vector x.\nIn many statistical applications, we want to minimize the sum of squares to find the optimal mu and sigma. Before we do that, we may want to evaluate or plot the function for many different values of mu or sigma. However, passing a vector of mus or sigmas won’t work with this function because it’s not vectorized.\n\n\nx <- rnorm(100)       ## Generate some data\nsumsq(1:10, 1:10, x)  ## This is not what we want\n\n\n[1] 110.2594\n\nNote that the call to sumsq() only produced one value instead of 10 values.\nHowever, we can do what we want to do by using mapply().\n\n\nmapply(sumsq, 1:10, 1:10, MoreArgs = list(x = x))\n\n\n [1] 196.2289 121.4765 108.3981 104.0788 102.1975 101.2393 100.6998\n [8] 100.3745 100.1685 100.0332\n\nThere’s even a function in R called Vectorize() that automatically can create a vectorized version of your function. So we could create a vsumsq() function that is fully vectorized as follows.\n\n\nvsumsq <- Vectorize(sumsq, c(\"mu\", \"sigma\"))\nvsumsq(1:10, 1:10, x)\n\n\n [1] 196.2289 121.4765 108.3981 104.0788 102.1975 101.2393 100.6998\n [8] 100.3745 100.1685 100.0332\n\nPretty cool, right?\nSummary\nThe loop functions in R are very powerful because they allow you to conduct a series of operations on data using a compact form\nThe operation of a loop function involves iterating over an R object (e.g. a list or vector or matrix), applying a function to each element of the object, and the collating the results and returning the collated results.\nLoop functions make heavy use of anonymous functions, which exist for the life of the loop function but are not stored anywhere\nThe split() function can be used to divide an R object in to subsets determined by another variable which can subsequently be looped over using loop functions.\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nWrite a function compute_s_n() that for any given n computes the sum\n\\[\nS_n = 1^2 + 2^2 + 3^2 + \\ldots + n^2\n\\]\nReport the value of the sum when \\(n\\) = 10.\nDefine an empty numerical vector s_n of size 25 using s_n <- vector(\"numeric\", 25) and store in the results of \\(S_1, S_2, \\ldots, S_n\\) using a for-loop.\nRepeat Q3, but this time use sapply().\nPlot s_n versus n. Use points defined by \\(n= 1, \\ldots, 25\\)\n\nAdditional Resources\n\nhttps://rdpeng.github.io/Biostat776/lecture-loop-functions.html\nhttps://rafalab.github.io/dsbook/programming-basics.html#vectorization\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-28T12:45:00-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-16-control-structures/",
    "title": "Control Structures",
    "description": "Introduction to control the flow of execution of a series of R expressions.",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-16",
    "categories": [
      "module 2",
      "week 3",
      "R",
      "programming"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nControl Structures\nif-else\nfor Loops\nNested for loops\nwhile Loops\nrepeat Loops\nnext, break\n\nSummary\nPost-lecture materials\nFinal Questions\nAdditional Resources\n\n\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://rafalab.github.io/dsbook/programming-basics.html\nhttps://r4ds.had.co.nz/iteration.html\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-control-structures.html\nhttps://r4ds.had.co.nz/iteration.html\nLearning objectives\n\nAt the end of this lesson you will:\nBe able to use commonly used control structures including if, while, repeat, and for\nBe able to skip an iteration of a loop using next\nBe able to exit a loop immediately using break\n\nControl Structures\nControl structures in R allow you to control the flow of execution of a series of R expressions. Basically, control structures allow you to put some “logic” into your R code, rather than just always executing the same R code every time. Control structures allow you to respond to inputs or to features of the data and execute different R expressions accordingly.\nCommonly used control structures are\nif and else: testing a condition and acting on it\nfor: execute a loop a fixed number of times\nwhile: execute a loop while a condition is true\nrepeat: execute an infinite loop (must break out of it to stop)\nbreak: break the execution of a loop\nnext: skip an interation of a loop\nMost control structures are not used in interactive sessions, but rather when writing functions or longer expresisons. However, these constructs do not have to be used in functions and it’s a good idea to become familiar with them before we delve into functions.\nif-else\nThe if-else combination is probably the most commonly used control structure in R (or perhaps any language). This structure allows you to test a condition and act on it depending on whether it’s true or false.\nFor starters, you can just use the if statement.\nif(<condition>) {\n        ## do something\n} \n## Continue with rest of code\nThe above code does nothing if the condition is false. If you have an action you want to execute when the condition is false, then you need an else clause.\nif(<condition>) {\n        ## do something\n} \nelse {\n        ## do something else\n}\nYou can have a series of tests by following the initial if with any number of else ifs.\nif(<condition1>) {\n        ## do something\n} else if(<condition2>)  {\n        ## do something different\n} else {\n        ## do something different\n}\nHere is an example of a valid if/else structure.\n\n\n## Generate a uniform random number\nx <- runif(1, 0, 10)  \nif(x > 3) {\n        y <- 10\n} else {\n        y <- 0\n}\n\n\n\nThe value of y is set depending on whether x > 3 or not. This expression can also be written a different, but equivalent, way in R.\n\n\ny <- if(x > 3) {\n        10\n} else { \n        0\n}\n\n\n\nNeither way of writing this expression is more correct than the other. Which one you use will depend on your preference and perhaps those of the team you may be working with.\nOf course, the else clause is not necessary. You could have a series of if clauses that always get executed if their respective conditions are true.\nif(<condition1>) {\n\n}\n\nif(<condition2>) {\n\n}\nfor Loops\nFor loops are pretty much the only looping construct that you will need in R. While you may occasionally find a need for other types of loops, in my experience doing data analysis, I’ve found very few situations where a for loop wasn’t sufficient.\nIn R, for loops take an interator variable and assign it successive values from a sequence or vector. For loops are most commonly used for iterating over the elements of an object (list, vector, etc.)\n\n\nfor(i in 1:10) {\n        print(i)\n}\n\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\nThis loop takes the i variable and in each iteration of the loop gives it values 1, 2, 3, …, 10, executes the code within the curly braces, and then the loop exits.\nThe following three loops all have the same behavior.\n\n\nx <- c(\"a\", \"b\", \"c\", \"d\")\n\nfor(i in 1:4) {\n        ## Print out each element of 'x'\n        print(x[i])  \n}\n\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n\nThe seq_along() function is commonly used in conjunction with for loops in order to generate an integer sequence based on the length of an object (in this case, the object x).\n\n\n## Generate a sequence based on length of 'x'\nfor(i in seq_along(x)) {   \n        print(x[i])\n}\n\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n\nIt is not necessary to use an index-type variable.\n\n\nfor(letter in x) {\n        print(letter)\n}\n\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n\nFor one line loops, the curly braces are not strictly necessary.\n\n\nfor(i in 1:4) print(x[i])\n\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n\nHowever, I like to use curly braces even for one-line loops, because that way if you decide to expand the loop to multiple lines, you won’t be burned because you forgot to add curly braces (and you will be burned by this).\nNested for loops\nfor loops can be nested inside of each other.\nx <- matrix(1:6, 2, 3)\n\nfor(i in seq_len(nrow(x))) {\n        for(j in seq_len(ncol(x))) {\n                print(x[i, j])\n        }   \n}\nNested loops are commonly needed for multidimensional or hierarchical data structures (e.g. matrices, lists). Be careful with nesting though. Nesting beyond 2 to 3 levels often makes it difficult to read/understand the code. If you find yourself in need of a large number of nested loops, you may want to break up the loops by using functions (discussed later).\nwhile Loops\nWhile loops begin by testing a condition. If it is true, then they execute the loop body. Once the loop body is executed, the condition is tested again, and so forth, until the condition is false, after which the loop exits.\n\n\ncount <- 0\nwhile(count < 10) {\n        print(count)\n        count <- count + 1\n}\n\n\n[1] 0\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n\nWhile loops can potentially result in infinite loops if not written properly. Use with care!\nSometimes there will be more than one condition in the test.\n\n\nz <- 5\nset.seed(1)\n\nwhile(z >= 3 && z <= 10) {\n        coin <- rbinom(1, 1, 0.5)\n        \n        if(coin == 1) {  ## random walk\n                z <- z + 1\n        } else {\n                z <- z - 1\n        } \n}\nprint(z)\n\n\n[1] 2\n\nConditions are always evaluated from left to right. For example, in the above code, if z were less than 3, the second test would not have been evaluated.\nrepeat Loops\nrepeat initiates an infinite loop right from the start. These are not commonly used in statistical or data analysis applications but they do have their uses. The only way to exit a repeat loop is to call break.\nOne possible paradigm might be in an iterative algorithm where you may be searching for a solution and you don’t want to stop until you’re close enough to the solution. In this kind of situation, you often don’t know in advance how many iterations it’s going to take to get “close enough” to the solution.\n\n\nx0 <- 1\ntol <- 1e-8\n\nrepeat {\n        x1 <- computeEstimate()\n        \n        if(abs(x1 - x0) < tol) {  ## Close enough?\n                break\n        } else {\n                x0 <- x1\n        } \n}\n\n\n\nNote that the above code will not run if the computeEstimate() function is not defined (I just made it up for the purposes of this demonstration).\nThe loop above is a bit dangerous because there’s no guarantee it will stop. You could get in a situation where the values of x0 and x1 oscillate back and forth and never converge. Better to set a hard limit on the number of iterations by using a for loop and then report whether convergence was achieved or not.\nnext, break\nnext is used to skip an iteration of a loop.\n\n\nfor(i in 1:100) {\n        if(i <= 20) {\n                ## Skip the first 20 iterations\n                next                 \n        }\n        ## Do something here\n}\n\n\n\nbreak is used to exit a loop immediately, regardless of what iteration the loop may be on.\n\n\nfor(i in 1:100) {\n      print(i)\n\n      if(i > 20) {\n              ## Stop loop after 20 iterations\n              break  \n      }    \n}\n\n\n\nSummary\nControl structures like if, while, and for allow you to control the flow of an R program\nInfinite loops should generally be avoided, even if (you believe) they are theoretically correct.\nControl structures mentioned here are primarily useful for writing programs; for command-line interactive work, the “apply” functions are more useful.\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nWrite for loops to compute the mean of every column in mtcars.\nImagine you have a directory full of CSV files that you want to read in. You have their paths in a vector, files <- dir(\"data/\", pattern = \"\\\\.csv$\", full.names = TRUE), and now want to read each one with read_csv(). Write the for loop that will load them into a single data frame.\nWhat happens if you use for (nm in names(x)) and x has no names? What if only some of the elements are named? What if the names are not unique?\n\nAdditional Resources\n\nhttps://rdpeng.github.io/Biostat776/lecture-control-structures.html\nhttps://rafalab.github.io/dsbook/programming-basics.html\nhttps://r4ds.had.co.nz/iteration.html\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-16T13:04:56-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-16-r-nuts-and-bolts/",
    "title": "R Nuts and Bolts",
    "description": "Introduction to data types and objects in R.",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-16",
    "categories": [
      "module 2",
      "week 3",
      "R",
      "programming"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nR Nuts and Bolts\nEntering Input\nEvaluation\nR Objects\nNumbers\nAttributes\nCreating Vectors\nMixing Objects\nExplicit Coercion\nMatrices\nLists\nFactors\nMissing Values\nData Frames\nNames\n\nSummary\nPost-lecture materials\nFinal Questions\nAdditional Resources\n\n\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://rafalab.github.io/dsbook/r-basics.html\nhttps://r4ds.had.co.nz/vectors.html?q=typeof#vectors\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-r-nuts-and-bolts.html\nLearning objectives\n\nAt the end of this lesson you will:\nKnow the 5 basic built-in data types (or classes) of objects in R\nKnow the types of attributes (or metadata) R objects can have\nBe able to create a vector, matrix, data frame, factor, and list in R\nRecognize missing values in R\n\nR Nuts and Bolts\nEntering Input\nAt the R prompt we type expressions. The <- symbol is the assignment operator.\n\n\nx <- 1\nprint(x)\n\n\n[1] 1\n\nx\n\n\n[1] 1\n\nmsg <- \"hello\"\n\n\n\nThe grammar of the language determines whether an expression is complete or not.\nx <-  ## Incomplete expression\nThe # character indicates a comment. Anything to the right of the # (including the # itself) is ignored. This is the only comment character in R. Unlike some other languages, R does not support multi-line comments or comment blocks.\nEvaluation\nWhen a complete expression is entered at the prompt, it is evaluated and the result of the evaluated expression is returned. The result may be auto-printed.\n\n\nx <- 5  ## nothing printed\nx       ## auto-printing occurs\n\n\n[1] 5\n\nprint(x)  ## explicit printing\n\n\n[1] 5\n\nThe [1] shown in the output indicates that x is a vector and 5 is its first element.\nTypically with interactive work, we do not explicitly print objects with the print function; it is much easier to just auto-print them by typing the name of the object and hitting return/enter. However, when writing scripts, functions, or longer programs, there is sometimes a need to explicitly print objects because auto-printing does not work in those settings.\nWhen an R vector is printed you will notice that an index for the vector is printed in square brackets [] on the side. For example, see this integer sequence of length 20.\n\n\n\n\n\nx <- 11:30\nx\n\n\n [1] 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30\n\n\n\n\nThe numbers in the square brackets are not part of the vector itself, they are merely part of the printed output.\nWith R, it’s important that one understand that there is a difference between the actual R object and the manner in which that R object is printed to the console. Often, the printed output may have additional bells and whistles to make the output more friendly to the users. However, these bells and whistles are not inherently part of the object.\nNote that the : operator is used to create integer sequences.\nR Objects\nR has five basic or “atomic” classes of vector objects:\nlogical: FALSE, TRUE, and NA\nnumeric: real numbers\ninteger (and doubles): these are known collectively as numeric vectors\ncomplex: complex numbers\ncharacter: the most complex type of atomic vector, because each element of a character vector is a string, and a string can contain an arbitrary amount of data.\n\n\n\nFigure 1: The hierarchy of R’s vector types\n\n\n\n[Source: R 4 Data Science]\n\nInteger and double vectors are known collectively as numeric vectors. In R, numbers are doubles by default. To make an integer, place an L after the number:\n\n\ntypeof(1)\n\n\n[1] \"double\"\n\ntypeof(1L)\n\n\n[1] \"integer\"\n\n1.5L\n\n\n[1] 1.5\n\n\n\nThe distinction between integers and doubles is not usually important, but there are two important differences that you should be aware of:\nDoubles are approximations. Doubles represent floating point numbers that can not always be precisely represented with a fixed amount of memory. This means that you should consider all doubles to be approximations.\nFor example, what is square of the square root of two?\n\n\nx <- sqrt(2) ^ 2\nx\n\n\n[1] 2\n\nx - 2\n\n\n[1] 4.440892e-16\n\n\nThe most basic type of R object is a vector. Empty vectors can be created with the vector() function. There is really only one rule about vectors in R, which is that A vector can only contain objects of the same class.\nFor example, if you run this, what happens?\n\n\nvector(\"a\", 1)\n\n\n\nBut of course, like any good rule, there is an exception, which is a list, which we will get to a bit later. A list is represented as a vector but can contain objects of different classes. Indeed, that’s usually why we use them.\nThere is also a class for “raw” objects, but they are not commonly used directly in data analysis and I won’t cover them here.\nNumbers\nNumbers in R are generally treated as numeric objects (i.e. double precision real numbers). This means that even if you see a number like “1” or “2” in R, which you might think of as integers, they are likely represented behind the scenes as numeric objects (so something like “1.00” or “2.00”). This isn’t important most of the time…except when it is.\nIf you explicitly want an integer, you need to specify the L suffix. So entering 1 in R gives you a numeric object; entering 1L explicitly gives you an integer object.\nThere is also a special number Inf which represents infinity. This allows us to represent entities like 1 / 0. This way, Inf can be used in ordinary calculations; e.g. 1 / Inf is 0.\nThe value NaN represents an undefined value (“not a number”); e.g. 0 / 0; NaN can also be thought of as a missing value (more on that later)\nAttributes\nR objects can have attributes, which are like metadata for the object. These metadata can be very useful in that they help to describe the object. For example, column names on a data frame help to tell us what data are contained in each of the columns. Some examples of R object attributes are\nnames, dimnames\ndimensions (e.g. matrices, arrays)\nclass (e.g. integer, numeric)\nlength\nother user-defined attributes/metadata\nAttributes of an object (if any) can be accessed using the attributes() function. Not all R objects contain attributes, in which case the attributes() function returns NULL.\nHowever, every vector has two key properties:\nIts type, which you can determine with typeof().\n\n\ntypeof(letters)\n\n\n[1] \"character\"\n\ntypeof(1:10)\n\n\n[1] \"integer\"\n\nIts length, which you can determine with length().\n\n\nx <- list(\"a\", \"b\", 1:10)\nlength(x)\n\n\n[1] 3\n\ntypeof(x)\n\n\n[1] \"list\"\n\nCreating Vectors\nThe c() function can be used to create vectors of objects by concatenating things together.\n\n\nx <- c(0.5, 0.6)       ## numeric\nx <- c(TRUE, FALSE)    ## logical\nx <- c(T, F)           ## logical\nx <- c(\"a\", \"b\", \"c\")  ## character\nx <- 9:29              ## integer\nx <- c(1+0i, 2+4i)     ## complex\n\n\n\nNote that in the above example, T and F are short-hand ways to specify TRUE and FALSE. However, in general one should try to use the explicit TRUE and FALSE values when indicating logical values. The T and F values are primarily there for when you’re feeling lazy.\nYou can also use the vector() function to initialize vectors.\n\n\nx <- vector(\"numeric\", length = 10) \nx\n\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\nMixing Objects\nThere are occasions when different classes of R objects get mixed together. Sometimes this happens by accident but it can also happen on purpose. So what happens with the following code?\n\n\ny <- c(1.7, \"a\")   ## character\ny <- c(TRUE, 2)    ## numeric\ny <- c(\"a\", TRUE)  ## character\n\n\n\nIn each case above, we are mixing objects of two different classes in a vector. But remember that the only rule about vectors says this is not allowed. When different objects are mixed in a vector, coercion occurs so that every element in the vector is of the same class.\nIn the example above, we see the effect of implicit coercion. What R tries to do is find a way to represent all of the objects in the vector in a reasonable fashion. Sometimes this does exactly what you want and…sometimes not. For example, combining a numeric object with a character object will create a character vector, because numbers can usually be easily represented as strings.\nExplicit Coercion\nObjects can be explicitly coerced from one class to another using the as.* functions, if available.\n\n\nx <- 0:6\nclass(x)\n\n\n[1] \"integer\"\n\nas.numeric(x)\n\n\n[1] 0 1 2 3 4 5 6\n\nas.logical(x)\n\n\n[1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n\nas.character(x)\n\n\n[1] \"0\" \"1\" \"2\" \"3\" \"4\" \"5\" \"6\"\n\nSometimes, R can’t figure out how to coerce an object and this can result in NAs being produced.\n\n\nx <- c(\"a\", \"b\", \"c\")\nas.numeric(x)\n\n\n[1] NA NA NA\n\nas.logical(x)\n\n\n[1] NA NA NA\n\nas.complex(x)\n\n\n[1] NA NA NA\n\nWhen nonsensical coercion takes place, you will usually get a warning from R.\nMatrices\nMatrices are vectors with a dimension attribute. The dimension attribute is itself an integer vector of length 2 (number of rows, number of columns)\n\n\nm <- matrix(nrow = 2, ncol = 3) \nm\n\n\n     [,1] [,2] [,3]\n[1,]   NA   NA   NA\n[2,]   NA   NA   NA\n\ndim(m)\n\n\n[1] 2 3\n\nattributes(m)\n\n\n$dim\n[1] 2 3\n\nMatrices are constructed column-wise, so entries can be thought of starting in the “upper left” corner and running down the columns.\n\n\nm <- matrix(1:6, nrow = 2, ncol = 3) \nm\n\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\nMatrices can also be created directly from vectors by adding a dimension attribute.\n\n\nm <- 1:10 \nm\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\ndim(m) <- c(2, 5)\nm\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    3    5    7    9\n[2,]    2    4    6    8   10\n\nMatrices can be created by column-binding or row-binding with the cbind() and rbind() functions.\n\n\nx <- 1:3\ny <- 10:12\ncbind(x, y)\n\n\n     x  y\n[1,] 1 10\n[2,] 2 11\n[3,] 3 12\n\nrbind(x, y) \n\n\n  [,1] [,2] [,3]\nx    1    2    3\ny   10   11   12\n\nLists\nLists are a special type of vector that can contain elements of different classes. Lists are a very important data type in R and you should get to know them well. Lists, in combination with the various “apply” functions discussed later, make for a powerful combination.\nLists can be explicitly created using the list() function, which takes an arbitrary number of arguments.\n\n\nx <- list(1, \"a\", TRUE, 1 + 4i) \nx\n\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] \"a\"\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] 1+4i\n\nWe can also create an empty list of a prespecified length with the vector() function\n\n\nx <- vector(\"list\", length = 5)\nx\n\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n[[4]]\nNULL\n\n[[5]]\nNULL\n\nFactors\nFactors are used to represent categorical data and can be unordered or ordered. One can think of a factor as an integer vector where each integer has a label. Factors are important in statistical modeling and are treated specially by modelling functions like lm() and glm().\nUsing factors with labels is better than using integers because factors are self-describing. Having a variable that has values “Yes” and “No” or “Smoker” and “Non-Smoker” is better than a variable that has values 1 and 2.\nFactor objects can be created with the factor() function.\n\n\nx <- factor(c(\"yes\", \"yes\", \"no\", \"yes\", \"no\")) \nx\n\n\n[1] yes yes no  yes no \nLevels: no yes\n\ntable(x) \n\n\nx\n no yes \n  2   3 \n\n## See the underlying representation of factor\nunclass(x)  \n\n\n[1] 2 2 1 2 1\nattr(,\"levels\")\n[1] \"no\"  \"yes\"\n\nOften factors will be automatically created for you when you read a dataset in using a function like read.table(). Those functions often default to creating factors when they encounter data that look like characters or strings.\nThe order of the levels of a factor can be set using the levels argument to factor(). This can be important in linear modelling because the first level is used as the baseline level.\n\n\nx <- factor(c(\"yes\", \"yes\", \"no\", \"yes\", \"no\"))\nx  ## Levels are put in alphabetical order\n\n\n[1] yes yes no  yes no \nLevels: no yes\n\nx <- factor(c(\"yes\", \"yes\", \"no\", \"yes\", \"no\"),\n            levels = c(\"yes\", \"no\"))\nx\n\n\n[1] yes yes no  yes no \nLevels: yes no\n\nMissing Values\nMissing values are denoted by NA or NaN for q undefined mathematical operations.\nis.na() is used to test objects if they are NA\nis.nan() is used to test for NaN\nNA values have a class also, so there are integer NA, character NA, etc.\nA NaN value is also NA but the converse is not true\n\n\n## Create a vector with NAs in it\nx <- c(1, 2, NA, 10, 3)  \n## Return a logical vector indicating which elements are NA\nis.na(x)    \n\n\n[1] FALSE FALSE  TRUE FALSE FALSE\n\n## Return a logical vector indicating which elements are NaN\nis.nan(x)   \n\n\n[1] FALSE FALSE FALSE FALSE FALSE\n\n\n\n## Now create a vector with both NA and NaN values\nx <- c(1, 2, NaN, NA, 4)\nis.na(x)\n\n\n[1] FALSE FALSE  TRUE  TRUE FALSE\n\nis.nan(x)\n\n\n[1] FALSE FALSE  TRUE FALSE FALSE\n\nData Frames\nData frames are used to store tabular data in R. They are an important type of object in R and are used in a variety of statistical modeling applications. Hadley Wickham’s package dplyr has an optimized set of functions designed to work efficiently with data frames.\nData frames are represented as a special type of list where every element of the list has to have the same length. Each element of the list can be thought of as a column and the length of each element of the list is the number of rows.\nUnlike matrices, data frames can store different classes of objects in each column. Matrices must have every element be the same class (e.g. all integers or all numeric).\nIn addition to column names, indicating the names of the variables or predictors, data frames have a special attribute called row.names which indicate information about each row of the data frame.\nData frames are usually created by reading in a dataset using the read.table() or read.csv(). However, data frames can also be created explicitly with the data.frame() function or they can be coerced from other types of objects like lists.\nData frames can be converted to a matrix by calling data.matrix(). While it might seem that the as.matrix() function should be used to coerce a data frame to a matrix, almost always, what you want is the result of data.matrix().\n\n\nx <- data.frame(foo = 1:4, bar = c(T, T, F, F)) \nx\n\n\n  foo   bar\n1   1  TRUE\n2   2  TRUE\n3   3 FALSE\n4   4 FALSE\n\nnrow(x)\n\n\n[1] 4\n\nncol(x)\n\n\n[1] 2\n\nNames\nR objects can have names, which is very useful for writing readable code and self-describing objects. Here is an example of assigning names to an integer vector.\n\n\nx <- 1:3\nnames(x)\n\n\nNULL\n\nnames(x) <- c(\"New York\", \"Seattle\", \"Los Angeles\") \nx\n\n\n   New York     Seattle Los Angeles \n          1           2           3 \n\nnames(x)\n\n\n[1] \"New York\"    \"Seattle\"     \"Los Angeles\"\n\nLists can also have names, which is often very useful.\n\n\nx <- list(\"Los Angeles\" = 1, Boston = 2, London = 3) \nx\n\n\n$`Los Angeles`\n[1] 1\n\n$Boston\n[1] 2\n\n$London\n[1] 3\n\nnames(x)\n\n\n[1] \"Los Angeles\" \"Boston\"      \"London\"     \n\nMatrices can have both column and row names.\n\n\nm <- matrix(1:4, nrow = 2, ncol = 2)\ndimnames(m) <- list(c(\"a\", \"b\"), c(\"c\", \"d\")) \nm\n\n\n  c d\na 1 3\nb 2 4\n\nColumn names and row names can be set separately using the colnames() and rownames() functions.\n\n\ncolnames(m) <- c(\"h\", \"f\")\nrownames(m) <- c(\"x\", \"z\")\nm\n\n\n  h f\nx 1 3\nz 2 4\n\nNote that for data frames, there is a separate function for setting the row names, the row.names() function. Also, data frames do not have column names, they just have names (like lists). So to set the column names of a data frame just use the names() function. Yes, I know its confusing. Here’s a quick summary:\nObject\nSet column names\nSet row names\ndata frame\nnames()\nrow.names()\nmatrix\ncolnames()\nrownames()\nSummary\nThere are a variety of different builtin-data types in R. In this chapter we have reviewed the following\natomic classes: numeric, logical, character, integer, complex\nvectors, lists\nfactors\nmissing values\ndata frames and matrices\nAll R objects can have attributes that help to describe what is in the object. Perhaps the most useful attribute is names, such as column and row names in a data frame, or simply names in a vector or list. Attributes like dimensions are also important as they can modify the behavior of objects, like turning a vector into a matrix.\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nDescribe the difference between is.finite(x) and !is.infinite(x).\nA logical vector can take 3 possible values. How many possible values can an integer vector take? How many possible values can a double take? Use google to do some research.\nWhat functions from the readr package allow you to turn a string into logical, integer, and double vector?\nTry and make a tibble that has columns with different lengths. What happens?\n\nAdditional Resources\n\nhttps://rdpeng.github.io/Biostat776/lecture-r-nuts-and-bolts.html\nhttps://rafalab.github.io/dsbook/r-basics.html\nhttps://r4ds.had.co.nz/vectors.html?q=typeof#vectors\n\n\n\n\n",
    "preview": "https://d33wubrfki0l68.cloudfront.net/1d1b4e1cf0dc5f6e80f621b0225354b0addb9578/6ee1c/diagrams/data-structures-overview.png",
    "last_modified": "2021-09-16T12:48:53-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-14-ggplot2-plotting-system-part-1/",
    "title": "The ggplot2 plotting system: qplot()",
    "description": "An overview of the ggplot2 plotting system in R with qplot().",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-14",
    "categories": [
      "module 1",
      "week 3",
      "R",
      "programming",
      "ggplot2",
      "data viz"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nThe ggplot2 Plotting System\nThe Basics: qplot()\nBefore you start: label your data\nggplot2 “Hello, world!”\nModifying aesthetics\nAdding a geom\nHistograms and boxplots\nFacets\nSummary\n\nPost-lecture materials\nCase Study: MAACS Cohort\nFinal Questions\nAdditional Resources\n\n\n\n“The greatest value of a picture is when it forces us to notice what we never expected to see.” —John Tukey\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://r4ds.had.co.nz/data-visualisation.html\nhttp://vita.had.co.nz/papers/layered-grammar.pdf\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-the-ggplot2-plotting-system-part-1.html\nLearning objectives\n\nAt the end of this lesson you will:\nRecognize the difference between aesthetics and geoms\nBecome familiar with different types of plots (e.g. scatterplots, boxplots, and histograms)\nBe able to facet plots into a grid\n\nThe ggplot2 Plotting System\nThe ggplot2 package in R is an implementation of The Grammar of Graphics as described by Leland Wilkinson in his book. The package was originally written by Hadley Wickham while he was a graduate student at Iowa State University (he still actively maintains the packgae). The package implements what might be considered a third graphics system for R (along with base graphics and lattice). The package is available from CRAN via install.packages(); the latest version of the source can be found on the package’s GitHub Repository. Documentation of the package can be found at the tidyverse web site.\nThe grammar of graphics represents an abstraction of graphics ideas and objects. You can think of this as developing the verbs, nouns, and adjectives for data graphics. Developing such a grammar allows for a “theory” of graphics on which to build new graphics and graphics objects. To quote from Hadley Wickham’s book on ggplot2, we want to “shorten the distance from mind to page”. In summary,\n\n“…the grammar tells us that a statistical graphic is a mapping from data to aesthetic attributes (colour, shape, size) of geometric objects (points, lines, bars). The plot may also contain statistical transformations of the data and is drawn on a specific coordinate system” – from ggplot2 book\n\nYou might ask yourself “Why do we need a grammar of graphics?” Well, for much the same reasons that having a grammar is useful for spoken languages. The grammar allows for a more compact summary of the base components of a language, and it allows us to extend the language and to handle situations that we have not before seen.\nIf you think about making a plot with the base graphics system, the plot is constructed by calling a series of functions that either create or annotate a plot. There’s no convenient agreed-upon way to describe the plot, except to just recite the series of R functions that were called to create the thing in the first place. In a previous lesson, we described the base plotting system as a kind of “artist’s palette” model, where you start with blank “canvas” and build up from there.\nFor example, consider the following plot made using base graphics.\n\n\nwith(airquality, { \n        plot(Temp, Ozone)\n        lines(loess.smooth(Temp, Ozone))\n})\n\n\n\n\nFigure 1: Scatterplot of Temperature and Ozone in New York (base graphics)\n\n\n\nHow would one describe the creation of this plot? Well, we could say that we called the plot() function and then added a loess smoother by calling the lines() function on the output of loess.smooth().\nThe base plotting system is convenient and it often mirrors how we think of building plots and analyzing data. But a key drawback is that you can’t go back once plot has started (e.g. to adjust margins), so there is in fact a need to plan in advance. Furthermore, it is difficult to “translate” a plot to others because there’s no formal graphical language; each plot is just a series of R commands.\nHere is the same plot made using ggplot2.\n\n\nlibrary(tidyverse)\nairquality %>%\n        ggplot(aes(Temp, Ozone)) + \n        geom_point() + \n        geom_smooth(method = \"loess\", \n                    se = FALSE) + \n        theme_minimal()\n\n\n\n\nFigure 2: Scatterplot of Temperature and Ozone in New York (ggplot2)\n\n\n\nNote: the output is roughly equivalent, and the amount of code is similar, but ggplot2 allows for a more elegant way of expressing the components of the plot. In this case, the plot is a dataset (airquality) with aesthetic mappings (visual properties of the objects in your plot) derived from the Temp and Ozone variables, a set of points, and a smoother. In a sense, the ggplot2 system takes many of the cues from the base plotting system and formalizes them a bit.\nThe ggplot2 system also takes some cues from lattice. With the lattice system, plots are created with a single function call (xyplot, bwplot, etc.). Things like margins and spacing are set automatically because the entire plot is specified at once. The lattice system is most useful for conditioning types of plots and is good for putting many many plots on a screen. That said, it is sometimes awkward to specify an entire plot in a single function call because many different options have to be specified at once. Furthermore, annotation in plots is not intuitive and the use of panel functions and subscripts is difficult to wield and requires intense preparation.\nThe ggplot2 system essentially takes the good parts of both the base graphics and lattice graphics system. It automatically handles things like margins and spacing, and also has the concept of “themes” which provide a default set of plotting symbols and colors. While ggplot2 bears a superficial similarity to lattice, ggplot2 is generally easier and more intuitive to use. The default themes makes many choices for you, but you can customize the presentation if you want.\nThe Basics: qplot()\nThe qplot() function in ggplot2 is meant to get you going quickly. It works much like the plot() function in base graphics system. It looks for variables to plot within a data frame, similar to lattice, or in the parent environment. In general, it is good to get used to putting your data in a data frame and then passing it to qplot().\n\nPro tip: The qplot() function is somewhat discouraged in ggplot2 now and new users are encouraged to use the more general ggplot() function (more details in the next lesson).\nHowever, the qplot() function is still useful and may be easier to use if transitioning from the base plotting system or a different statistical package.\n\nPlots are made up of aesthetics (e.g. size, shape, color) and geoms (e.g. points, lines). Factors play an important role for indicating subsets of the data (if they are to have different properties) so they should be labeled properly. The qplot() hides much of what goes on underneath, which is okay for most operations, ggplot() is the core function and is very flexible for doing things qplot() cannot do.\nBefore you start: label your data\nOne thing that is always true, but is particularly useful when using ggplot2, is that you should always use informative and descriptive labels on your data. More generally, your data should have appropriate metadata so that you can quickly look at a dataset and know\nwhat the variables are\nwhat the values of each variable mean\nThis means that each column of a data frame should have a meaningful (but concise) variable name that accurately reflects the data stored in that column. Also, non-numeric or categorical variables should be coded as factor variables and have meaningful labels for each level of the factor. For example, it is common to code a binary variable as a “0” or a “1”, but the problem is that from quickly looking at the data, it’s impossible to know whether which level of that variable is represented by a “0” or a “1”. Much better to simply label each observation as what they are. If a variable represents temperature categories, it might be better to use “cold”, “mild”, and “hot” rather than “1”, “2”, and “3”.\nWhile it is sometimes a pain to make sure all of your data are properly labeled, this investment in time can pay dividends down the road when you’re trying to figure out what you were plotting. In other words, including the proper metadata can make your exploratory plots essentially self-documenting.\nggplot2 “Hello, world!”\nThis example dataset comes with the ggplot2 package and contains data on the fuel economy of 38 popular car models from 1999 to 2008.\n\n\nlibrary(tidyverse) # this loads the ggplot2 R package\n# library(ggplot2) # an alternative way to just load the ggplot2 R package\nglimpse(mpg)\n\n\nRows: 234\nColumns: 11\n$ manufacturer <chr> \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\",…\n$ model        <chr> \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 q…\n$ displ        <dbl> 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.…\n$ year         <int> 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999,…\n$ cyl          <int> 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6,…\n$ trans        <chr> \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"auto(a…\n$ drv          <chr> \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\", \"4…\n$ cty          <int> 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15,…\n$ hwy          <int> 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25,…\n$ fl           <chr> \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p…\n$ class        <chr> \"compact\", \"compact\", \"compact\", \"compact\", \"co…\n\nYou can see from the glimpse() (part of the dplyr package) output that all of the categorical variables (like “manufacturer” or “class”) are appropriately coded with meaningful labels. This will come in handy when qplot() has to label different aspects of a plot. Also note that all of the columns/variables have meaningful (if sometimes abbreviated) names, rather than names like “X1”, and “X2”, etc.\nWe can make a quick scatterplot of the engine displacement (displ) and the highway miles per gallon (hwy).\n\n\nqplot(x = displ, y = hwy, data = mpg)\n\n\n\n\nFigure 3: Plot of engine displacement and highway mileage using the mtcars dataset\n\n\n\nIt has a very similar feeling to plot() in base R.\n\nNote: In the call to qplot() you must specify the data argument so that qplot() knows where to look up the variables.\nYou must also specify x and y, but hopefully that part is obvious.\n\nModifying aesthetics\nWe can introduce a third variable into the plot by modifying the color of the points based on the value of that third variable.\nColor (or colour) is one type of aesthetic and using the ggplot2 language:\n\n“the color of each point can be mapped to a variable”.\n\nThis sounds technical, but let’s give an example.\n\nNote: the x-coordinates and y-coordinates are aesthetics too, and they got mapped to the displ and hwy variables, respectively\n\nHere, we will map the color to the drv variable, which indicates whether a car is front wheel drive, rear wheel drive, or 4-wheel drive.\n\n\nqplot(displ, hwy, data = mpg, color = drv)\n\n\n\n\nFigure 4: Engine displacement and highway mileage by drive class\n\n\n\nNow we can see that the front wheel drive cars tend to have lower displacement relative to the 4-wheel or rear wheel drive cars. Also, it’s clear that the 4-wheel drive cars have the lowest highway gas mileage.\n\nQuestion: In the above plot, I did not specify the x and y variable. What happens when you run these two code chunks. What’s the difference?\n\n\nqplot(displ, hwy, data = mpg, color = drv)\n\n\n\n\n\nqplot(x = displ, y = hwy, data = mpg, color = drv)\n\n\n\n\n\nqplot(hwy, displ, data = mpg, color = drv)\n\n\n\n\n\nqplot(y = hwy, x = displ, data = mpg, color = drv)\n\n\n\n\n\nExample: Let’s try mapping colors in another dataset, namely the palmerpenguins dataset. These data contain observations for 344 penguins. There are 3 different species of penguins in this dataset, collected from 3 islands in the Palmer Archipelago, Antarctica.\n\n\nlibrary(palmerpenguins)\nglimpse(penguins)\n\n\nRows: 344\nColumns: 8\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Ad…\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen…\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39…\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19…\n$ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181, 195, 193…\n$ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650, 3625, 46…\n$ sex               <fct> male, female, female, NA, female, male, fe…\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, …\n\nIf we wanted to count the number of penguins for each of the three species, we can use the count() function in dplyr:\n\n\npenguins %>% \n  count(species)\n\n\n# A tibble: 3 × 2\n  species       n\n  <fct>     <int>\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\n\n\nFigure 5: Palmer penguins\n\n\n\n[Source: Artwork by Allison Horst]\nQuestion: If we wanted to use qplot() to map flipper_length_mm and bill_length_mm to the x and y coordinates, what would we do?\n\n\n# try it yourself\n\n\n\nNow try mapping color to the species variable on top of the code you just wrote:\n\n\n# try it yourself\n\n\n\n\nAdding a geom\nSometimes it is nice to add a smoother to a scatterplot to highlight any trends. Trends can be difficult to see if the data are very noisy or there are many data points obscuring the view. A smooth is a “geom” that you can add along with your data points.\n\n\nqplot(displ, hwy, data = mpg, geom = c(\"point\", \"smooth\"))\n\n\n\n\nFigure 6: Engine displacement and highway mileage w/smoother\n\n\n\nNote that previously, we did not have to specify geom = \"point\" because that was done automatically. But if you want the smoother overlaid with the points, then you need to specify both explicitly.\nHere it seems that engine displacement and highway mileage have a nonlinear U-shaped relationship, but from the previous plot we know that this is largely due to confounding by the drive class of the car.\nLook at what happens if we do not include the point geom.\n\n\nqplot(displ, hwy, data = mpg, geom = c(\"smooth\"))\n\n\n\n\nFigure 7: Engine displacement and highway mileage w/smoother\n\n\n\nSometimes that is the plot you want to show, but in this case it might make more sense to show the data along with the smoother.\n\nExample: Let’s add a smoother to our palmerpenguins dataset example. Using the code we previously wrote mapping variables to points and color, add a “point” and “smooth” geom:\n\n\n# try it yourself\n\n\n\n\nHistograms and boxplots\nThe qplot() function can be used to be used to plot 1-dimensional data too. By specifying a single variable, qplot() will by default make a histogram.\nHere, we make a histogram if the highway mileage data and stratify on the drive class. So technically this is three histograms overlayed on top of each other.\n\n\nqplot(hwy, data = mpg, fill = drv, binwidth = 2)\n\n\n\n\nFigure 8: Histogram of highway mileage by drive class\n\n\n\n\nQuestion: Notice, I used fill here to map color to the drv variable. Why is this? What happens when you use color instead?\n\n\n# try it yourself\n\n\n\n\nHaving the different colors for each drive class is nice, but the three histograms can be a bit difficult to separate out. Side-by-side boxplots is one solution to this problem.\n\n\nqplot(drv, hwy, data = mpg, geom = \"boxplot\")\n\n\n\n\nFigure 9: Boxplots of highway mileage by drive class\n\n\n\nAnother solution is to plot the histograms in separate panels using facets.\nFacets\nFacets are a way to create multiple panels of plots based on the levels of categorical variable. Here, we want to see a histogram of the highway mileages and the categorical variable is the drive class variable. We can do that using the facets argument to qplot().\nThe facets argument expects a formula type of input, with a ~ separating the left hand side variable and the right hand side variable. The left hand side variable indicates how the rows of the panels should be divided and the right hand side variable indicates how the columns of the panels should be divided. Here, we just want three rows of histograms (and just one column), one for each drive class, so we specify drv on the left hand side and . on the right hand side indicating that there’s no variable there (it’s empty).\n\n\nqplot(hwy, data = mpg, facets = drv ~ ., binwidth = 2)\n\n\n\n\nFigure 10: Histogram of highway mileage by drive class\n\n\n\nWe could also look at more data using facets, so instead of histograms we could look at scatterplots of engine displacement and highway mileage by drive class. Here, we put the drv variable on the right hand side to indicate that we want a column for each drive class (as opposed to splitting by rows like we did above).\n\n\nqplot(displ, hwy, data = mpg, facets = . ~ drv)\n\n\n\n\nFigure 11: Engine displacement and highway mileage by drive class\n\n\n\nWhat if you wanted to add a smoother to each one of those panels? Simple, you literally just add the smoother as another geom.\n\n\nqplot(displ, hwy, data = mpg, facets = . ~ drv) + geom_smooth(method = \"lm\")\n\n\n\n\nFigure 12: Engine displacement and highway mileage by drive class w/smoother\n\n\n\n\nNote: we used a different type of smoother. Here, we add a linear regression line (a type of smoother) to each group to see if there’s any difference.\n\n\nExample: Let’s facet our palmerpenguins dataset example and explore different types of plots.\nBuilding off the code we previously wrote, perform the following tasks:\nFacet the plot based on species with the the three species along rows.\nAdd a linear regression line to each the types of species\n\n\n# try it yourself\n\n\n\nNext, make a histogram of the body_mass_g for each of the species colored by the three species.\n\n\n# try it yourself\n\n\n\n\nSummary\nThe qplot() function in ggplot2 is the analog of plot() in base graphics but with many built-in features that the traditionaly plot() does not provide. The syntax is somewhere in between the base and lattice graphics system. The qplot() function is useful for quickly putting data on the page/screen, but for ultimate customization, it may make more sense to use some of the lower level functions that we discuss later in the next lesson.\nPost-lecture materials\nCase Study: MAACS Cohort\n\nClick here for case study practicing the qplot() function.\nThis case study will use data based on the Mouse Allergen and Asthma Cohort Study (MAACS). This study was aimed at characterizing the indoor (home) environment and its relationship with asthma morbidity amonst children aged 5–17 living in Baltimore, MD. The children all had persistent asthma, defined as having had an exacerbation in the past year. A representative publication of results from this study can be found in this paper by Lu, et al.\n\nBecause the individual-level data for this study are protected by various U.S. privacy laws, we cannot make those data available. For the purposes of this lesson, we have simulated data that share many of the same features of the original data, but do not contain any of the actual measurements or values contained in the original dataset.\n\nHere is a snapshot of what the data look like.\n\n\nlibrary(here)\nmaacs <- read_csv(here(\"data\", \"maacs_sim.csv\"), col_types = \"icnn\")\nmaacs\n\n\n# A tibble: 750 × 4\n      id mopos  pm25    eno\n   <int> <chr> <dbl>  <dbl>\n 1     1 yes    6.01  28.8 \n 2     2 no    25.2   17.7 \n 3     3 yes   21.8   43.6 \n 4     4 no    13.4  288.  \n 5     5 no    49.4    7.60\n 6     6 no    43.4   12.0 \n 7     7 yes   33.0   79.2 \n 8     8 yes   32.7   34.2 \n 9     9 yes   52.2   12.1 \n10    10 yes   51.9   65.0 \n# … with 740 more rows\n\nThe key variables are:\nmopos: an indicator of whether the subject is allergic to mouse allergen (yes/no)\npm25: average level of PM2.5 over the course of 7 days (micrograms per cubic meter)\neno: exhaled nitric oxide\nThe outcome of interest for this analysis will be exhaled nitric oxide (eNO), which is a measure of pulmonary inflamation. We can get a sense of how eNO is distributed in this population by making a quick histogram of the variable. Here, we take the log of eNO because some right-skew in the data.\n\n\nqplot(log(eno), data = maacs)\n\n\n\n\nFigure 13: Histogram of log eNO\n\n\n\nA quick glance suggests that the histogram is a bit “fat”, suggesting that there might be multiple groups of people being lumped together. We can stratify the histogram by whether they are allergic to mouse.\n\n\nqplot(log(eno), data = maacs, fill = mopos)\n\n\n\n\nFigure 14: Histogram of log eNO by mouse allergic status\n\n\n\nWe can see from this plot that the non-allergic subjects are shifted slightly to the left, indicating a lower eNO and less pulmonary inflammation. That said, there is significant overlap between the two groups.\nAn alternative to histograms is a density smoother, which sometimes can be easier to visualize when there are multiple groups. Here is a density smooth of the entire study population.\n\n\nqplot(log(eno), data = maacs, geom = \"density\")\n\n\n\n\nFigure 15: Density smooth of log eNO\n\n\n\nAnd here are the densities straitified by allergic status. We can map the color aesthetic to the mopos variable.\n\n\nqplot(log(eno), data = maacs, geom = \"density\", color = mopos)\n\n\n\n\nFigure 16: Density smooth of log eNO by mouse allergic status\n\n\n\nThese tell the same story as the stratified histograms, which should come as no surprise.\nNow we can examine the indoor environment and its relationship to eNO. Here, we use the level of indoor PM2.5 as a measure of indoor environment air quality. We can make a simple scatterplot of PM2.5 and eNO.\n\n\nqplot(log(pm25), log(eno), data = maacs, geom = c(\"point\", \"smooth\"))\n\n\n\n\nFigure 17: eNO and PM2.5\n\n\n\nThe relationship appears modest at best, as there is substantial noise in the data. However, one question that we might be interested in is whether allergic individuals are perhaps more sensitive to PM2.5 inhalation than non-allergic individuals. To examine that question we can stratify the data into two groups.\nThis first plot uses different plot symbols for the two groups and overlays them on a single canvas. We can do this by mapping the mopos variable to the shape aesthetic.\n\n\nqplot(log(pm25), log(eno), data = maacs, shape = mopos)\n\n\n\n\nFigure 18: eNO and PM2.5 by mouse allergic status\n\n\n\nBecause there is substantial overlap in the data it is a bit challenging to discern the circles from the triangles. Part of the reason might be that all of the symbols are the same color (black).\nWe can plot each group a different color to see if that helps.\n\n\nqplot(log(pm25), log(eno), data = maacs, color = mopos)\n\n\n\n\nFigure 19: eNO and PM2.5 by mouse allergic status\n\n\n\nThis is slightly better but the substantial overlap makes it difficult to discern any trends in the data. For this we need to add a smoother of some sort. Here we add a linear regression line (a type of smoother) to each group to see if there’s any difference.\n\n\nqplot(log(pm25), log(eno), data = maacs, color = mopos) + \n        geom_smooth(method = \"lm\")\n\n\n\n\nHere we see quite clearly that the red group and the green group exhibit rather different relationships between PM2.5 and eNO. For the non-allergic individuals, there appears to be a slightly negative relationship between PM2.5 and eNO and for the allergic individuals, there is a positive relationship. This suggests a strong interaction between PM2.5 and allergic status, an hypothesis perhaps worth following up on in greater detail than this brief exploratory analysis.\nAnother, and perhaps more clear, way to visualize this interaction is to use separate panels for the non-allergic and allergic individuals using the facets argument to qplot().\n\n\nqplot(log(pm25), log(eno), data = maacs, facets = . ~ mopos) + \n        geom_smooth(method = \"lm\")\n\n\n\n\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nWhat is gone wrong with this code? Why are the points not blue?\n\n\nqplot(x = displ, y = hwy, data = mpg, color = \"blue\")\n\n\n\n\nWhich variables in mpg are categorical? Which variables are continuous? (Hint: type ?mpg to read the documentation for the dataset). How can you see this information when you run mpg?\nMap a continuous variable to color, size, and shape aesthetics. How do these aesthetics behave differently for categorical vs. continuous variables?\n\nAdditional Resources\n\nhttps://r4ds.had.co.nz/data-visualisation.html\nhttps://rdpeng.github.io/Biostat776/lecture-the-ggplot2-plotting-system-part-1.html\n\n\n\n\n",
    "preview": "posts/2021-09-14-ggplot2-plotting-system-part-1/ggplot2-plotting-system-part-1_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-09-13T21:15:21-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 960
  },
  {
    "path": "posts/2021-09-14-ggplot2-plotting-system-part-2/",
    "title": "The ggplot2 plotting system: ggplot()",
    "description": "An overview of the ggplot2 plotting system in R with ggplot().",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-14",
    "categories": [
      "module 1",
      "week 3",
      "R",
      "programming",
      "ggplot2",
      "data viz"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nThe ggplot2 Plotting System\nBasic components of a ggplot2 plot\nExample: BMI, PM2.5, Asthma\nBuilding up in layers\nFirst plot with point layer\nAdding more layers: smooth\nAdding more layers: facets\nModifying geom properties\nModifying labels\nCustomizing the smooth\nChanging the theme\nMore complex example\nA quick aside about axis limits\nResources\n\n\nPost-lecture materials\nFinal Questions\nAdditional Resources\n\n\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://r4ds.had.co.nz/data-visualisation.html\nhttp://vita.had.co.nz/papers/layered-grammar.pdf\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-the-ggplot2-plotting-system-part-2.html\nLearning objectives\n\nAt the end of this lesson you will:\nBe able to build up layers of graphics using ggplot()\nBe able to modify properties of a ggplot() including layers and labels\n\nThe ggplot2 Plotting System\nIn this lesson, we will get into a little more of the nitty gritty of how ggplot2 builds plots and how you can customize various aspects of any plot.\nIn the previous lesson, we used the qplot() function to quickly put points on a page. The qplot() function’s syntax is very similar to that of the plot() function in base graphics so for those switching over, it makes for an easy transition. But it is worth knowing the underlying details of how ggplot2 works so that you can really exploit its power.\nBasic components of a ggplot2 plot\nA ggplot2 plot consists of a number of key components. Here are a few of the more commonly used ones.\nA data frame: stores all of the data that will be displayed on the plot\naesthetic mappings: describe how data are mapped to color, size, shape, location\ngeoms: geometric objects like points, lines, shapes.\nfacets: describes how conditional/panel plots should be constructed\nstats: statistical transformations like binning, quantiles, smoothing.\nscales: what scale an aesthetic map uses (example: left-handed = red, right-handed = blue).\ncoordinate system: describes the system in which the locations of the geoms will be drawn\nIt is essential that you properly organize your data into a data frame before you start with ggplot2. In particular, it is important that you provide all of the appropriate metadata so that your data frame is self-describing and your plots will be self-documenting.\nWhen building plots in ggplot2 (rather than using qplot()), the “artist’s palette” model may be the closest analogy. Essentially, you start with some raw data, and then you gradually add bits and pieces to it to create a plot. Plots are built up in layers, with the typically ordering being\nPlot the data\nOverlay a summary\nAdd metadata and annotation\nFor quick exploratory plots you may not get past step 1.\nExample: BMI, PM2.5, Asthma\nTo demonstrate the various pieces of ggplot2 we will use a running example from the Mouse Allergen and Asthma Cohort Study (MAACS), which was used as a case study in the previous lesson. Here, the question we are interested in is\n\n“Are overweight individuals, as measured by body mass index (BMI), more susceptible than normal weight individuals to the harmful effects of PM2.5 on asthma symptoms?”\n\nThere is a suggestion that overweight individuals may be more susceptible to the negative effects of inhaling PM2.5. This would suggest that increases in PM2.5 exposure in the home of an overweight child would be more deleterious to his/her asthma symptoms than they would be in the home of a normal weight child. We want to see if we can see that difference in the data from MAACS.\n\nBecause the individual-level data for this study are protected by various U.S. privacy laws, we cannot make those data available. For the purposes of this lesson, we have simulated data that share many of the same features of the original data, but do not contain any of the actual measurements or values contained in the original dataset.\n\nWe can look at the data quickly by reading it in as a tibble with read_csv() in the tidyverse package.\n\n\nlibrary(tidyverse)\nlibrary(here)\nmaacs <- read_csv(here(\"data\", \"bmi_pm25_no2_sim.csv\"),\n                  col_types = \"nnci\")\nmaacs\n\n\n# A tibble: 517 × 4\n   logpm25 logno2_new bmicat        NocturnalSympt\n     <dbl>      <dbl> <chr>                  <int>\n 1   1.25       1.18  normal weight              1\n 2   1.12       1.55  overweight                 0\n 3   1.93       1.43  normal weight              0\n 4   1.37       1.77  overweight                 2\n 5   0.775      0.765 normal weight              0\n 6   1.49       1.11  normal weight              0\n 7   2.16       1.43  normal weight              0\n 8   1.65       1.40  normal weight              0\n 9   1.55       1.81  normal weight              0\n10   2.04       1.35  overweight                 3\n# … with 507 more rows\n\nThe outcome we will look at here, NocturnalSymp, is the number of days in the past 2 weeks where the child experienced asthma symptoms (e.g. coughing, wheezing) while sleeping.\nThe other key variables are:\nlogpm25: average level of PM2.5 over the course of 7 days (micrograms per cubic meter) on the log scale\nlogno2_new: exhaled nitric oxide on the log scale\nbmicat: categorical variable with BMI status\nBuilding up in layers\nFirst, we can create a ggplot object that stores the dataset and the basic aesthetics for mapping the x- and y-coordinates for the plot. Here, we will eventually be plotting the log of PM2.5 and NocturnalSymp variable.\n\n\ng <- ggplot(maacs, aes(x = logpm25, y = NocturnalSympt))\nsummary(g)\n\n\ndata: logpm25, logno2_new, bmicat, NocturnalSympt [517x4]\nmapping:  x = ~logpm25, y = ~NocturnalSympt\nfaceting: <ggproto object: Class FacetNull, Facet, gg>\n    compute_layout: function\n    draw_back: function\n    draw_front: function\n    draw_labels: function\n    draw_panels: function\n    finish_data: function\n    init_scales: function\n    map_data: function\n    params: list\n    setup_data: function\n    setup_params: function\n    shrink: TRUE\n    train_scales: function\n    vars: function\n    super:  <ggproto object: Class FacetNull, Facet, gg>\n\nclass(g)\n\n\n[1] \"gg\"     \"ggplot\"\n\nYou can see above that the object g contains the dataset maacs and the mappings.\nNow, normally if you were to print() a ggplot object a plot would appear on the plot device, however, our object g actually does not contain enough information to make a plot yet.\n\n\ng <- maacs %>%\n        ggplot(aes(logpm25, NocturnalSympt))\nprint(g)\n\n\n\n\nFigure 1: Nothing to see here!\n\n\n\nFirst plot with point layer\nTo make a scatterplot we need add at least one geom, such as points. Here, we add the geom_point() function to create a traditional scatterplot.\n\n\ng <- maacs %>%\n        ggplot(aes(logpm25, NocturnalSympt))\ng + geom_point()\n\n\n\n\nHow does ggplot know what points to plot? In this case, it can grab them from the data frame maacs that served as the input into the ggplot() function.\nAdding more layers: smooth\nBecause the data appear rather noisy, it might be better if we added a smoother on top of the points to see if there is a trend in the data with PM2.5.\n\n\ng + geom_point() + geom_smooth()\n\n\n\n\nFigure 2: Scatterplot with smoother\n\n\n\nThe default smoother is a loess smoother, which is flexible and nonparametric but might be too flexible for our purposes. Perhaps we’d prefer a simple linear regression line to highlight any first order trends. We can do this by specifying method = \"lm\" to geom_smooth().\n\n\ng + geom_point() + geom_smooth(method = \"lm\")\n\n\n\n\nFigure 3: Scatterplot with linear regression line\n\n\n\nHere, we can see there appears to be a slight increasing trend, suggesting that higher levels of PM2.5 are associated with increased days with nocturnal symptoms.\n\nExample: Let’s use the ggplot() function with our palmerpenguins dataset example and make a scatter plot with flipper_length_mm on the x-axis, bill_length_mm on the y-axis, colored by species, and a smoother by adding a linear regression.\n\n\n# try it yourself\n\nlibrary(palmerpenguins)\npenguins \n\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm\n   <fct>   <fct>              <dbl>         <dbl>             <int>\n 1 Adelie  Torgersen           39.1          18.7               181\n 2 Adelie  Torgersen           39.5          17.4               186\n 3 Adelie  Torgersen           40.3          18                 195\n 4 Adelie  Torgersen           NA            NA                  NA\n 5 Adelie  Torgersen           36.7          19.3               193\n 6 Adelie  Torgersen           39.3          20.6               190\n 7 Adelie  Torgersen           38.9          17.8               181\n 8 Adelie  Torgersen           39.2          19.6               195\n 9 Adelie  Torgersen           34.1          18.1               193\n10 Adelie  Torgersen           42            20.2               190\n# … with 334 more rows, and 3 more variables: body_mass_g <int>,\n#   sex <fct>, year <int>\n\n\nAdding more layers: facets\nBecause our primary question involves comparing overweight individuals to normal weight individuals, we can stratify the scatter plot of PM2.5 and nocturnal symptoms by the BMI category (bmicat) variable, which indicates whether an individual is overweight or now. To visualize this we can add a facet_grid(), which takes a formula argument. Here we want one row and two columns, one column for each weight category. So we specify bmicat on the right hand side of the forumla passed to facet_grid().\n\n\ng + geom_point() + \n        geom_smooth(method = \"lm\") +\n        facet_grid(. ~ bmicat) \n\n\n\n\nFigure 4: Scatterplot of PM2.5 and nocturnal symptoms by BMI category\n\n\n\nNow it seems clear that the relationship between PM2.5 and nocturnal symptoms is relatively flat among normal weight individuals, while the relationship is increasing among overweight individuals. This plot suggests that overweight individuals may be more susceptible to the effects of PM2.5.\nThere are a variety of annotations you can add to a plot, including different kinds of labels. You can use xlab() for x-axis labels, ylab() for y-axis labels, and ggtitle() for specifying plot titles. The labs() function is generic and can be used to modify multiple types of labels at once.\nFor things that only make sense globally, use theme(), i.e. theme(legend.position = \"none\"). Two standard appearance themes are included\ntheme_gray(): The default theme (gray background)\ntheme_bw(): More stark/plain\nModifying geom properties\nYou can modify properties of geoms by specifying options to their respective geom_* functions. For example, here we modify the points in the scatterplot to make the color “steelblue”, the size larger , and the alpha transparency greater.\n\n\ng + geom_point(color = \"steelblue\", size = 4, alpha = 1/2)\n\n\n\n\nFigure 5: Modifying point color with a constant\n\n\n\nIn addition to setting specific geom attributes to constants, we can map aesthetics to variables. So, here, we map the color aesthetic color to the variable bmicat, so the points will be colored according to the levels of bmicat. We use the aes() function to indicate this difference from the plot above.\n\n\ng + geom_point(aes(color = bmicat), size = 4, alpha = 1/2)\n\n\n\n\nFigure 6: Mapping color to a variable\n\n\n\nModifying labels\nHere is an example of modifying the title and the x and y labels to make the plot a bit more informative.\n\n\ng + geom_point(aes(color = bmicat)) + \n        labs(title = \"MAACS Cohort\") + \n        labs(x = expression(\"log \" * PM[2.5]), y = \"Nocturnal Symptoms\")\n\n\n\n\nFigure 7: Modifying plot labels\n\n\n\nCustomizing the smooth\nWe can also customize aspects of the smoother that we overlay on the points with geom_smooth(). Here we change the line type and increase the size from the default. We also remove the shaded standard error from the line.\n\n\ng + geom_point(aes(color = bmicat), \n               size = 2, alpha = 1/2) + \n        geom_smooth(size = 4, linetype = 3, \n                    method = \"lm\", se = FALSE)\n\n\n\n\nFigure 8: Customizing a smoother\n\n\n\nChanging the theme\nThe default theme for ggplot2 uses the gray background with white grid lines. If you don’t find this suitable, you can use the black and white theme by using the theme_bw() function. The theme_bw() function also allows you to set the typeface for the plot, in case you don’t want the default Helvetica. Here we change the typeface to Times.\n\n\ng + geom_point(aes(color = bmicat)) + \n        theme_bw(base_family = \"Times\")\n\n\n\n\nFigure 9: Modifying the theme for a plot\n\n\n\n\nExample: Let’s take our palmerpenguins scatterplot from above and change out the theme to use theme_dark().\n\n\n# try it yourself\n\nlibrary(palmerpenguins)\npenguins \n\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm\n   <fct>   <fct>              <dbl>         <dbl>             <int>\n 1 Adelie  Torgersen           39.1          18.7               181\n 2 Adelie  Torgersen           39.5          17.4               186\n 3 Adelie  Torgersen           40.3          18                 195\n 4 Adelie  Torgersen           NA            NA                  NA\n 5 Adelie  Torgersen           36.7          19.3               193\n 6 Adelie  Torgersen           39.3          20.6               190\n 7 Adelie  Torgersen           38.9          17.8               181\n 8 Adelie  Torgersen           39.2          19.6               195\n 9 Adelie  Torgersen           34.1          18.1               193\n10 Adelie  Torgersen           42            20.2               190\n# … with 334 more rows, and 3 more variables: body_mass_g <int>,\n#   sex <fct>, year <int>\n\n\nMore complex example\nNow you get the sense that plots in the ggplot2 system are constructed by successively adding components to the plot, starting with the base dataset and maybe a scatterplot. In this section bleow, you can see a slightly more complicated example with an additional variable.\n\nClick here for a slightly more complicated example with ggplot().\nNow, we will ask the question\n\nHow does the relationship between PM2.5 and nocturnal symptoms vary by BMI category and nitrogen dioxide (NO2)?\n\nUnlike our previous BMI variable, NO2 is continuous, and so we need to make NO2 categorical so we can condition on it in the plotting. We can use the cut() function for this purpose. We will divide the NO2 variable into tertiles.\nFirst we need to calculate the tertiles with the quantile() function.\n\n\ncutpoints <- quantile(maacs$logno2_new, seq(0, 1, length = 4), na.rm = TRUE)\n\n\n\nThen we need to divide the original logno2_new variable into the ranges defined by the cut points computed above.\n\n\nmaacs$no2tert <- cut(maacs$logno2_new, cutpoints)\n\n\n\nThe not2tert variable is now a categorical factor variable containing 3 levels, indicating the ranges of NO2 (on the log scale).\n\n\n## See the levels of the newly created factor variable\nlevels(maacs$no2tert)\n\n\n[1] \"(0.342,1.23]\" \"(1.23,1.47]\"  \"(1.47,2.17]\" \n\nThe final plot shows the relationship between PM2.5 and nocturnal symptoms by BMI category and NO2 tertile.\n\n\n## Setup ggplot with data frame\ng <- maacs %>%\n        ggplot(aes(logpm25, NocturnalSympt))\n\n## Add layers\ng + geom_point(alpha = 1/3) + \n        facet_grid(bmicat ~ no2tert) + \n        geom_smooth(method=\"lm\", se=FALSE, col=\"steelblue\") + \n        theme_bw(base_family = \"Avenir\", base_size = 10) + \n        labs(x = expression(\"log \" * PM[2.5])) + \n        labs(y = \"Nocturnal Symptoms\") + \n        labs(title = \"MAACS Cohort\")\n\n\n\n\nFigure 10: PM2.5 and nocturnal symptoms by BMI category and NO2 tertile\n\n\n\nA quick aside about axis limits\nOne quick quirk about ggplot2 that caught me up when I first started using the package can be displayed in the following example. I make a lot of time series plots and I often want to restrict the range of the y-axis while still plotting all the data. In the base graphics system you can do that as follows.\n\n\ntestdat <- data.frame(x = 1:100, y = rnorm(100))\ntestdat[50,2] <- 100  ## Outlier!\nplot(testdat$x, testdat$y, type = \"l\", ylim = c(-3,3))\n\n\n\n\nFigure 11: Time series plot with base graphics\n\n\n\nHere I’ve restricted the y-axis range to be between -3 and 3, even though there is a clear outlier in the data.\nWith ggplot2 the default settings will give you this.\n\n\ng <- ggplot(testdat, aes(x = x, y = y))\ng + geom_line()\n\n\n\n\nFigure 12: Time series plot with default settings\n\n\n\nModifying the ylim() attribute would seem to give you the same thing as the base plot, but it doesn’t.\n\n\ng + geom_line() + ylim(-3, 3)\n\n\n\n\nFigure 13: Time series plot with modified ylim\n\n\n\nEffectively, what this does is subset the data so that only observations between -3 and 3 are included, then plot the data.\nTo plot the data without subsetting it first and still get the restricted range, you have to do the following.\n\n\ng + geom_line() + coord_cartesian(ylim = c(-3, 3))\n\n\n\n\nFigure 14: Time series plot with restricted y-axis range\n\n\n\nAnd now you know!\nResources\nThe ggplot2 book by Hadley Wickham\nThe R Graphics Cookbook by Winston Chang (examples in base plots and in ggplot2)\ntidyverse web site\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nWhat happens if you facet on a continuous variable?\nRead ?facet_wrap. What does nrow do? What does ncol do? What other options control the layout of the individual panels? Why doesn’t facet_grid() have nrow and ncol arguments?\nWhat geom would you use to draw a line chart? A boxplot? A histogram? An area chart?\nWhat does geom_col() do? How is it different to geom_bar()?\n\nAdditional Resources\n\nhttps://r4ds.had.co.nz/data-visualisation.html\nhttps://rdpeng.github.io/Biostat776/lecture-the-ggplot2-plotting-system-part-2.html\n\n\n\n\n",
    "preview": "posts/2021-09-14-ggplot2-plotting-system-part-2/ggplot2-plotting-system-part-2_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-09-13T21:15:36-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-09-07-joining-data-in-r/",
    "title": "Joining data in R",
    "description": "Introduction to relational data and join functions in the dplyr R package.",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-09",
    "categories": [
      "module 1",
      "week 2",
      "R",
      "Programming",
      "dplyr",
      "here",
      "tidyverse"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nRelational data\nKeys\n\nMutating joins\nThe first table\nA second table\nLeft Join\nLeft Join with Incomplete Data\nInner Join\nRight Join\n\nSummary\nPost-lecture materials\nFinal Questions\nAdditional Resources\n\n\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://r4ds.had.co.nz/relational-data.html\nhttps://rafalab.github.io/dsbook/joining-tables.html\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-joining-data-in-r-basics.html\nhttps://r4ds.had.co.nz/relational-data.html\nhttps://rafalab.github.io/dsbook/joining-tables.html\nLearning objectives\n\nAt the end of this lesson you will:\nBe able to define relational data and keys\nBe able to define the three types of join functions for relational data\nBe able to implement mutational join functions\n\nRelational data\nData analyses rarely involve only a single table of data. Typically you have many tables of data, and you must combine them to answer the questions that you are interested in. Collectively, multiple tables of data are called relational data because it is the relations, not just the individual datasets, that are important.\nRelations are always defined between a pair of tables. All other relations are built up from this simple idea: the relations of three or more tables are always a property of the relations between each pair. Sometimes both elements of a pair can be the same table! This is needed if, for example, you have a table of people, and each person has a reference to their parents.\nTo work with relational data you need verbs that work with pairs of tables. There are three families of verbs designed to work with relational data:\nMutating joins: A mutating join allows you to combine variables from two tables. It first matches observations by their keys, then copies across variables from one table to the other on the right side of the table (similar to mutate()). We will discuss a few of these below.\nFiltering joins: filtering joins match observations in the same way as mutating joins, but affect the observations, not the variables (i.e. filter observations from one data frame based on whether or not they match an observation in the other). Two types: semi_join(x, y) and anti_join(x, y).\nSet operations: treat observations as if they were set elements. Typically used less frequently, but occasionally useful when you want to break a single complex filter into simpler pieces. All these operations work with a complete row, comparing the values of every variable. These expect the x and y inputs to have the same variables, and treat the observations like sets: e.g. intersect(x, y), union(x, y), and setdiff(x, y).\nKeys\nThe variables used to connect each pair of tables are called keys. A key is a variable (or set of variables) that uniquely identifies an observation. In simple cases, a single variable is sufficient to identify an observation.\nThere are two types of keys:\nA primary key uniquely identifies an observation in its own table.\nA foreign key uniquely identifies an observation in another table.\nMutating joins\nThe dplyr package provides a set of functions for joining two data frames into a single data frame based on a set of key columns. There are several functions in the *_join() family. These functions all merge together two data frames; they differ in how they handle observations that exist in one but not both data frames. Here, are the four functions from this family that you will likely use the most often:\n\nFunction\nWhat it includes in merged data frame\nleft_join()\nIncludes all observations in the left data frame, whether or not there is a match in the right data frame\nright_join()\nIncludes all observations in the right data frame, whether or not there is a match in the left data frame\ninner_join()\nIncludes only observations that are in both data frames\nfull_join()\nIncludes all observations from both data frames\n\nThe first table\nImagine you are conduct a study and collecting data on subjects and a health outcome. Often, subjects will make multiple visits (a so-called longitudinal study) and so we will record the outcome for each visit. Similarly, we may record other information about them, such as the kind of housing they live in.\nThis code creates a simple table with some made up data about some hypothetical subjects’ outcomes.\n\n\nlibrary(tidyverse)\n\noutcomes <- tibble(\n        id = rep(c(\"a\", \"b\", \"c\"), each = 3),\n        visit = rep(0:2, 3),\n        outcome = rnorm(3 * 3, 3)\n)\n\nprint(outcomes)\n\n\n# A tibble: 9 × 3\n  id    visit outcome\n  <chr> <int>   <dbl>\n1 a         0    3.45\n2 a         1    4.04\n3 a         2    2.68\n4 b         0    2.53\n5 b         1    4.38\n6 b         2    3.36\n7 c         0    3.22\n8 c         1    2.05\n9 c         2    3.63\n\nNote that subjects are labeled by their id in the id column.\nA second table\nHere is some code to create a second table (we will be joining the first and second tables shortly). This table contains some data about the hypothetical subjects’ housing situation by recording the type of house they live in.\n\n\nsubjects <- tibble(\n        id = c(\"a\", \"b\", \"c\"),\n        house = c(\"detached\", \"rowhouse\", \"rowhouse\")\n)\n\nprint(subjects)\n\n\n# A tibble: 3 × 2\n  id    house   \n  <chr> <chr>   \n1 a     detached\n2 b     rowhouse\n3 c     rowhouse\n\nLeft Join\nNow suppose we want to create a table that combines the information about houses with the information about the outcomes. We can use the left_join() function to merge the outcomes and subjects tables and produce the output above.\n\n\nleft_join(x = outcomes, y = subjects, by = \"id\")\n\n\n# A tibble: 9 × 4\n  id    visit outcome house   \n  <chr> <int>   <dbl> <chr>   \n1 a         0    3.45 detached\n2 a         1    4.04 detached\n3 a         2    2.68 detached\n4 b         0    2.53 rowhouse\n5 b         1    4.38 rowhouse\n6 b         2    3.36 rowhouse\n7 c         0    3.22 rowhouse\n8 c         1    2.05 rowhouse\n9 c         2    3.63 rowhouse\n\nThe by argument indicates the column (or columns) that the two tables have in common.\nLeft Join with Incomplete Data\nIn the previous examples, the subjects table didn’t have a visit column. But suppose it did? Maybe people move around during the study. We could image a table like this one.\n\n\nsubjects <- tibble(\n        id = c(\"a\", \"b\", \"c\"),\n        visit = c(0, 1, 0),\n        house = c(\"detached\", \"rowhouse\", \"rowhouse\"),\n)\n\nprint(subjects)\n\n\n# A tibble: 3 × 3\n  id    visit house   \n  <chr> <dbl> <chr>   \n1 a         0 detached\n2 b         1 rowhouse\n3 c         0 rowhouse\n\nWhen we left joint the tables now we get:\n\n\nleft_join(outcomes, subjects, by = c(\"id\", \"visit\"))\n\n\n# A tibble: 9 × 4\n  id    visit outcome house   \n  <chr> <dbl>   <dbl> <chr>   \n1 a         0    3.45 detached\n2 a         1    4.04 <NA>    \n3 a         2    2.68 <NA>    \n4 b         0    2.53 <NA>    \n5 b         1    4.38 rowhouse\n6 b         2    3.36 <NA>    \n7 c         0    3.22 rowhouse\n8 c         1    2.05 <NA>    \n9 c         2    3.63 <NA>    \n\nNotice how now if we do not have information about a subject’s housing in a given visit, the left_join() function automatically inserts an NA value to indicate that it is missing.\nAlso, in the above example, we joined on the id and the visit columns.\nWe may even have a situation where we are missing housing data for a subject completely. The following table has no information about subject a.\n\n\nsubjects <- tibble(\n        id = c(\"b\", \"c\"),\n        visit = c(1, 0),\n        house = c(\"rowhouse\", \"rowhouse\"),\n)\n\nsubjects\n\n\n# A tibble: 2 × 3\n  id    visit house   \n  <chr> <dbl> <chr>   \n1 b         1 rowhouse\n2 c         0 rowhouse\n\nBut we can still join the tables together and the house values for subject a will all be NA.\n\n\nleft_join(x = outcomes, y = subjects, by = c(\"id\", \"visit\"))\n\n\n# A tibble: 9 × 4\n  id    visit outcome house   \n  <chr> <dbl>   <dbl> <chr>   \n1 a         0    3.45 <NA>    \n2 a         1    4.04 <NA>    \n3 a         2    2.68 <NA>    \n4 b         0    2.53 <NA>    \n5 b         1    4.38 rowhouse\n6 b         2    3.36 <NA>    \n7 c         0    3.22 rowhouse\n8 c         1    2.05 <NA>    \n9 c         2    3.63 <NA>    \n\nThe bottom line for left_join() is that it always retains the values in the “left” argument (in this case the outcomes table). If there are no corresponding values in the “right” argument, NA values will be filled in.\nInner Join\nThe inner_join() function only retains the rows of both tables that have corresponding values. Here we can see the difference.\n\n\ninner_join(x = outcomes, y = subjects, by = c(\"id\", \"visit\"))\n\n\n# A tibble: 2 × 4\n  id    visit outcome house   \n  <chr> <dbl>   <dbl> <chr>   \n1 b         1    4.38 rowhouse\n2 c         0    3.22 rowhouse\n\nRight Join\nThe right_join() function is like the left_join() function except that it gives priority to the “right” hand argument.\n\n\nright_join(x = outcomes, y = subjects, by = c(\"id\", \"visit\"))\n\n\n# A tibble: 2 × 4\n  id    visit outcome house   \n  <chr> <dbl>   <dbl> <chr>   \n1 b         1    4.38 rowhouse\n2 c         0    3.22 rowhouse\n\nSummary\nleft_join() is useful for merging a “large” data frame with a “smaller” one while retaining all the rows of the “large” data frame\ninner_join() gives you the intersection of the rows between two data frames\nright_join() is like left_join() with the arguments reversed (likely only useful at the end of a pipeline)\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nIf you had three data frames to combine with a shared key, how would you join them using the verbs you now know?\nUsing df1 and df2 below, what is the difference between inner_join(df1, df2), semi_join(df1, df2) and anti_join(df1, df2)?\n\n\n# Create first example data frame\ndf1 <- data.frame(ID = 1:3,\n                  X1 = c(\"a1\", \"a2\", \"a3\"))\n# Create second example data frame\ndf2 <- data.frame(ID = 2:4, \n                  X2 = c(\"b1\", \"b2\", \"b3\"))\n\n\n\nTry changing the order from the above e.g. inner_join(df2, df1), semi_join(df2, df1) and anti_join(df2, df1). What changed? What did not change?\n\nAdditional Resources\n\nhttps://rdpeng.github.io/Biostat776/lecture-joining-data-in-r-basics.html\nhttps://r4ds.had.co.nz/relational-data.html\nhttps://rafalab.github.io/dsbook/joining-tables.html\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-08T23:26:49-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-07-tidy-data-and-the-tidyverse/",
    "title": "Tidy data and the Tidyverse",
    "description": "Introduction to tidy data and how to convert between wide and long data with the tidyr R package.",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-09",
    "categories": [
      "module 1",
      "week 2",
      "R",
      "Programming",
      "tidyr",
      "here",
      "tidyverse"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nTidy data\nThe “Tidyverse”\npivot_longer() and pivot_wider()\nseparate() and unite()\n\nPost-lecture materials\nFinal Questions\nAdditional Resources\n\n\n\n“Happy families are all alike; every unhappy family is unhappy in its own way.” –– Leo Tolstoy\n\n\n“Tidy datasets are all alike, but every messy dataset is messy in its own way.” –– Hadley Wickham\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nTidy Data paper published in the Journal of Statistical Software\nhttps://r4ds.had.co.nz/tidy-data.html\ntidyr cheat sheet from RStudio\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-tidy-data-and-the-tidyverse.html\nhttps://r4ds.had.co.nz/tidy-data.html\nLearning objectives\n\nAt the end of this lesson you will:\nDefine tidy data\nBe able to transform non-tidy data into tidy data\nBe able to transform wide data into long data\nBe able to separate character columns into multiple columns\nBe able to unite multiple character columns into one column\n\nTidy data\nAs we learned in the last lesson, one unifying concept of the tidyverse is the notion of tidy data. As defined by Hadley Wickham in his 2014 paper published in the Journal of Statistical Software, a tidy dataset has the following properties:\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\n\n\nFigure 1: Artwork by Allison Horst on tidy data\n\n\n\n[Source: Artwork by Allison Horst]\nThe purpose of defining tidy data is to highlight the fact that most data do not start out life as tidy. In fact, much of the work of data analysis may involve simply making the data tidy (at least this has been our experience). Once a dataset is tidy, it can be used as input into a variety of other functions that may transform, model, or visualize the data.\nAs a quick example, consider the following data illustrating religion and income survey data with the number of respondees with income range in column name. This is in a classic table format:\n\n\nlibrary(tidyr)\nrelig_income\n\n\n# A tibble: 18 × 11\n   religion  `<$10k` `$10-20k` `$20-30k` `$30-40k` `$40-50k` `$50-75k`\n   <chr>       <dbl>     <dbl>     <dbl>     <dbl>     <dbl>     <dbl>\n 1 Agnostic       27        34        60        81        76       137\n 2 Atheist        12        27        37        52        35        70\n 3 Buddhist       27        21        30        34        33        58\n 4 Catholic      418       617       732       670       638      1116\n 5 Don’t kn…      15        14        15        11        10        35\n 6 Evangeli…     575       869      1064       982       881      1486\n 7 Hindu           1         9         7         9        11        34\n 8 Historic…     228       244       236       238       197       223\n 9 Jehovah'…      20        27        24        24        21        30\n10 Jewish         19        19        25        25        30        95\n11 Mainline…     289       495       619       655       651      1107\n12 Mormon         29        40        48        51        56       112\n13 Muslim          6         7         9        10         9        23\n14 Orthodox       13        17        23        32        32        47\n15 Other Ch…       9         7        11        13        13        14\n16 Other Fa…      20        33        40        46        49        63\n17 Other Wo…       5         2         3         4         2         7\n18 Unaffili…     217       299       374       365       341       528\n# … with 4 more variables: $75-100k <dbl>, $100-150k <dbl>,\n#   >150k <dbl>, Don't know/refused <dbl>\n\nWhile this format is canonical and is useful for quickly observing the relationship between multiple variables, it is not tidy. This format violates the tidy form because there are variables in the columns. In this case the variables are religion, income bracket, and the number of respondents, which is the third variable, is presented inside the table.\nConverting this data to tidy format would give us\n\n\nlibrary(tidyverse)\n\nrelig_income %>%\n  pivot_longer(-religion, names_to = \"income\", values_to = \"respondents\") %>%\n  mutate(religion = factor(religion), income = factor(income))\n\n\n# A tibble: 180 × 3\n   religion income             respondents\n   <fct>    <fct>                    <dbl>\n 1 Agnostic <$10k                       27\n 2 Agnostic $10-20k                     34\n 3 Agnostic $20-30k                     60\n 4 Agnostic $30-40k                     81\n 5 Agnostic $40-50k                     76\n 6 Agnostic $50-75k                    137\n 7 Agnostic $75-100k                   122\n 8 Agnostic $100-150k                  109\n 9 Agnostic >150k                       84\n10 Agnostic Don't know/refused          96\n# … with 170 more rows\n\nSome of these functions you have seen before, others might be new to you. Let’s talk about each one in the context of the Tidyverse package.\nThe “Tidyverse”\nThere are a number of R packages that take advantage of the tidy data form and can be used to do interesting things with data. Many (but not all) of these packages are written by Hadley Wickham and the collection of packages is sometimes referred to as the “tidyverse” because of their dependence on and presumption of tidy data. “Tidyverse” packages include:\nggplot2: a plotting system based on the grammar of graphics\nmagrittr: defines the %>% operator for chaining functions together in a series of operations on data\ndplyr: a suite of (fast) functions for working with data frames\ntidyr: easily tidy data with pivot_wider() and pivot_longer() functions (also separate() and unite())\nWe will be using these packages quite a bit this week.\nThe “tidyverse” package can be used to install all of the packages in the tidyverse at once. For example, instead of starting an R script with this:\n\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(ggplot2)\n\n\n\nYou can start with this:\n\n\nlibrary(tidyverse)\n\n\n\nIn the code above, let’s talk about what we did using the pivot_longer() function. We will also talk about pivot_wider().\npivot_longer() and pivot_wider()\nThe tidyr package includes functions to transfer a data frame between long and wide.\nWide format data tends to have different attributes or variables describing an observation placed in separate columns.\nLong format data tends to have different attributes encoded as levels of a single variable, followed by another column that contains tha values of the observation at those different levels.\nIn the section above, we showed an example that used pivot_longer() to convert data into a tidy format.\nThe key problem with the tidyness of the data is that the income variables are not in their own columns, but rather are embedded in the structure of the columns.\nTo fix this, you can use the pivot_longer() function to gather values spread across several columns into a single column, with the column names gathered into an income column. When gathering, exclude any columns that you do not want “gathered” (religion in this case) by including the column names with a the minus sign in the pivot_longer() function. For example:\n\n\n# Gather everything EXCEPT religion to tidy data\nrelig_income %>%\n  pivot_longer(-religion, names_to = \"income\", values_to = \"respondents\")\n\n\n# A tibble: 180 × 3\n   religion income             respondents\n   <chr>    <chr>                    <dbl>\n 1 Agnostic <$10k                       27\n 2 Agnostic $10-20k                     34\n 3 Agnostic $20-30k                     60\n 4 Agnostic $30-40k                     81\n 5 Agnostic $40-50k                     76\n 6 Agnostic $50-75k                    137\n 7 Agnostic $75-100k                   122\n 8 Agnostic $100-150k                  109\n 9 Agnostic >150k                       84\n10 Agnostic Don't know/refused          96\n# … with 170 more rows\n\nEven if your data is in a tidy format, pivot_longer() is occasionally useful for pulling data together to take advantage of faceting, or plotting separate plots based on a grouping variable. We will talk more about that in a future lecture.\nThe pivot_wider() function is less commonly needed to tidy data. It can, however, be useful for creating summary tables. For example, you use the summarize() function in dplyr to summarize the total number of respondents per income category.\n\n\nrelig_income %>%\n  pivot_longer(-religion, names_to = \"income\", values_to = \"respondents\") %>%\n  mutate(religion = factor(religion), income = factor(income)) %>% \n  group_by(income) %>% \n  summarize(total_respondents = sum(respondents)) %>%\n  pivot_wider(names_from = \"income\", \n              values_from = \"total_respondents\") %>%\n  knitr::kable()\n\n\n<$10k\n>150k\n$10-20k\n$100-150k\n$20-30k\n$30-40k\n$40-50k\n$50-75k\n$75-100k\nDon’t know/refused\n1930\n2608\n2781\n3197\n3357\n3302\n3085\n5185\n3990\n6121\n\nNotice in this example how pivot_wider() has been used at the very end of the code sequence to convert the summarized data into a shape that offers a better tabular presentation for a report. In the pivot_wider() call, you first specify the name of the column to use for the new column names (income in this example) and then specify the column to use for the cell values (total_respondents here).\n\nExample: Let’s try another dataset. This data contain an excerpt of the Gapminder data on life expectancy, GDP per capita, and population by country.\n\n\nlibrary(gapminder)\ngapminder\n\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# … with 1,694 more rows\n\nIf we wanted to make lifeExp, pop and gdpPercap (all measurements that we observe) go from a wide table into a long table, what would we do?\n\n\n# try it yourself\n\n\n\n\n\nOne more! Try using pivot_longer() to convert the the following data that contains made-up revenues for three companies by quarter for years 2006 to 2009.\nAfterward, use group_by() and summarize() to calculate the average revenue for each company across all years and all quarters.\nBonus: Calculate a mean revenue for each company AND each year (averaged across all 4 quarters).\n\n\ndf <- tibble(\n  \"company\" = rep(1:3, each=4), \n  \"year\"  = rep(2006:2009, 3),\n  \"Q1\"    = sample(x = 0:100, size = 12),\n  \"Q2\"    = sample(x = 0:100, size = 12),\n  \"Q3\"    = sample(x = 0:100, size = 12),\n  \"Q4\"    = sample(x = 0:100, size = 12),\n)\ndf\n\n\n# A tibble: 12 × 6\n   company  year    Q1    Q2    Q3    Q4\n     <int> <int> <int> <int> <int> <int>\n 1       1  2006   100    90    24    86\n 2       1  2007    23    29    30    67\n 3       1  2008    42    39    53    77\n 4       1  2009    98    98    60    87\n 5       2  2006     8    97    17    73\n 6       2  2007    15    83    18     8\n 7       2  2008     7    12    38    72\n 8       2  2009    22    49    99    82\n 9       3  2006    65    28    39    22\n10       3  2007    20    33    14    56\n11       3  2008    56    78    72    44\n12       3  2009    36    67    91    42\n\n\n\n# try it yourself \n\n\n\n\nseparate() and unite()\nThe same tidyr package also contains two useful functions:\nunite(): combine contents of two or more columns into a single column\nseparate(): separate contents of a column into two or more columns\nFirst, we combine the first three columns into one new column using unite().\n\n\ngapminder %>% \n  unite(col=\"country_continent_year\", country:year, sep=\"_\")\n\n\n# A tibble: 1,704 × 4\n   country_continent_year lifeExp      pop gdpPercap\n   <chr>                    <dbl>    <int>     <dbl>\n 1 Afghanistan_Asia_1952     28.8  8425333      779.\n 2 Afghanistan_Asia_1957     30.3  9240934      821.\n 3 Afghanistan_Asia_1962     32.0 10267083      853.\n 4 Afghanistan_Asia_1967     34.0 11537966      836.\n 5 Afghanistan_Asia_1972     36.1 13079460      740.\n 6 Afghanistan_Asia_1977     38.4 14880372      786.\n 7 Afghanistan_Asia_1982     39.9 12881816      978.\n 8 Afghanistan_Asia_1987     40.8 13867957      852.\n 9 Afghanistan_Asia_1992     41.7 16317921      649.\n10 Afghanistan_Asia_1997     41.8 22227415      635.\n# … with 1,694 more rows\n\nNext, we show how to separate the columns into three separate columns using separate() using the col, into and sep arguments.\n\n\ngapminder %>% \n  unite(col=\"country_continent_year\", country:year, sep=\"_\") %>% \n  separate(col=\"country_continent_year\", into=c(\"country\", \"continent\", \"year\"), sep=\"_\")\n\n\n# A tibble: 1,704 × 6\n   country     continent year  lifeExp      pop gdpPercap\n   <chr>       <chr>     <chr>   <dbl>    <int>     <dbl>\n 1 Afghanistan Asia      1952     28.8  8425333      779.\n 2 Afghanistan Asia      1957     30.3  9240934      821.\n 3 Afghanistan Asia      1962     32.0 10267083      853.\n 4 Afghanistan Asia      1967     34.0 11537966      836.\n 5 Afghanistan Asia      1972     36.1 13079460      740.\n 6 Afghanistan Asia      1977     38.4 14880372      786.\n 7 Afghanistan Asia      1982     39.9 12881816      978.\n 8 Afghanistan Asia      1987     40.8 13867957      852.\n 9 Afghanistan Asia      1992     41.7 16317921      649.\n10 Afghanistan Asia      1997     41.8 22227415      635.\n# … with 1,694 more rows\n\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nUsing prose, describe how the variables and observations are organised in a tidy dataset versus an non-tidy dataset.\nWhat do the extra and fill arguments do in separate()? Experiment with the various options for the following two toy datasets.\n\n\ntibble(x = c(\"a,b,c\", \"d,e,f,g\", \"h,i,j\")) %>% \n  separate(x, c(\"one\", \"two\", \"three\"))\n\ntibble(x = c(\"a,b,c\", \"d,e\", \"f,g,i\")) %>% \n  separate(x, c(\"one\", \"two\", \"three\"))\n\n\n\nBoth unite() and separate() have a remove argument. What does it do? Why would you set it to FALSE?\nCompare and contrast separate() and extract(). Why are there three variations of separation (by position, by separator, and with groups), but only one unite()?\n\nAdditional Resources\n\nTidy Data paper published in the Journal of Statistical Software\nhttps://r4ds.had.co.nz/tidy-data.html\ntidyr cheat sheet from RStudio\n\n\n\n\n",
    "preview": "https://github.com/allisonhorst/stats-illustrations/raw/master/rstats-artwork/tidydata_1.jpg",
    "last_modified": "2021-09-08T23:26:14-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-09-plotting-systems/",
    "title": "Plotting Systems",
    "description": "Overview of three plotting systems in R",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-09",
    "categories": [
      "module 1",
      "week 2",
      "R",
      "Programming",
      "ggplot2",
      "data viz"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nPlotting Systems\nThe Base Plotting System\nThe Lattice System\nThe ggplot2 System\nReferences\n\nPost-lecture materials\nFinal Questions\nAdditional Resources\n\n\n\nThe data may not contain the answer. And, if you torture the data long enough, it will tell you anything. —John W. Tukey\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://r4ds.had.co.nz/data-visualisation.html\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-plotting-systems.html\nLearning objectives\n\nAt the end of this lesson you will:\nBe able to identify and describe the three plotting systems in R\n\nPlotting Systems\nThere are three different plotting systems in R and they each have different characteristics and modes of operation. They three systems are the base plotting system, the lattice system, and the ggplot2 system. This course will focus primarily on the ggplot2 plotting system. The other two systems are presented for context.\nThe Base Plotting System\nThe base plotting system is the original plotting system for R. The basic model is sometimes referred to as the “artist’s palette” model. The idea is you start with blank canvas and build up from there.\nIn more R-specific terms, you typically start with plot function (or similar plot creating function) to initiate a plot and then annotate the plot with various annotation functions (text, lines, points, axis)\nThe base plotting system is often the most convenient plotting system to use because it mirrors how we sometimes think of building plots and analyzing data. If we do not have a completely well-formed idea of how we want to look at some data, often we will start by “throwing some data on the page” and then slowly add more information to it as our thought process evolves.\nFor example, we might look at a simple scatterplot and then decide to add a linear regression line or a smoother to it to highlight the trends.\n\n\ndata(airquality)\nwith(airquality, {\n        plot(Temp, Ozone)\n        lines(loess.smooth(Temp, Ozone))\n})\n\n\n\n\nFigure 1: Scatterplot with loess curve\n\n\n\nIn the code above, the plot function creates the initial plot and draws the points (circles) on the canvas. The lines function is used to annotate or add to the plot; in this case it adds a loess smoother to the scatterplot.\nHere, we use the plot() function to draw the points on the scatterplot and then use the title function to add a main title to the plot.\nOne downside with constructing base plots is that you can not go backwards once the plot has started. So it is possible that you could start down the road of constructing a plot and realize later (when it is too late) that you do not have enough room to add a y-axis label or something like that.\nIf you have specific plot in mind, there is then a need to plan in advance to make sure, for example, that you have set your margins to be the right size to fit all of the annotations that you may want to include. While the base plotting system is nice in that it gives you the flexibility to specify these kinds of details to painstaking accuracy, sometimes it would be nice if the system could just figure it out for you.\nAnother downside of the base plotting system is that it is difficult to describe or translate a plot to others because there is no clear graphical language or grammar that can be used to communicate what you have done. The only real way to describe what you have done in a base plot is to just list the series of commands/functions that you have executed, which is not a particularly compact way of communicating things. This is one problem that the ggplot2 package attempts to address.\nAnother typical base plot is constructed with the following code.\n\n\ndata(cars)\n\n## Create the plot / draw canvas\nwith(cars, plot(speed, dist))\n\n## Add annotation\ntitle(\"Speed vs. Stopping distance\")\n\n\n\n\nFigure 2: Base plot with title\n\n\n\nWe will go into more detail on what these functions do in later chapters.\nThe Lattice System\nThe lattice plotting system is implemented in the lattice package which comes with every installation of R (although it is not loaded by default). To use the lattice plotting functions you must first load the lattice package with the library function.\n\n\nlibrary(lattice)\n\n\n\nWith the lattice system, plots are created with a single function call, such as xyplot() or bwplot(). There is no real distinction between functions that create or initiate plots and functions that annotate plots because it all happens at once.\nLattice plots tend to be most useful for conditioning types of plots, i.e. looking at how y changes with x across levels of z. These types of plots are useful for looking at multi-dimensional data and often allow you to squeeze a lot of information into a single window or page.\nAnother aspect of lattice that makes it different from base plotting is that things like margins and spacing are set automatically. This is possible because entire plot is specified at once via a single function call, so all of the available information needed to figure out the spacing and margins is already there.\nHere is an example of a lattice plot that looks at the relationship between life expectancy and income and how that relationship varies by region in the United States.\n\n\nstate <- data.frame(state.x77, region = state.region)\nxyplot(Life.Exp ~ Income | region, data = state, layout = c(4, 1))\n\n\n\n\nFigure 3: Lattice plot\n\n\n\nYou can see that the entire plot was generated by the call to xyplot and all of the data for the plot were stored in the state data frame. The plot itself contains four panels—one for each region—and within each panel is a scatterplot of life expectancy and income. The notion of panels comes up a lot with lattice plots because you typically have many panels in a lattice plot (each panel typically represents a condition, like “region”).\nOne downside with the lattice system is that it can sometimes be very awkward to specify an entire plot in a single function call (you end up with functions with many many arguments). Also, annotation in panels in plots is not especially intuitive and can be difficult to explain. In particular, the use of custom panel functions and subscripts can be difficult to wield and requires intense preparation. Finally, once a plot is created, you cannot “add” to the plot (but of course you can just make it again with modifications).\nThe ggplot2 System\nThe ggplot2 plotting system attempts to split the difference between base and lattice in a number of ways. Taking cues from lattice, the ggplot2 system automatically deals with spacings, text, titles but also allows you to annotate by “adding” to a plot.\nThe ggplot2 system is implemented in the ggplot2 package (part of the tidyverse package), which is available from CRAN (it does not come with R). You can install it from CRAN via\n\n\ninstall.packages(\"ggplot2\")\n\n\n\nand then load it into R via the library() function.\n\n\nlibrary(ggplot2)\n\n\n\nSuperficially, the ggplot2 functions are similar to lattice, but the system isgenerally easier and more intuitive to use. The defaults used in ggplot2 make many choices for you, but you can still customize plots to your heart’s desire.\nA typical plot with the ggplot package looks as follows.\n\n\nlibrary(tidyverse)\ndata(mpg)\nmpg %>%\n  ggplot(aes(displ, hwy)) + \n  geom_point()\n\n\n\n\nFigure 4: ggplot2 plot\n\n\n\nThere are additional functions in ggplot2 that allow you to make arbitrarily sophisticated plots.\nReferences\nPaul Murrell (2011). R Graphics, CRC Press.\nHadley Wickham (2009). ggplot2, Springer.\nDeepayan Sarkar (2008). Lattice: Multivariate Data Visualization with R, Springer.\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\n\n\n\n\nAdditional Resources\n\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-09-09-plotting-systems/plotting-systems_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-09-08T22:38:56-04:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 960
  },
  {
    "path": "posts/2021-09-07-managing-data-frames-with-tidyverse/",
    "title": "Managing data frames with the Tidyverse",
    "description": "An introduction to data frames in R and the managing them with the dplyr R package.",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-07",
    "categories": [
      "module 1",
      "week 2",
      "R",
      "Programming",
      "dplyr",
      "here",
      "tibble",
      "tidyverse"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nData Frames\nTibbles\n\nThe dplyr Package\ndplyr grammar\ndplyr functions\ndplyr installation\nselect()\nfilter()\narrange()\nrename()\nmutate()\ngroup_by()\n%>%\nslice_*()\n\nSummary\nPost-lecture materials\nFinal Questions\nAdditional Resources\n\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://r4ds.had.co.nz/tibbles.html\nhttps://jhudatascience.org/tidyversecourse/wrangle-data.html#data-wrangling\ndplyr cheat sheet from RStudio\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-managing-data-frames-with-the-tidyverse.html\nhttps://jhudatascience.org/tidyversecourse/get-data.html#tibbles\nLearning objectives\n\nAt the end of this lesson you will:\nUnderstand the advantages of a tibble and data.frame data objects in R\nLearn about the dplyr R package to manage data frames\nRecognize the key verbs to manage data frames in dplyr\nUse the “pipe” operator to combine verbs together\n\nData Frames\nThe data frame (or data.frame) is a key data structure in statistics and in R. The basic structure of a data frame is that there is one observation per row and each column represents a variable, a measure, feature, or characteristic of that observation. R has an internal implementation of data frames that is likely the one you will use most often. However, there are packages on CRAN that implement data frames via things like relational databases that allow you to operate on very very large data frames (but we will not discuss them here).\nGiven the importance of managing data frames, it is important that we have good tools for dealing with them. For example, operations like filtering rows, re-ordering rows, and selecting columns, can often be tedious operations in R whose syntax is not very intuitive. The dplyr package is designed to mitigate a lot of these problems and to provide a highly optimized set of routines specifically for dealing with data frames.\nTibbles\nAnother type of data structure that we need to discuss is called the tibble! It’s best to think of tibbles as an updated and stylish version of the data.frame. And, tibbles are what tidyverse packages work with most seamlessly. Now, that does not mean tidyverse packages require tibbles. In fact, they still work with data.frames, but the more you work with tidyverse and tidyverse-adjacent packages, the more you will see the advantages of using tibbles.\nBefore we go any further, tibbles are data frames, but they have some new bells and whistles to make your life easier.\nHow tibbles differ from data.frame\nThere are a number of differences between tibbles and data.frames. To see a full vignette about tibbles and how they differ from data.frame, you will want to execute vignette(\"tibble\") and read through that vignette. However, we will summarize some of the most important points here:\nInput type remains unchanged - data.frame is notorious for treating strings as factors; this will not happen with tibbles\nVariable names remain unchanged - In base R, creating data.frames will remove spaces from names, converting them to periods or add “x” before numeric column names. Creating tibbles will not change variable (column) names.\nThere are no row.names() for a tibble - Tidy data requires that variables be stored in a consistent way, removing the need for row names.\nTibbles print first ten rows and columns that fit on one screen - Printing a tibble to screen will never print the entire huge data frame out. By default, it just shows what fits to your screen.\nCreating a tibble\nThe tibble package is part of the tidyverse and can thus be loaded in (once installed) using:\n\n\nlibrary(tidyverse)\n\n\n\nas_tibble()\nSince many packages use the historical data.frame from base R, you will often find yourself in the situation that you have a data.frame and want to convert that data.frame to a tibble. To do so, the as_tibble() function is exactly what you are looking for.\nFor the example, in this lesson we will be using a dataset containing air pollution and temperature data for the city of Chicago in the U.S. The dataset is available in this repository.\nYou can load the data into R using the readRDS() function.\n\n\nlibrary(here)\nchicago <- readRDS(here(\"data\", \"chicago.rds\"))\n\n\n\nYou can see some basic characteristics of the dataset with the dim() and str() functions.\n\n\ndim(chicago)\n\n\n[1] 6940    8\n\nstr(chicago)\n\n\n'data.frame':   6940 obs. of  8 variables:\n $ city      : chr  \"chic\" \"chic\" \"chic\" \"chic\" ...\n $ tmpd      : num  31.5 33 33 29 32 40 34.5 29 26.5 32.5 ...\n $ dptp      : num  31.5 29.9 27.4 28.6 28.9 ...\n $ date      : Date, format: \"1987-01-01\" ...\n $ pm25tmean2: num  NA NA NA NA NA NA NA NA NA NA ...\n $ pm10tmean2: num  34 NA 34.2 47 NA ...\n $ o3tmean2  : num  4.25 3.3 3.33 4.38 4.75 ...\n $ no2tmean2 : num  20 23.2 23.8 30.4 30.3 ...\n\nWe see this data structure is a data.frame with 6940 observations and 8 variables. To convert this data.frame to a tibble you would use the following:\n\n\nstr(as_tibble(chicago))\n\n\ntibble [6,940 × 8] (S3: tbl_df/tbl/data.frame)\n $ city      : chr [1:6940] \"chic\" \"chic\" \"chic\" \"chic\" ...\n $ tmpd      : num [1:6940] 31.5 33 33 29 32 40 34.5 29 26.5 32.5 ...\n $ dptp      : num [1:6940] 31.5 29.9 27.4 28.6 28.9 ...\n $ date      : Date[1:6940], format: \"1987-01-01\" ...\n $ pm25tmean2: num [1:6940] NA NA NA NA NA NA NA NA NA NA ...\n $ pm10tmean2: num [1:6940] 34 NA 34.2 47 NA ...\n $ o3tmean2  : num [1:6940] 4.25 3.3 3.33 4.38 4.75 ...\n $ no2tmean2 : num [1:6940] 20 23.2 23.8 30.4 30.3 ...\n\nNote in the above example and as mentioned earlier, that tibbles, by default, only print the first ten rows to screen. If you were to print chicago to screen, all 6940 rows would be displayed. When working with large data.frames, this default behavior can be incredibly frustrating. Using tibbles removes this frustration because of the default settings for tibble printing.\nAdditionally, you will note that the type of the variable is printed for each variable in the tibble. This helpful feature is another added bonus of tibbles relative to data.frame.\nIf you do want to see more rows from the tibble, there are a few options! First, the View() function in RStudio is incredibly helpful. The input to this function is the data.frame or tibble you would like to see. Specifically, View(chicago) would provide you, the viewer, with a scrollable view (in a new tab) of the complete dataset.\nA second option is the fact that print() enables you to specify how many rows and columns you would like to display. Here, we again display the chicago data.frame as a tibble but specify that we’d only like to see 5 rows. The width = Inf argument specifies that we would like to see all the possible columns. Here, there are only 8, but for larger datasets, this can be helpful to specify.\n\n\nas_tibble(chicago) %>% \n  print(n = 5, width = Inf)\n\n\n# A tibble: 6,940 × 8\n  city   tmpd  dptp date       pm25tmean2 pm10tmean2 o3tmean2\n  <chr> <dbl> <dbl> <date>          <dbl>      <dbl>    <dbl>\n1 chic   31.5  31.5 1987-01-01         NA       34       4.25\n2 chic   33    29.9 1987-01-02         NA       NA       3.30\n3 chic   33    27.4 1987-01-03         NA       34.2     3.33\n4 chic   29    28.6 1987-01-04         NA       47       4.38\n5 chic   32    28.9 1987-01-05         NA       NA       4.75\n  no2tmean2\n      <dbl>\n1      20.0\n2      23.2\n3      23.8\n4      30.4\n5      30.3\n# … with 6,935 more rows\n\ntibble()\nAlternatively, you can create a tibble on the fly by using tibble() and specifying the information you’d like stored in each column. Note that if you provide a single value, this value will be repeated across all rows of the tibble. This is referred to as “recycling inputs of length 1.”\nIn the example here, we see that the column c will contain the value ‘1’ across all rows.\n\n\ntibble(\n  a = 1:5,\n  b = 6:10,\n  c = 1,\n  z = (a + b)^2 + c\n)\n\n\n# A tibble: 5 × 4\n      a     b     c     z\n  <int> <int> <dbl> <dbl>\n1     1     6     1    50\n2     2     7     1    82\n3     3     8     1   122\n4     4     9     1   170\n5     5    10     1   226\n\nThe tibble() function allows you to quickly generate tibbles and even allows you to reference columns within the tibble you’re creating, as seen in column z of the example above.\nWe also noted previously that tibbles can have column names that are not allowed in data.frame. In this example, we see that to utilize a nontraditional variable name, you surround the column name with backticks. Note that to refer to such columns in other tidyverse packages, you’ll continue to use backticks surrounding the variable name.\n\n\ntibble(\n  `two words` = 1:5,\n  `12` = \"numeric\",\n  `:)` = \"smile\",\n)\n\n\n# A tibble: 5 × 3\n  `two words` `12`    `:)` \n        <int> <chr>   <chr>\n1           1 numeric smile\n2           2 numeric smile\n3           3 numeric smile\n4           4 numeric smile\n5           5 numeric smile\n\nSubsetting\nSubsetting tibbles also differs slightly from how subsetting occurs with data.frame. When it comes to tibbles, [[ can subset by name or position; $ only subsets by name. For example:\n\n\ndf <- tibble(\n  a = 1:5,\n  b = 6:10,\n  c = 1,\n  z = (a + b)^2 + c\n)\n\n# Extract by name using $ or [[]]\ndf$z\n\n\n[1]  50  82 122 170 226\n\ndf[[\"z\"]]\n\n\n[1]  50  82 122 170 226\n\n\n# Extract by position requires [[]]\ndf[[4]]\n\n\n[1]  50  82 122 170 226\n\nHaving now discussed tibbles, which are the type of object most tidyverse and tidyverse-adjacent packages work best with, we now know the goal. In many cases, tibbles are ultimately what we want to work with in R. However, data are stored in many different formats outside of R. We will spend the rest of this lesson discussing wrangling functions that work either a data.frame or tibble.\nThe dplyr Package\nThe dplyr package was developed by RStudio and is an optimized and distilled version of the older plyr package for data manipulation. The dplyr package does not provide any “new” functionality to R per se, in the sense that everything dplyr does could already be done with base R, but it greatly simplifies existing functionality in R.\nOne important contribution of the dplyr package is that it provides a “grammar” (in particular, verbs) for data manipulation and for operating on data frames. With this grammar, you can sensibly communicate what it is that you are doing to a data frame that other people can understand (assuming they also know the grammar). This is useful because it provides an abstraction for data manipulation that previously did not exist. Another useful contribution is that the dplyr functions are very fast, as many key operations are coded in C++.\n\n\n\nFigure 1: Artwork by Allison Horst on the dplyr package\n\n\n\n[Source: Artwork by Allison Horst]\ndplyr grammar\nSome of the key “verbs” provided by the dplyr package are\nselect(): return a subset of the columns of a data frame, using a flexible notation\nfilter(): extract a subset of rows from a data frame based on logical conditions\narrange(): reorder rows of a data frame\nrename(): rename variables in a data frame\nmutate(): add new variables/columns or transform existing variables\nsummarise() / summarize(): generate summary statistics of different variables in the data frame, possibly within strata\n%>%: the “pipe” operator is used to connect multiple verb actions together into a pipeline\nThe dplyr package as a number of its own data types that it takes advantage of. For example, there is a handy print method that prevents you from printing a lot of data to the console. Most of the time, these additional data types are transparent to the user and do not need to be worried about.\ndplyr functions\nAll of the functions that we will discuss in this Chapter will have a few common characteristics. In particular,\nThe first argument is a data frame.\nThe subsequent arguments describe what to do with the data frame specified in the first argument, and you can refer to columns in the data frame directly (without using the $ operator, just use the column names).\nThe return result of a function is a new data frame.\nData frames must be properly formatted and annotated for this to all be useful. In particular, the data must be tidy. In short, there should be one observation per row, and each column should represent a feature or characteristic of that observation.\n\n\n\nFigure 2: Artwork by Allison Horst on tidy data\n\n\n\n[Source: Artwork by Allison Horst]\ndplyr installation\nThe dplyr package can be installed from CRAN or from GitHub using the devtools package and the install_github() function. The GitHub repository will usually contain the latest updates to the package and the development version.\nTo install from CRAN, just run\n\n\ninstall.packages(\"dplyr\")\n\n\n\nThe dplyr package is also installed when you install the tidyverse meta-package.\nAfter installing the package it is important that you load it into your R session with the library() function.\n\n\nlibrary(dplyr)\n\n\n\nYou may get some warnings when the package is loaded because there are functions in the dplyr package that have the same name as functions in other packages. For now you can ignore the warnings.\nselect()\nWe will continue to use the chicago dataset containing air pollution and temperature data.\n\n\nchicago <- as_tibble(chicago)\nstr(chicago)\n\n\ntibble [6,940 × 8] (S3: tbl_df/tbl/data.frame)\n $ city      : chr [1:6940] \"chic\" \"chic\" \"chic\" \"chic\" ...\n $ tmpd      : num [1:6940] 31.5 33 33 29 32 40 34.5 29 26.5 32.5 ...\n $ dptp      : num [1:6940] 31.5 29.9 27.4 28.6 28.9 ...\n $ date      : Date[1:6940], format: \"1987-01-01\" ...\n $ pm25tmean2: num [1:6940] NA NA NA NA NA NA NA NA NA NA ...\n $ pm10tmean2: num [1:6940] 34 NA 34.2 47 NA ...\n $ o3tmean2  : num [1:6940] 4.25 3.3 3.33 4.38 4.75 ...\n $ no2tmean2 : num [1:6940] 20 23.2 23.8 30.4 30.3 ...\n\nThe select() function can be used to select columns of a data frame that you want to focus on. Often you will have a large data frame containing “all” of the data, but any given analysis might only use a subset of variables or observations. The select() function allows you to get the few columns you might need.\nSuppose we wanted to take the first 3 columns only. There are a few ways to do this. We could for example use numerical indices. But we can also use the names directly.\n\n\nnames(chicago)[1:3]\n\n\n[1] \"city\" \"tmpd\" \"dptp\"\n\nsubset <- select(chicago, city:dptp)\nhead(subset)\n\n\n# A tibble: 6 × 3\n  city   tmpd  dptp\n  <chr> <dbl> <dbl>\n1 chic   31.5  31.5\n2 chic   33    29.9\n3 chic   33    27.4\n4 chic   29    28.6\n5 chic   32    28.9\n6 chic   40    35.1\n\nNote that the : normally cannot be used with names or strings, but inside the select() function you can use it to specify a range of variable names.\nYou can also omit variables using the select() function by using the negative sign. With select() you can do\n\n\nselect(chicago, -(city:dptp))\n\n\n\nwhich indicates that we should include every variable except the variables city through dptp. The equivalent code in base R would be\n\n\ni <- match(\"city\", names(chicago))\nj <- match(\"dptp\", names(chicago))\nhead(chicago[, -(i:j)])\n\n\n\nNot super intuitive, right?\nThe select() function also allows a special syntax that allows you to specify variable names based on patterns. So, for example, if you wanted to keep every variable that ends with a “2”, we could do\n\n\nsubset <- select(chicago, ends_with(\"2\"))\nstr(subset)\n\n\ntibble [6,940 × 4] (S3: tbl_df/tbl/data.frame)\n $ pm25tmean2: num [1:6940] NA NA NA NA NA NA NA NA NA NA ...\n $ pm10tmean2: num [1:6940] 34 NA 34.2 47 NA ...\n $ o3tmean2  : num [1:6940] 4.25 3.3 3.33 4.38 4.75 ...\n $ no2tmean2 : num [1:6940] 20 23.2 23.8 30.4 30.3 ...\n\nOr if we wanted to keep every variable that starts with a “d”, we could do\n\n\nsubset <- select(chicago, starts_with(\"d\"))\nstr(subset)\n\n\ntibble [6,940 × 2] (S3: tbl_df/tbl/data.frame)\n $ dptp: num [1:6940] 31.5 29.9 27.4 28.6 28.9 ...\n $ date: Date[1:6940], format: \"1987-01-01\" ...\n\nYou can also use more general regular expressions if necessary. See the help page (?select) for more details.\nfilter()\nThe filter() function is used to extract subsets of rows from a data frame. This function is similar to the existing subset() function in R but is quite a bit faster in my experience.\n\n\n\nFigure 3: Artwork by Allison Horst on the filter function\n\n\n\n[Source: Artwork by Allison Horst]\nSuppose we wanted to extract the rows of the chicago data frame where the levels of PM2.5 are greater than 30 (which is a reasonably high level), we could do\n\n\nchic.f <- filter(chicago, pm25tmean2 > 30)\nstr(chic.f)\n\n\ntibble [194 × 8] (S3: tbl_df/tbl/data.frame)\n $ city      : chr [1:194] \"chic\" \"chic\" \"chic\" \"chic\" ...\n $ tmpd      : num [1:194] 23 28 55 59 57 57 75 61 73 78 ...\n $ dptp      : num [1:194] 21.9 25.8 51.3 53.7 52 56 65.8 59 60.3 67.1 ...\n $ date      : Date[1:194], format: \"1998-01-17\" ...\n $ pm25tmean2: num [1:194] 38.1 34 39.4 35.4 33.3 ...\n $ pm10tmean2: num [1:194] 32.5 38.7 34 28.5 35 ...\n $ o3tmean2  : num [1:194] 3.18 1.75 10.79 14.3 20.66 ...\n $ no2tmean2 : num [1:194] 25.3 29.4 25.3 31.4 26.8 ...\n\nYou can see that there are now only 194 rows in the data frame and the distribution of the pm25tmean2 values is.\n\n\nsummary(chic.f$pm25tmean2)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  30.05   32.12   35.04   36.63   39.53   61.50 \n\nWe can place an arbitrarily complex logical sequence inside of filter(), so we could for example extract the rows where PM2.5 is greater than 30 and temperature is greater than 80 degrees Fahrenheit.\n\n\nchic.f <- filter(chicago, pm25tmean2 > 30 & tmpd > 80)\nselect(chic.f, date, tmpd, pm25tmean2)\n\n\n# A tibble: 17 × 3\n   date        tmpd pm25tmean2\n   <date>     <dbl>      <dbl>\n 1 1998-08-23    81       39.6\n 2 1998-09-06    81       31.5\n 3 2001-07-20    82       32.3\n 4 2001-08-01    84       43.7\n 5 2001-08-08    85       38.8\n 6 2001-08-09    84       38.2\n 7 2002-06-20    82       33  \n 8 2002-06-23    82       42.5\n 9 2002-07-08    81       33.1\n10 2002-07-18    82       38.8\n11 2003-06-25    82       33.9\n12 2003-07-04    84       32.9\n13 2005-06-24    86       31.9\n14 2005-06-27    82       51.5\n15 2005-06-28    85       31.2\n16 2005-07-17    84       32.7\n17 2005-08-03    84       37.9\n\nNow there are only 17 observations where both of those conditions are met.\nOther logical operators you should be aware of include:\nOperator\nMeaning\nExample\n==\nEquals\ncity == chic\n!=\nDoes not equal\ncity != chic\n>\nGreater than\ntmpd > 32.0\n>=\nGreater than or equal to\ntmpd >- 32.0\n<\nLess than\ntmpd < 32.0\n<=\nLess than or equal to\ntmpd <= 32.0\n%in%\nIncluded in\ncity %in% c(\"chic\", \"bmore\")\nis.na()\nIs a missing value\nis.na(pm10tmean2)\nIf you are ever unsure of how to write a logical statement, but know how to write its opposite, you can use the ! operator to negate the whole statement. A common use of this is to identify observations with non-missing data (e.g., !(is.na(pm10tmean2))).\narrange()\nThe arrange() function is used to reorder rows of a data frame according to one of the variables/columns. Reordering rows of a data frame (while preserving corresponding order of other columns) is normally a pain to do in R. The arrange() function simplifies the process quite a bit.\nHere we can order the rows of the data frame by date, so that the first row is the earliest (oldest) observation and the last row is the latest (most recent) observation.\n\n\nchicago <- arrange(chicago, date)\n\n\n\nWe can now check the first few rows\n\n\nhead(select(chicago, date, pm25tmean2), 3)\n\n\n# A tibble: 3 × 2\n  date       pm25tmean2\n  <date>          <dbl>\n1 1987-01-01         NA\n2 1987-01-02         NA\n3 1987-01-03         NA\n\nand the last few rows.\n\n\ntail(select(chicago, date, pm25tmean2), 3)\n\n\n# A tibble: 3 × 2\n  date       pm25tmean2\n  <date>          <dbl>\n1 2005-12-29       7.45\n2 2005-12-30      15.1 \n3 2005-12-31      15   \n\nColumns can be arranged in descending order too by useing the special desc() operator.\n\n\nchicago <- arrange(chicago, desc(date))\n\n\n\nLooking at the first three and last three rows shows the dates in descending order.\n\n\nhead(select(chicago, date, pm25tmean2), 3)\n\n\n# A tibble: 3 × 2\n  date       pm25tmean2\n  <date>          <dbl>\n1 2005-12-31      15   \n2 2005-12-30      15.1 \n3 2005-12-29       7.45\n\ntail(select(chicago, date, pm25tmean2), 3)\n\n\n# A tibble: 3 × 2\n  date       pm25tmean2\n  <date>          <dbl>\n1 1987-01-03         NA\n2 1987-01-02         NA\n3 1987-01-01         NA\n\nrename()\nRenaming a variable in a data frame in R is surprisingly hard to do! The rename() function is designed to make this process easier.\nHere you can see the names of the first five variables in the chicago data frame.\n\n\nhead(chicago[, 1:5], 3)\n\n\n# A tibble: 3 × 5\n  city   tmpd  dptp date       pm25tmean2\n  <chr> <dbl> <dbl> <date>          <dbl>\n1 chic     35  30.1 2005-12-31      15   \n2 chic     36  31   2005-12-30      15.1 \n3 chic     35  29.4 2005-12-29       7.45\n\nThe dptp column is supposed to represent the dew point temperature adn the pm25tmean2 column provides the PM2.5 data. However, these names are pretty obscure or awkward and probably be renamed to something more sensible.\n\n\nchicago <- rename(chicago, dewpoint = dptp, pm25 = pm25tmean2)\nhead(chicago[, 1:5], 3)\n\n\n# A tibble: 3 × 5\n  city   tmpd dewpoint date        pm25\n  <chr> <dbl>    <dbl> <date>     <dbl>\n1 chic     35     30.1 2005-12-31 15   \n2 chic     36     31   2005-12-30 15.1 \n3 chic     35     29.4 2005-12-29  7.45\n\nThe syntax inside the rename() function is to have the new name on the left-hand side of the = sign and the old name on the right-hand side.\n\nQuestion: How would you do the equivalent in base R without dplyr?\n\nmutate()\nThe mutate() function exists to compute transformations of variables in a data frame. Often, you want to create new variables that are derived from existing variables and mutate() provides a clean interface for doing that.\n\n\n\nFigure 4: Artwork by Allison Horst on the mutate function\n\n\n\n[Source: Artwork by Allison Horst]\nFor example, with air pollution data, we often want to detrend the data by subtracting the mean from the data. That way we can look at whether a given day’s air pollution level is higher than or less than average (as opposed to looking at its absolute level).\nHere we create a pm25detrend variable that subtracts the mean from the pm25 variable.\n\n\nchicago <- mutate(chicago, pm25detrend = pm25 - mean(pm25, na.rm = TRUE))\nhead(chicago)\n\n\n# A tibble: 6 × 9\n  city   tmpd dewpoint date        pm25 pm10tmean2 o3tmean2 no2tmean2\n  <chr> <dbl>    <dbl> <date>     <dbl>      <dbl>    <dbl>     <dbl>\n1 chic     35     30.1 2005-12-31 15          23.5     2.53      13.2\n2 chic     36     31   2005-12-30 15.1        19.2     3.03      22.8\n3 chic     35     29.4 2005-12-29  7.45       23.5     6.79      20.0\n4 chic     37     34.5 2005-12-28 17.8        27.5     3.26      19.3\n5 chic     40     33.6 2005-12-27 23.6        27       4.47      23.5\n6 chic     35     29.6 2005-12-26  8.4         8.5    14.0       16.8\n# … with 1 more variable: pm25detrend <dbl>\n\nThere is also the related transmute() function, which does the same thing as mutate() but then drops all non-transformed variables.\nHere, we de-trend the PM10 and ozone (O3) variables.\n\n\nhead(transmute(chicago, \n               pm10detrend = pm10tmean2 - mean(pm10tmean2, na.rm = TRUE),\n               o3detrend = o3tmean2 - mean(o3tmean2, na.rm = TRUE)))\n\n\n# A tibble: 6 × 2\n  pm10detrend o3detrend\n        <dbl>     <dbl>\n1      -10.4     -16.9 \n2      -14.7     -16.4 \n3      -10.4     -12.6 \n4       -6.40    -16.2 \n5       -6.90    -15.0 \n6      -25.4      -5.39\n\nNote that there are only two columns in the transmuted data frame.\ngroup_by()\nThe group_by() function is used to generate summary statistics from the data frame within strata defined by a variable. For example, in this air pollution dataset, you might want to know what the average annual level of PM2.5 is. So the stratum is the year, and that is something we can derive from the date variable.\nIn conjunction with the group_by() function we often use the summarize() function (or summarise() for some parts of the world).\nThe general operation here is a combination of splitting a data frame into separate pieces defined by a variable or group of variables (group_by()), and then applying a summary function across those subsets (summarize()).\nFirst, we can create a year variable using as.POSIXlt().\n\n\nchicago <- mutate(chicago, year = as.POSIXlt(date)$year + 1900)\n\n\n\nNow we can create a separate data frame that splits the original data frame by year.\n\n\nyears <- group_by(chicago, year)\n\n\n\nFinally, we compute summary statistics for each year in the data frame with the summarize() function.\n\n\nsummarize(years, pm25 = mean(pm25, na.rm = TRUE), \n          o3 = max(o3tmean2, na.rm = TRUE), \n          no2 = median(no2tmean2, na.rm = TRUE))\n\n\n# A tibble: 19 × 4\n    year  pm25    o3   no2\n   <dbl> <dbl> <dbl> <dbl>\n 1  1987 NaN    63.0  23.5\n 2  1988 NaN    61.7  24.5\n 3  1989 NaN    59.7  26.1\n 4  1990 NaN    52.2  22.6\n 5  1991 NaN    63.1  21.4\n 6  1992 NaN    50.8  24.8\n 7  1993 NaN    44.3  25.8\n 8  1994 NaN    52.2  28.5\n 9  1995 NaN    66.6  27.3\n10  1996 NaN    58.4  26.4\n11  1997 NaN    56.5  25.5\n12  1998  18.3  50.7  24.6\n13  1999  18.5  57.5  24.7\n14  2000  16.9  55.8  23.5\n15  2001  16.9  51.8  25.1\n16  2002  15.3  54.9  22.7\n17  2003  15.2  56.2  24.6\n18  2004  14.6  44.5  23.4\n19  2005  16.2  58.8  22.6\n\nsummarize() returns a data frame with year as the first column, and then the annual averages of pm25, o3, and no2.\nIn a slightly more complicated example, we might want to know what are the average levels of ozone (o3) and nitrogen dioxide (no2) within quintiles of pm25. A slicker way to do this would be through a regression model, but we can actually do this quickly with group_by() and summarize().\nFirst, we can create a categorical variable of pm25 divided into quantiles\n\n\nqq <- quantile(chicago$pm25, seq(0, 1, 0.2), na.rm = TRUE)\nchicago <- mutate(chicago, pm25.quint = cut(pm25, qq))\n\n\n\nNow we can group the data frame by the pm25.quint variable.\n\n\nquint <- group_by(chicago, pm25.quint)\n\n\n\nFinally, we can compute the mean of o3 and no2 within quintiles of pm25.\n\n\nsummarize(quint, o3 = mean(o3tmean2, na.rm = TRUE), \n          no2 = mean(no2tmean2, na.rm = TRUE))\n\n\n# A tibble: 6 × 3\n  pm25.quint     o3   no2\n  <fct>       <dbl> <dbl>\n1 (1.7,8.7]    21.7  18.0\n2 (8.7,12.4]   20.4  22.1\n3 (12.4,16.7]  20.7  24.4\n4 (16.7,22.6]  19.9  27.3\n5 (22.6,61.5]  20.3  29.6\n6 <NA>         18.8  25.8\n\nFrom the table, it seems there is not a strong relationship between pm25 and o3, but there appears to be a positive correlation between pm25 and no2. More sophisticated statistical modeling can help to provide precise answers to these questions, but a simple application of dplyr functions can often get you most of the way there.\n%>%\nThe pipeline operator %>% is very handy for stringing together multiple dplyr functions in a sequence of operations. Notice above that every time we wanted to apply more than one function, the sequence gets buried in a sequence of nested function calls that is difficult to read, i.e.\n\n\nthird(second(first(x)))\n\n\n\nThis nesting is not a natural way to think about a sequence of operations. The %>% operator allows you to string operations in a left-to-right fashion, i.e.\n\n\nfirst(x) %>% second %>% third\n\n\n\nTake the example that we just did in the last section where we computed the mean of o3 and no2 within quintiles of pm25. There we had to\ncreate a new variable pm25.quint\nsplit the data frame by that new variable\ncompute the mean of o3 and no2 in the sub-groups defined by pm25.quint\nThat can be done with the following sequence in a single R expression.\n\n\nmutate(chicago, pm25.quint = cut(pm25, qq)) %>%    \n        group_by(pm25.quint) %>% \n        summarize(o3 = mean(o3tmean2, na.rm = TRUE), \n                  no2 = mean(no2tmean2, na.rm = TRUE))\n\n\n# A tibble: 6 × 3\n  pm25.quint     o3   no2\n  <fct>       <dbl> <dbl>\n1 (1.7,8.7]    21.7  18.0\n2 (8.7,12.4]   20.4  22.1\n3 (12.4,16.7]  20.7  24.4\n4 (16.7,22.6]  19.9  27.3\n5 (22.6,61.5]  20.3  29.6\n6 <NA>         18.8  25.8\n\nThis way we do not have to create a set of temporary variables along the way or create a massive nested sequence of function calls.\nNotice in the above code that I pass the chicago data frame to the first call to mutate(), but then afterwards I do not have to pass the first argument to group_by() or summarize(). Once you travel down the pipeline with %>%, the first argument is taken to be the output of the previous element in the pipeline.\nAnother example might be computing the average pollutant level by month. This could be useful to see if there are any seasonal trends in the data.\n\n\nmutate(chicago, month = as.POSIXlt(date)$mon + 1) %>% \n        group_by(month) %>% \n        summarize(pm25 = mean(pm25, na.rm = TRUE), \n                  o3 = max(o3tmean2, na.rm = TRUE), \n                  no2 = median(no2tmean2, na.rm = TRUE))\n\n\n# A tibble: 12 × 4\n   month  pm25    o3   no2\n   <dbl> <dbl> <dbl> <dbl>\n 1     1  17.8  28.2  25.4\n 2     2  20.4  37.4  26.8\n 3     3  17.4  39.0  26.8\n 4     4  13.9  47.9  25.0\n 5     5  14.1  52.8  24.2\n 6     6  15.9  66.6  25.0\n 7     7  16.6  59.5  22.4\n 8     8  16.9  54.0  23.0\n 9     9  15.9  57.5  24.5\n10    10  14.2  47.1  24.2\n11    11  15.2  29.5  23.6\n12    12  17.5  27.7  24.5\n\nHere, we can see that o3 tends to be low in the winter months and high in the summer while no2 is higher in the winter and lower in the summer.\nslice_*()\nThe slice_sample() function of the dplyr package will allow you to see a sample of random rows in random order. The number of rows to show is specified by the n argument. This can be useful if you don’t want to print the entire tibble, but you want to get a greater sense of the values. This is a good option for data analysis reports, where printing the entire tibble would not be appropriate if the tibble is quite large.\n\n\nslice_sample(chicago, n = 10)\n\n\n# A tibble: 10 × 11\n   city   tmpd dewpoint date        pm25 pm10tmean2 o3tmean2 no2tmean2\n   <chr> <dbl>    <dbl> <date>     <dbl>      <dbl>    <dbl>     <dbl>\n 1 chic   76       65   1993-07-19  NA         43      28.8       21.8\n 2 chic   24       14.3 2004-01-21  20.2       26      10.2       26.0\n 3 chic   58       33.6 1988-05-13  NA         30      30.0       14.6\n 4 chic   50.5     32   1987-11-15  NA         51      19.2       36.9\n 5 chic   71       48   1994-09-21  NA         82      30.5       48.5\n 6 chic   67       51.3 2004-06-27  12         26.5    26.1       23.2\n 7 chic   79       51.1 1988-06-07  NA        139      54.2       34.7\n 8 chic   35       29.7 2001-02-13  37.3       34       7.17      29.5\n 9 chic   65.5     56.9 1989-09-20  NA         61      27.4       44.0\n10 chic   53       40.2 1992-04-16  NA         26      17.2       28.2\n# … with 3 more variables: pm25detrend <dbl>, year <dbl>,\n#   pm25.quint <fct>\n\nYou can also use slice_head() or slice_tail() to take a look at the top rows or bottom rows of your tibble. Again the number of rows can be specified with the n argument.\nThis will show the first 5 rows.\n\n\nslice_head(chicago, n = 5)\n\n\n# A tibble: 5 × 11\n  city   tmpd dewpoint date        pm25 pm10tmean2 o3tmean2 no2tmean2\n  <chr> <dbl>    <dbl> <date>     <dbl>      <dbl>    <dbl>     <dbl>\n1 chic     35     30.1 2005-12-31 15          23.5     2.53      13.2\n2 chic     36     31   2005-12-30 15.1        19.2     3.03      22.8\n3 chic     35     29.4 2005-12-29  7.45       23.5     6.79      20.0\n4 chic     37     34.5 2005-12-28 17.8        27.5     3.26      19.3\n5 chic     40     33.6 2005-12-27 23.6        27       4.47      23.5\n# … with 3 more variables: pm25detrend <dbl>, year <dbl>,\n#   pm25.quint <fct>\n\nThis will show the last 5 rows.\n\n\nslice_tail(chicago, n = 5)\n\n\n# A tibble: 5 × 11\n  city   tmpd dewpoint date        pm25 pm10tmean2 o3tmean2 no2tmean2\n  <chr> <dbl>    <dbl> <date>     <dbl>      <dbl>    <dbl>     <dbl>\n1 chic   32       28.9 1987-01-05    NA       NA       4.75      30.3\n2 chic   29       28.6 1987-01-04    NA       47       4.38      30.4\n3 chic   33       27.4 1987-01-03    NA       34.2     3.33      23.8\n4 chic   33       29.9 1987-01-02    NA       NA       3.30      23.2\n5 chic   31.5     31.5 1987-01-01    NA       34       4.25      20.0\n# … with 3 more variables: pm25detrend <dbl>, year <dbl>,\n#   pm25.quint <fct>\n\nSummary\nThe dplyr pacfkage provides a concise set of operations for managing data frames. With these functions we can do a number of complex operations in just a few lines of code. In particular, we can often conduct the beginnings of an exploratory analysis with the powerful combination of group_by() and summarize().\nOnce you learn the dplyr grammar there are a few additional benefits\ndplyr can work with other data frame “back ends” such as SQL databases. There is an SQL interface for relational databases via the DBI package\ndplyr can be integrated with the data.table package for large fast tables\nThe dplyr package is handy way to both simplify and speed up your data frame management code. It is rare that you get such a combination at the same time!\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nHow can you tell if an object is a tibble?\nWhat option controls how many additional column names are printed at the footer of a tibble?\nUsing the trees dataset in base R (this dataset stores the girth, height, and volume for Black Cherry Trees) and using the pipe operator: (i) convert the data.frame to a tibble, (ii) filter for rows with a tree height of greater than 70, and (iii) order rows by Volume (smallest to largest).\n\n\nhead(trees)\n\n\n  Girth Height Volume\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\n\n\nAdditional Resources\n\nhttps://r4ds.had.co.nz/tibbles.html\nhttps://jhudatascience.org/tidyversecourse/wrangle-data.html#data-wrangling\ndplyr cheat sheet from RStudio\n\n\n\n\n",
    "preview": "https://github.com/allisonhorst/stats-illustrations/raw/master/rstats-artwork/dplyr_wrangling.png",
    "last_modified": "2021-09-05T22:33:53-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-07-reading-and-writing-data/",
    "title": "Reading and Writing data",
    "description": "How to get data in and out of R using relative paths",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-07",
    "categories": [
      "module 1",
      "week 2",
      "R",
      "Programming",
      "readr",
      "here",
      "tidyverse"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nCat meme\nIntroduction\nRelative versus absolute paths\nThe here package\nFinding and creating files locally\n\nReading data in base R\ntxt or csv\nR code\nR objects\n\nReading data files with read.table()\nReading in larger datasets with read.table()\nCalculating Memory Requirements for R Objects\nUsing the readr package\nPost-lecture materials\nFinal Questions\nAdditional Resources\n\n\n\n\n“When writing code, you’re always collaborating with future-you; and past-you doesn’t respond to emails”. —Hadley Wickham\n\n[Source]\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://rdpeng.github.io/Biostat776/lecture-getting-and-cleaning-data.html\nhttps://jhudatascience.org/tidyversecourse/get-data.html\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-getting-and-cleaning-data.html\nhttps://r4ds.had.co.nz/data-import.html\nLearning objectives\n\nAt the end of this lesson you will:\nKnow difference between relative vs absolute paths\nBe able to read and write text / csv files in R\nBe able to read and write R data objects in R\nBe able to calculate memory requirements for R objects\nUse modern R packages for reading and writing data\n\nCat meme\n\n\n\nFigure 1: Meme: that variable should be cat-egorical\n\n\n\n[Source]\nIntroduction\nThis lesson introduces ways to read and write data (e.g. .txt and .csv files) using base R functions as well as more modern R packages, such as readr, which is typically 10x faster than base R.\nWe will also briefly describe different ways for reading and writing other data types such as, Excel files, google spreadsheets, or SQL databases.\nRelative versus absolute paths\nWhen you are starting a data analysis, you have already learned about the use of .Rproj files. When you open up a .Rproj file, RStudio changes the path (location on your computer) to the .Rproj location.\nAfter opening up a .Rproj file, you can test this by\n\n\ngetwd()\n\n\n\nWhen you open up someone else’s R code or analysis, you might also see the\n\n\nsetwd()\n\n\n\nfunction being used which explicitly tells R to change the absolute path or absolute location of which directory to move into.\nFor example, say I want to clone a GitHub repo from Roger, which has 100 R script files, and in every one of those files at the top is:\n\nsetwd(\"C:\\Users\\Roger\\path\\only\\that\\Roger\\has\")\n\nThe problem is, if I want to use his code, I will need to go and hand-edit every single one of those paths (C:\\Users\\Roger\\path\\only\\that\\Roger\\has) to the path that I want to use on my computer or wherever I saved the folder on my computer (e.g.  /Users/Stephanie/Documents/path/only/I/have).\nThis is an unsustainable practice.\nI can go in and manually edit the path, but this assumes I know how to set a working directory. Not everyone does.\nSo instead of absolute paths:\n\n\nsetwd(\"/Users/jtleek/data\")\nsetwd(\"~/Desktop/files/data\")\nsetwd(\"C:\\\\Users\\\\Michelle\\\\Downloads\")\n\n\n\nA better idea is to use relative paths:\n\n\nsetwd(\"../data\")\nsetwd(\"../files\")\nsetwd(\"..\\tmp\")\n\n\n\nWithin R, an even better idea is to use the here R package will recognize the top-level directory of a Git repo and supports building all paths relative to that. For more on project-oriented workflow suggestions, read this post from Jenny Bryan.\n\n\n\nFigure 2: Artwork by Allison Horst on on the setwd function\n\n\n\n[Source: Artwork by Allison Horst]\nThe here package\nIn her post, Jenny Bryan writes\n\n“I suggest organizing each data analysis into a project: a folder on your computer that holds all the files relevant to that particular piece of work.”\n\nInstead of using setwd() at the top your .R or .Rmd file, she suggests:\nOrganize each logical project into a folder on your computer.\nMake sure the top-level folder advertises itself as such. This can be as simple as having an empty file named .here. Or, if you use RStudio and/or Git, those both leave characteristic files behind that will get the job done.\nUse the here() function from the here package to build the path when you read or write a file. Create paths relative to the top-level directory.\nWhenever you work on this project, launch the R process from the project’s top-level directory. If you launch R from the shell, cd to the correct folder first.\nLet’s test this out. We can use getwd() to see our current working directory path and the files available using list.files()\n\n\ngetwd()\n\n\n[1] \"/Users/shicks/Documents/github/teaching/jhustatcomputing2021/_posts/2021-09-07-reading-and-writing-data\"\n\nlist.files()\n\n\n[1] \"reading-and-writing-data_files\" \"reading-and-writing-data.html\" \n[3] \"reading-and-writing-data.Rmd\"  \n\nOK so our current location is in the reading and writing lectures sub-folder of the jhustatcomputing2021 course repository. Let’s try using the here package.\n\n\nlibrary(here)\n\nlist.files(here::here())\n\n\n [1] \"_custom.html\"               \"_exercises\"                \n [3] \"_post_template.Rmd\"         \"_posts\"                    \n [5] \"_projects\"                  \"_projects_custom.html\"     \n [7] \"_site.yml\"                  \"courses.css\"               \n [9] \"data\"                       \"docs\"                      \n[11] \"downloads\"                  \"images\"                    \n[13] \"index.Rmd\"                  \"jhustatcomputing2021.Rproj\"\n[15] \"lectures.Rmd\"               \"projects.Rmd\"              \n[17] \"README.md\"                  \"resources.Rmd\"             \n[19] \"schedule.Rmd\"               \"syllabus.Rmd\"              \n[21] \"videos\"                    \n\nlist.files(here(\"data\"))\n\n\n[1] \"2016-07-19.csv.bz2\" \"chicago.rds\"        \"team_standings.csv\"\n\nNow we see that using the here::here() function is a relative path (relative to the .Rproj file in our jhustatcomputing2021 repository. We also see there is are two .csv files in the data folder. We will learn how to read those files into R in the next section.\n\n\n\nFigure 3: Artwork by Allison Horst on the here package\n\n\n\n[Source: Artwork by Allison Horst]\nFinding and creating files locally\nOne last thing. If you want to download a file, one way to use the file.exists(), dir.create() and list.files() functions.\nfile.exists(here(\"my\", \"relative\", \"path\")): logical test if the file exists\ndir.create(here(\"my\", \"relative\", \"path\")): create a folder\nlist.files(here(\"my\", \"relative\", \"path\")): list contents of folder\nfile.create(here(\"my\", \"relative\", \"path\")): create a file\nfile.remove(here(\"my\", \"relative\", \"path\")): delete a file\nFor example, I can put all this together by\nChecking to see if a file exists in my path. If not, then\nCreate a directory in that path.\nList the files in the path.\n\n\nif(!file.exists(here(\"my\", \"relative\", \"path\"))){\n  dir.create(here(\"my\", \"relative\", \"path\"))\n}\nlist.files(here(\"my\", \"relative\", \"path\"))\n\n\n\nLet’s put relative paths to use while reading and writing data.\nReading data in base R\nIn this section, we’re going to demonstrate the essential functions you need to know to read and write (or save) data in R.\ntxt or csv\nThere are a few primary functions reading data from base R.\nread.table(), read.csv(): for reading tabular data\nreadLines(): for reading lines of a text file\nThere are analogous functions for writing data to files\nwrite.table(): for writing tabular data to text files (i.e. CSV) or connections\nwriteLines(): for writing character data line-by-line to a file or connection\nLet’s try reading some data into R with the read.csv() function.\n\n\ndf <- read.csv(here(\"data\", \"team_standings.csv\"))\ndf\n\n\n   Standing         Team\n1         1        Spain\n2         2  Netherlands\n3         3      Germany\n4         4      Uruguay\n5         5    Argentina\n6         6       Brazil\n7         7        Ghana\n8         8     Paraguay\n9         9        Japan\n10       10        Chile\n11       11     Portugal\n12       12          USA\n13       13      England\n14       14       Mexico\n15       15  South Korea\n16       16     Slovakia\n17       17  Ivory Coast\n18       18     Slovenia\n19       19  Switzerland\n20       20 South Africa\n21       21    Australia\n22       22  New Zealand\n23       23       Serbia\n24       24      Denmark\n25       25       Greece\n26       26        Italy\n27       27      Nigeria\n28       28      Algeria\n29       29       France\n30       30     Honduras\n31       31     Cameroon\n32       32  North Korea\n\nWe can use the $ symbol to pick out a specific column:\n\n\ndf$Team\n\n\n [1] \"Spain\"        \"Netherlands\"  \"Germany\"      \"Uruguay\"     \n [5] \"Argentina\"    \"Brazil\"       \"Ghana\"        \"Paraguay\"    \n [9] \"Japan\"        \"Chile\"        \"Portugal\"     \"USA\"         \n[13] \"England\"      \"Mexico\"       \"South Korea\"  \"Slovakia\"    \n[17] \"Ivory Coast\"  \"Slovenia\"     \"Switzerland\"  \"South Africa\"\n[21] \"Australia\"    \"New Zealand\"  \"Serbia\"       \"Denmark\"     \n[25] \"Greece\"       \"Italy\"        \"Nigeria\"      \"Algeria\"     \n[29] \"France\"       \"Honduras\"     \"Cameroon\"     \"North Korea\" \n\nWe can also ask for the full paths for specific files\n\n\nhere(\"data\", \"team_standings.csv\")\n\n\n[1] \"/Users/shicks/Documents/github/teaching/jhustatcomputing2021/data/team_standings.csv\"\n\n\nQuestions:\nWhat happens when you use readLines() function with the team_standings.csv data?\nHow would you only read in the first 5 lines?\n\nR code\nSometimes, someone will give you a file that ends in a .R. This is what’s called an R script file. It may contain code someone has written (maybe even you!), for example, a function that you can use with your data. In this case, you want the function available for you to use. To use the function, you have to first, read in the function from R script file into R.\nYou can check to see if the function already is loaded in R by looking at the Environment tab.\nThe function you want to use is\nsource(): for reading in R code files\nFor example, it might be something like this:\n\n\nsource(here::here('functions.R'))\n\n\n\nR objects\nAlternatively, you might be interested in reading and writing R objects.\nWriting data in e.g. .txt, .csv or Excel file formats is good if you want to open these files with other analysis software, such as Excel. However, these formats do not preserve data structures, such as column data types (numeric, character or factor). In order to do that, the data should be written out in a R data format.\nThere are several types R data file formats to be aware of:\n.RData: Stores multiple R objects\n.Rda: This is short for .RData and is equivalent.\n.Rds: Stores a single R object\n\nQuestion: why is saving data in as a R object useful?\nSaving data into R data formats can typically reduce considerably the size of large files by compression.\n\nNext, we will learn how to save\nA single R object\nMultiple R objects\nYour entire work space in a specified file\nReading in data from files\nload(): for reading in single or multiple R objects (opposite of save()) with a .Rda or .RData file format (objects must be same name)\nreadRDS(): for reading in a single object with a .Rds file format (can rename objects)\nunserialize(): for reading single R objects in binary form\nWriting data to files\nsave(): for saving an arbitrary number of R objects in binary format (possibly compressed) to a file.\nsaveRDS(): for saving a single object\nserialize(): for converting an R object into a binary format for outputting to a connection (or file).\nsave.image(): short for ’save my current workspace; while this sounds nice, it’s not terribly useful for reproducibility (hence not suggested); it’s also what happens when you try to quit R and it asks if you want to save your work space.\n\n\n\nFigure 4: Save data into R data file formats: RDS and RDATA\n\n\n\n[Source]\nLet’s try an example. Let’s save a vector of length 5 into the two file formats.\n\n\nx <- 1:5\nsave(x, file=here(\"data\", \"x.Rda\"))\nsaveRDS(x, file=here(\"data\", \"x.Rds\"))\nlist.files(path=here(\"data\"))\n\n\n[1] \"2016-07-19.csv.bz2\" \"chicago.rds\"        \"team_standings.csv\"\n[4] \"x.Rda\"              \"x.Rds\"             \n\nHere we assign the imported data to an object using readRDS()\n\n\nnew_x1 <- readRDS(here(\"data\", \"x.Rds\"))\nnew_x1\n\n\n[1] 1 2 3 4 5\n\nHere we assign the imported data to an object using load()\n\n\nnew_x2 <- load(here(\"data\", \"x.Rda\"))\nnew_x2\n\n\n[1] \"x\"\n\nNOTE: load() simply returns the name of the objects loaded. Not the values.\nLet’s clean up our space.\n\n\nfile.remove(here(\"data\", \"x.Rda\"))\n\n\n[1] TRUE\n\nfile.remove(here(\"data\", \"x.Rds\"))\n\n\n[1] TRUE\n\nrm(x)\n\n\n\n\nWhat do you think this code will do?\nHint: change eval=TRUE to see result\n\n\nx <- 1:5\ny <- x^2\nsave(x,y, file=here(\"data\", \"x.Rda\"))\nnew_x2 <- load(here(\"data\", \"x.Rda\"))\n\n\n\nWhen you are done:\n\n\nfile.remove(here(\"data\", \"x.Rda\"))\n\n\n\n\nNow, there are of course, many R packages that have been developed to read in all kinds of other datasets, and you may need to resort to one of these packages if you are working in a specific area.\nFor example, check out\nDBI for relational databases\nhaven for SPSS, Stata, and SAS data\nhttr for web APIs\nreadxl for .xls and .xlsx sheets\ngooglesheets4 for Google Sheets\ngoogledrive for Google Drive files\nrvest for web scraping\njsonlite for JSON\nxml2 for XML.\nReading data files with read.table()\nThe read.table() function is one of the most commonly used functions for reading data. The help file for read.table() is worth reading in its entirety if only because the function gets used a lot (run ?read.table in R). I know, I know, everyone always says to read the help file, but this one is actually worth reading.\nThe read.table() function has a few important arguments:\nfile, the name of a file, or a connection\nheader, logical indicating if the file has a header line\nsep, a string indicating how the columns are separated\ncolClasses, a character vector indicating the class of each column in the dataset\nnrows, the number of rows in the dataset. By default read.table() reads an entire file.\ncomment.char, a character string indicating the comment character. This defalts to \"#\". If there are no commented lines in your file, it’s worth setting this to be the empty string \"\".\nskip, the number of lines to skip from the beginning\nstringsAsFactors, should character variables be coded as factors? This defaults to FALSE. However, back in the “old days”, it defaulted to TRUE. The reason for this was because, if you had data that were stored as strings, it was because those strings represented levels of a categorical variable. Now, we have lots of data that is text data and they do not always represent categorical variables. So you may want to set this to be FALSE in those cases. If you always want this to be FALSE, you can set a global option via options(stringsAsFactors = FALSE). I’ve never seen so much heat generated on discussion forums about an R function argument than the stringsAsFactors argument. Seriously.\nFor small to moderately sized datasets, you can usually call read.table() without specifying any other arguments\n\n\ndata <- read.table(\"foo.txt\")\n\n\n\nNote: foo.txt is not a real dataset here. It is only used as an example for how to use read.table().\nIn this case, R will automatically:\nskip lines that begin with a #\nfigure out how many rows there are (and how much memory needs to be allocated)\nfigure what type of variable is in each column of the table.\nTelling R all these things directly makes R run faster and more efficiently. The read.csv() function is identical to read.table() except that some of the defaults are set differently (like the sep argument).\nReading in larger datasets with read.table()\nWith much larger datasets, there are a few things that you can do that will make your life easier and will prevent R from choking.\nRead the help page for read.table(), which contains many hints\nMake a rough calculation of the memory required to store your dataset (see the next section for an example of how to do this). If the dataset is larger than the amount of RAM on your computer, you can probably stop right here.\nSet comment.char = \"\" if there are no commented lines in your file.\nUse the colClasses argument. Specifying this option instead of using the default can make read.table() run MUCH faster, often twice as fast. In order to use this option, you have to know the class of each column in your data frame. If all of the columns are “numeric”, for example, then you can just set colClasses = \"numeric\". A quick an dirty way to figure out the classes of each column is the following:\n\n\ninitial <- read.table(\"datatable.txt\", nrows = 100)\nclasses <- sapply(initial, class)\ntabAll <- read.table(\"datatable.txt\", colClasses = classes)\n\n\n\nNote: datatable.txt is not a real dataset here. It is only used as an example for how to use read.table().\nSet nrows. This does not make R run faster but it helps with memory usage. A mild overestimate is okay. You can use the Unix tool wc to calculate the number of lines in a file.\nIn general, when using R with larger datasets, it’s also useful to know a few things about your system.\nHow much memory is available on your system?\nWhat other applications are in use? Can you close any of them?\nAre there other users logged into the same system?\nWhat operating system ar you using? Some operating systems can limit the amount of memory a single process can access\nCalculating Memory Requirements for R Objects\nBecause R stores all of its objects physical memory, it is important to be cognizant of how much memory is being used up by all of the data objects residing in your workspace. One situation where it is particularly important to understand memory requirements is when you are reading in a new dataset into R. Fortunately, it is easy to make a back of the envelope calculation of how much memory will be required by a new dataset.\nFor example, suppose I have a data frame with 1,500,000 rows and 120 columns, all of which are numeric data. Roughly, how much memory is required to store this data frame?\nWell, on most modern computers double precision floating point numbers are stored using 64 bits of memory, or 8 bytes. Given that information, you can do the following calculation\n1,500,000 × 120 × 8 bytes/numeric = 1,440,000,000 bytes\n= 1,440,000,000 / 220 bytes/MB\n= 1,373.29 MB\n= 1.34 GB\nSo the dataset would require about 1.34 GB of RAM. Most computers these days have at least that much RAM. However, you need to be aware of\nwhat other programs might be running on your computer, using up RAM\nwhat other R objects might already be taking up RAM in your workspace\nReading in a large dataset for which you do not have enough RAM is one easy way to freeze up your computer (or at least your R session). This is usually an unpleasant experience that usually requires you to kill the R process, in the best case scenario, or reboot your computer, in the worst case. So make sure to do a rough calculation of memory requirements before reading in a large dataset. You’ll thank me later.\nUsing the readr package\nThe readr package is recently developed by RStudio to deal with reading in large flat files quickly. The package provides replacements for functions like read.table() and read.csv(). The analogous functions in readr are read_table() and read_csv(). These functions are often much faster than their base R analogues and provide a few other nice features such as progress meters.\nFor example, the package includes a variety of functions in the read_* family that allow you to read in data from different formats of flat files. The following table gives a guide to several functions in the read_* family.\n\nreadr function\nUse\nread_csv\nReads comma-separated file\nread_csv2\nReads semicolon-separated file\nread_tsv\nReads tab-separated file\nread_delim\nGeneral function for reading delimited files\nread_fwf\nReads fixed width files\nread_log\nReads log files\n\nNote: In this code, I have used the kable() function from the knitr package to create the summary table in a table format, rather than as basic R output. This function is very useful for formatting basic tables in R markdown documents. For more complex tables, check out the pander and xtable packages.\nFor the most part, you can read use read_table() and read_csv() pretty much anywhere you might use read.table() and read.csv(). In addition, if there are non-fatal problems that occur while reading in the data, you will get a warning and the returned data frame will have some information about which rows/observations triggered the warning. This can be very helpful for “debugging” problems with your data before you get neck deep in data analysis.\nThe importance of the read_csv() function is perhaps better understood from an historical perspective. R’s built in read.csv() function similarly reads CSV files, but the read_csv() function in readr builds on that by removing some of the quirks and “gotchas” of read.csv() as well as dramatically optimizing the speed with which it can read data into R. The read_csv() function also adds some nice user-oriented features like a progress meter and a compact method for specifying column types.\nA typical call to read_csv() will look as follows.\n\n\nlibrary(readr)\nteams <- read_csv(here(\"data\", \"team_standings.csv\"))\nteams\n\n\n# A tibble: 32 × 2\n   Standing Team       \n      <dbl> <chr>      \n 1        1 Spain      \n 2        2 Netherlands\n 3        3 Germany    \n 4        4 Uruguay    \n 5        5 Argentina  \n 6        6 Brazil     \n 7        7 Ghana      \n 8        8 Paraguay   \n 9        9 Japan      \n10       10 Chile      \n# … with 22 more rows\n\nBy default, read_csv() will open a CSV file and read it in line-by-line. Similar to read.table(), you can tell the function to skip lines or which lines are comments:\n\n\nread_csv(\"The first line of metadata\n  The second line of metadata\n  x,y,z\n  1,2,3\",\n  skip = 2)\n\n\n# A tibble: 1 × 3\n      x     y     z\n  <dbl> <dbl> <dbl>\n1     1     2     3\n\n\n\nread_csv(\"# A comment I want to skip\n  x,y,z\n  1,2,3\",\n  comment = \"#\")\n\n\n# A tibble: 1 × 3\n      x     y     z\n  <dbl> <dbl> <dbl>\n1     1     2     3\n\nIt will also (by default), read in the first few rows of the table in order to figure out the type of each column (i.e. integer, character, etc.). From the read_csv() help page:\n\nIf ‘NULL’, all column types will be imputed from the first 1000 rows on the input. This is convenient (and fast), but not robust. If the imputation fails, you’ll need to supply the correct types yourself.\n\nYou can specify the type of each column with the col_types argument.\nIn general, it is a good idea to specify the column types explicitly. This rules out any possible guessing errors on the part of read_csv(). Also, specifying the column types explicitly provides a useful safety check in case anything about the dataset should change without you knowing about it.\n\n\nteams <- read_csv(here(\"data\", \"team_standings.csv\"), \n                  col_types = \"cc\")\n\n\n\nNote that the col_types argument accepts a compact representation. Here \"cc\" indicates that the first column is character and the second column is character (there are only two columns). Using the col_types argument is useful because often it is not easy to automatically figure out the type of a column by looking at a few rows (especially if a column has many missing values).\nThe read_csv() function will also read compressed files automatically. There is no need to decompress the file first or use the gzfile connection function. The following call reads a gzip-compressed CSV file containing download logs from the RStudio CRAN mirror.\n\n\nlogs <- read_csv(here(\"data\", \"2016-07-19.csv.bz2\"), \n                 n_max = 10)\n\n\n\nNote that the warnings indicate that read_csv() may have had some difficulty identifying the type of each column. This can be solved by using the col_types argument.\n\n\nlogs <- read_csv(here(\"data\", \"2016-07-19.csv.bz2\"), \n                 col_types = \"ccicccccci\", \n                 n_max = 10)\nlogs\n\n\n# A tibble: 10 × 10\n   date   time     size r_version r_arch r_os  package version country\n   <chr>  <chr>   <int> <chr>     <chr>  <chr> <chr>   <chr>   <chr>  \n 1 2016-… 22:00… 1.89e6 3.3.0     x86_64 ming… data.t… 1.9.6   US     \n 2 2016-… 22:00… 4.54e4 3.3.1     x86_64 ming… assert… 0.1     US     \n 3 2016-… 22:00… 1.43e7 3.3.1     x86_64 ming… stringi 1.1.1   DE     \n 4 2016-… 22:00… 1.89e6 3.3.1     x86_64 ming… data.t… 1.9.6   US     \n 5 2016-… 22:00… 3.90e5 3.3.1     x86_64 ming… foreach 1.4.3   US     \n 6 2016-… 22:00… 4.88e4 3.3.1     x86_64 linu… tree    1.0-37  CO     \n 7 2016-… 22:00… 5.25e2 3.3.1     x86_64 darw… surviv… 2.39-5  US     \n 8 2016-… 22:00… 3.23e6 3.3.1     x86_64 ming… Rcpp    0.12.5  US     \n 9 2016-… 22:00… 5.56e5 3.3.1     x86_64 ming… tibble  1.1     US     \n10 2016-… 22:00… 1.52e5 3.3.1     x86_64 ming… magrit… 1.5     US     \n# … with 1 more variable: ip_id <int>\n\nYou can specify the column type in a more detailed fashion by using the various col_* functions. For example, in the log data above, the first column is actually a date, so it might make more sense to read it in as a Date object. If we wanted to just read in that first column, we could do\n\n\nlogdates <- read_csv(here(\"data\", \"2016-07-19.csv.bz2\"), \n                     col_types = cols_only(date = col_date()),\n                     n_max = 10)\nlogdates\n\n\n# A tibble: 10 × 1\n   date      \n   <date>    \n 1 2016-07-19\n 2 2016-07-19\n 3 2016-07-19\n 4 2016-07-19\n 5 2016-07-19\n 6 2016-07-19\n 7 2016-07-19\n 8 2016-07-19\n 9 2016-07-19\n10 2016-07-19\n\nNow the date column is stored as a Date object which can be used for relevant date-related computations (for example, see the lubridate package).\nNote: The read_csv() function has a progress option that defaults to TRUE. This options provides a nice progress meter while the CSV file is being read. However, if you are using read_csv() in a function, or perhaps embedding it in a loop, it is probably best to set progress = FALSE.\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nWhat is the point of reference for using relative paths with the here::here() function?\nWhy was the argument stringsAsFactors=TRUE historically used?\nWhat is the difference between .Rds and .Rda file formats?\nWhat function in readr would you use to read a file where fields were separated with “|”?\n\nAdditional Resources\n\nhttps://swcarpentry.github.io/r-novice-inflammation/11-supp-read-write-csv\nhttps://support.rstudio.com/hc/en-us/articles/218611977-Importing-Data-with-the-RStudio-IDE\nhttps://jhudatascience.org/tidyversecourse/get-data.html\n\n\n\n\n",
    "preview": "https://github.com/allisonhorst/stats-illustrations/raw/master/rstats-artwork/cracked_setwd.png",
    "last_modified": "2021-09-06T22:05:09-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-02-literate-programming/",
    "title": "Literate Statistical Programming",
    "description": "Introduction to literate statistical programming tools including R Markdown.",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-02",
    "categories": [
      "module 1",
      "week 1",
      "R Markdown",
      "programming"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nIntroduction\nWeaving and Tangling\nSweave\nrmarkdown\nknitr\n\nCreate and Knit Your First R Markdown Document\nWebsites and Books in R Markdown\nblogdown\nbookdown\ndistill\n\nTips and tricks in R Markdown in RStudio\nRun code\nInsert a comment in R and R Markdown\nKnit a R Markdown document\nCode snippets\nOrdered list in R Markdown\nNew code chunk in R Markdown\nReformat code\nRStudio addins\nOthers\n\nPost-lecture materials\nFinal Questions\nAdditional Resources\n\n\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://rafalab.github.io/dsbook/reproducible-projects-with-rstudio-and-r-markdown.html\nhttps://statsandr.com/blog/tips-and-tricks-in-rstudio-and-r-markdown/\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-literate-statistical-programming.html\nhttps://statsandr.com/blog/tips-and-tricks-in-rstudio-and-r-markdown/\nLearning objectives\n\nAt the end of this lesson you will:\nBe able to define literate programming\nRecognize differences between available tools to for literate programming\nKnow how to efficiently work within RStudio for efficient literate programming\nCreate a R Markdown document\n\nIntroduction\nOne basic idea to make writing reproducible reports easier is what’s known as literate statistical programming (or sometimes called literate statistical practice). This comes from the idea of literate programming in the area of writing computer programs.\nThe idea is to think of a report or a publication as a stream of text and code. The text is readable by people and the code is readable by computers. The analysis is described in a series of text and code chunks. Each kind of code chunk will do something like load some data or compute some results. Each text chunk will relay something in a human readable language. There might also be presentation code that formats tables and figures and there’s article text that explains what’s going on around all this code. This stream of text and code is a literate statistical program or a literate statistical analysis.\nWeaving and Tangling\nLiterate programs by themselves are a bit difficult to work with, but they can be processed in two important ways. Literate programs can be weaved to produce human readable documents like PDFs or HTML web pages, and they can tangled to produce machine-readable “documents,” or in other words, machine readable code. The basic idea behind literate programming in order to generate the different kinds of output you might need, you only need a single source document—you can weave and tangle to get the rest. In order to use a system like this you need a documentational language, that’s human readable, and you need a programming language that’s machine readable (or can be compiled/interpreted into something that’s machine readable).\nSweave\nOne of the original literate programming systems in R that was designed to do this was called Sweave. Sweave enables users to combine R code with a documentation program called LaTeX. Sweave files ends a .Rnw and have R code weaved through the document:\n<<plot1, height=4, width=5, eval=FALSE>>=\ndata(airquality)\nplot(airquality$Ozone ~ airquality$Wind)\n@\nOnce you have created your .Rnw file, Sweave will process the file, executing the R chunks and replacing them with output as appropriate before creating the PDF document.\nIt was originally developed by Fritz Leisch, who is a core member of R, and the code base is still maintained by R Core. The Sweave system comes with any installation of R.\nThere are many limitations to the original Sweave system. One of the limitations is that it is focused primarily on LaTeX, which is not a documentation language that many people are familiar with. Therefore, it can be difficult to learn this type of markup language if you’re not already in a field that uses it regularly. Sweave also lacks a lot of features that people find useful like caching, and multiple plots per page and mixing programming languages.\nInstead, folks have moved towards using something called knitr, which offers everything Sweave does, plus it extends it further. With Sweave, additional tools are required for advanced operations, whereas knitr supports more internally. We’ll discuss knitr below.\nrmarkdown\nAnother choice for literate programming is to build documents based on Markdown language. A markdown file is a plain text file that is typically given the extension .md.. The rmarkdown R package takes a R Markdown file (.Rmd) and weaves together R code chunks like this:\n```{r plot1, height=4, width=5, eval=FALSE, echo=TRUE}\ndata(airquality)\nplot(airquality$Ozone ~ airquality$Wind)\n```\n\nThe best resource for learning about R Markdown this by Yihui Xie, J. J. Allaire, and Garrett Grolemund:\nhttps://bookdown.org/yihui/rmarkdown/\nThe R Markdown Cookbook by Yihui Xie, Christophe Dervieux, and Emily Riederer is really good too:\nhttps://bookdown.org/yihui/rmarkdown-cookbook/\nThe authors of the 2nd book describe the motivation for the 2nd book as:\n\n“However, we have received comments from our readers and publisher that it would be beneficial to provide more practical and relatively short examples to show the interesting and useful usage of R Markdown, because it can be daunting to find out how to achieve a certain task from the aforementioned reference book (put another way, that book is too dry to read). As a result, this cookbook was born.”\n\n\nBecause this is lecture is built in a .Rmd file, let’s demonstrate how this work. I am going to change eval=FALSE to eval=TRUE.\n\n\ndata(airquality)\nplot(airquality$Ozone ~ airquality$Wind)\n\n\n\n\n\nWhy do we not see the back ticks ``` anymore in the code chunk above that made the plot?\nWhat do you think we should do if we want to have the code executed, but we want to hide the code that made it?\n\nBefore we leave this section, I find that there is quite a bit of terminology to understand the magic behind rmarkdown that can be confusing, so let’s break it down:\nPandoc. Pandoc is a command line tool with no GUI that converts documents (e.g. from number of different markup formats to many other formats, such as .doc, .pdf etc). It is completely independent from R (but does come bundled with RStudio).\nMarkdown (markup language). Markdown is a lightweight markup language with plain text formatting syntax designed so that it can be converted to HTML and many other formats. A markdown file is a plain text file that is typically given the extension .md. It is completely independent from R.\nmarkdown (R package). markdown is an R package which converts .md files into HTML. It is no longer recommended for use has been surpassed by rmarkdown (discussed below).\nR Markdown (markup language). R Markdown is an extension of the markdown syntax. R Markdown files are plain text files that typically have the file extension .Rmd.\nrmarkdown (R package). The R package rmarkdown is a library that uses pandoc to process and convert .Rmd files into a number of different formats. This core function is rmarkdown::render(). Note: this package only deals with the markdown language. If the input file is e.g. .Rhtml or .Rnw, then you need to use knitr prior to calling pandoc (see below).\n\nCheck out the R Markdown Quick Tour for more:\nhttps://rmarkdown.rstudio.com/authoring_quick_tour.html\n\nknitr\nOne of the alternative that has come up in recent times is something called knitr. The knitr package for R takes a lot of these ideas of literate programming and updates and improves upon them. knitr still uses R as its programming language, but it allows you to mix other programming languages in. You can also use a variety of documentation languages now, such as LaTeX, markdown and HTML. knitr was developed by Yihui Xie while he was a graduate student at Iowa State and it has become a very popular package for writing literate statistical programs.\nKnitr takes a plain text document with embedded code, executes the code and ‘knits’ the results back into the document.\nFor for example, it converts\nAn R Markdown (.Rmd) file into a standard markdown file (.md)\nAn .Rnw (Sweave) file into to .tex format.\nAn .Rhtml file into to .html.\nThe core function is knitr::knit() and by default this will look at the input document and try and guess what type it is e.g. Rnw, Rmd etc.\nThis core function performs three roles:\nA source parser, which looks at the input document and detects which parts are code that the user wants to be evaluated.\nA code evaluator, which evaluates this code\nAn output renderer, which writes the results of evaluation back to the document in a format which is interpretable by the raw output type. For instance, if the input file is an .Rmd, the output render marks up the output of code evaluation in .md format.\n\n\n\nFigure 1: Converting a Rmd file to many outputs using knitr and pandoc\n\n\n\n[Source]\nAs seen in the figure above, from there pandoc is used to convert e.g. a .md file into many other types of file formats into a .html, etc.\nSo in summary:\n\n“R Markdown stands on the shoulders of knitr and Pandoc. The former executes the computer code embedded in Markdown, and converts R Markdown to Markdown. The latter renders Markdown to the output format you want (such as PDF, HTML, Word, and so on).”\n\n[Source]\nCreate and Knit Your First R Markdown Document\n\n\nWhen creating your first R Markdown document, in RStudio you can\nGo to File > New File > R Markdown…\nFeel free to edit the Title\nMake sure to select “Default Output Format” to be HTML\nClick “OK.” RStudio creates the R Markdown document and places some boilerplate text in there just so you can see how things are setup.\nClick the “Knit” button (or go to File > Knit Document) to make sure you can create the HTML output\nIf you successfully knit your first R Markdown document, then congratulations!\n\n\n\nFigure 2: Mission accomplished!\n\n\n\nWebsites and Books in R Markdown\nNow that you are on the road to using R Markdown documents, it is important to know about other wonderful things you do with these documents. For example, let’s say you have multiple .Rmd documents that you want to put together into a website, blog, book, etc.\nThere are primarily two ways to build multiple .Rmd documents together:\nblogdown for building websites\nbookdown for authoring books\nIn this section, we briefly introduce both packages, but it’s worth mentioning that the rmarkdown package also has a built-in site generator to build websites.\nblogdown\n\n\n\nFigure 3: blogdown logo\n\n\n\n[Source]\nThe blogdown R package is built on top of R Markdown, supports multi-page HTML output to write a blog post or a general page in an Rmd document, or a plain Markdown document. These source documents (e.g. .Rmd or .md) are built into a static website (i.e. a bunch of static HTML files, images and CSS files). Using this folder of files, it is very easy to publish it to any web server as a website. Also, it is easy to maintain because it is only a single folder.\n\nFor example, my personal website was built in blogdown:\nhttps://www.stephaniehicks.com\nOther really great examples can be found here:\nhttps://awesome-blogdown.com\n\nOther advantages include the content likely being reproducible, easier to maintain, and easy to convert pages to e.g. PDF or other formats in the future if you do not want to convert to HTML files. Because it is based on the Markdown syntax, it is easy to write technical documents, including math equations, insert figures or tables with captions, cross-reference with figure or table numbers, add citations, and present theorems or proofs.\nHere’s a video you can watch of someone making a blogdown website.\n\n\n\n\n[Source on YouTube]\nbookdown\n\n\n\nFigure 4: blogdown logo\n\n\n\n[Source]\nSimilar to blogdown, the bookdown R package is built on top of R Markdown, but also offers features like multi-page HTML output, numbering and cross-referencing figures/tables/sections/equations, inserting parts/appendices, and imported the GitBook style (https://www.gitbook.com) to create elegant and appealing HTML book pages. Share\n\nFor example, the previous version of this course was built in bookdown:\nhttps://rdpeng.github.io/Biostat776/\nAnother example is the Tidyverse Skills for Data Science book that the JHU Data Science Lab wrote. The github repo that contains all the .Rmd files can be found here.\nhttps://jhudatascience.org/tidyversecourse/\nhttps://github.com/jhudsl/tidyversecourse\n\nNote: Even though the word “book” is in “bookdown,” this package is not only for books. It really can be anything that consists of multiple .Rmd documents meant to be read in a linear sequence such as course dissertation/thesis, handouts, study notes, a software manual, a thesis, or even a diary.\nhttps://bookdown.org/yihui/rmarkdown/basics-examples.html#examples-books\ndistill\nThere is another great way to build blogs or websites using the distill for R Markdown.\nhttps://rstudio.github.io/distill\nDistill for R Markdown combines the technical authoring features of the Distill web framework (optimized for scientific and technical communication) with R Markdown, enabling a fully reproducible workflow based on literate programming (Knuth 1984).\nDistill articles include:\nReader-friendly typography that adapts well to mobile devices.\nFeatures essential to technical writing like LaTeX math, citations, and footnotes.\nFlexible figure layout options (e.g. displaying figures at a larger width than the article text).\nAttractively rendered tables with optional support for pagination.\nSupport for a wide variety of diagramming tools for illustrating concepts. The ability to incorporate JavaScript and D3-based interactive visualizations.\nA variety of ways to publish articles, including support for publishing sets of articles as a Distill website or as a Distill blog.\nThis course website is built in Distill for R Markdown:\nWebsite: https://stephaniehicks.com/jhustatcomputing2021\nGithub: https://github.com/stephaniehicks/jhustatcomputing2021\nSome other cool things about distill is the use of footnotes and asides.\nFor example.1 The number of the footnote will be automatically generated.\nYou can also optionally include notes in the gutter of the article (immediately to the right of the article text). To do this use the aside tag.\n\nThis content will appear in the gutter of the article.\nYou can also include figures in the gutter. Just enclose the code chunk which generates the figure in an aside tag\nTips and tricks in R Markdown in RStudio\nHere are shortcuts and tips on efficiently using RStudio to improve how you write code.\nRun code\nIf you want to run a code chunk:\ncommand + Enter on Mac\nCtrl + Enter on Windows\nInsert a comment in R and R Markdown\nTo insert a comment:\ncommand + Shift + C on Mac\nCtrl + Shift + C on Windows\nThis shortcut can be used both for:\nR code when you want to comment your code. It will add a # at the beginning of the line\nfor text in R Markdown. It will add <!-- and --> around the text\nNote that if you want to comment more than one line, select all the lines you want to comment then use the shortcut. If you want to uncomment a comment, apply the same shortcut.\nKnit a R Markdown document\nYou can knit R Markdown documents by using this shortcut:\ncommand + Shift + K on Mac\nCtrl + Shift + K on Windows\nCode snippets\nCode snippets is usually a few characters long and is used as a shortcut to insert a common piece of code. You simply type a few characters then press Tab and it will complete your code with a larger code. Tab is then used again to navigate through the code where customization is required. For instance, if you type fun then press Tab, it will auto-complete the code with the required code to create a function:\nname <- function(variables) {\n  \n}\nPressing Tab again will jump through the placeholders for you to edit it. So you can first edit the name of the function, then the variables and finally the code inside the function (try by yourself!).\nThere are many code snippets by default in RStudio. Here are the code snippets I use most often:\nlib to call library()\n\n\nlibrary(package)\n\n\n\nmat to create a matrix\n\n\nmatrix(data, nrow = rows, ncol = cols)\n\n\n\nif, el, and ei to create conditional expressions such as if() {}, else {} and else if () {}\n\nif (condition) {\n  \n}\n\nelse {\n  \n}\n\nelse if (condition) {\n  \n}\n\nfun to create a function\n\n\nname <- function(variables) {\n  \n}\n\n\n\nfor to create for loops\n\n\nfor (variable in vector) {\n  \n}\n\n\n\nts to insert a comment with the current date and time (useful if you have very long code and share it with others so they see when it has been edited)\n\n\n# Tue Jan 21 20:20:14 2020 ------------------------------\n\n\n\nYou can see all default code snippets and add yours by clicking on Tools > Global Options… > Code (left sidebar) > Edit Snippets…\nOrdered list in R Markdown\nIn R Markdown, when creating an ordered list such as this one:\nItem 1\nItem 2\nItem 3\nInstead of bothering with the numbers and typing\n1. Item 1\n2. Item 2\n3. Item 3\nyou can simply type\n1. Item 1\n1. Item 2\n1. Item 3\nfor the exact same result (try it yourself or check the code of this article!). This way you do not need to bother which number is next when creating a new item.\nTo go even further, any numeric will actually render the same result as long as the first item is the number you want to start from. For example, you could type:\n1. Item 1\n7. Item 2\n3. Item 3\nwhich renders\nItem 1\nItem 2\nItem 3\nHowever, I suggest always using the number you want to start from for all items because if you move one item at the top, the list will start with this new number. For instance, if we move 7. Item 2 from the previous list at the top, the list becomes:\n7. Item 2\n1. Item 1\n3. Item 3\nwhich incorrectly renders\nItem 2\nItem 1\nItem 3\nNew code chunk in R Markdown\nWhen editing R Markdown documents, you will need to insert a new R code chunk many times. The following shortcuts will make your life easier:\ncommand + option + I on Mac (or command + alt + I depending on your keyboard)\nCtrl + ALT + I on Windows\nReformat code\nA clear and readable code is always easier and faster to read (and look more professional when sharing it to collaborators). To automatically apply the most common coding guidelines such as white spaces, indents, etc., use:\ncmd + Shift + A on Mac\nCtrl + Shift + A on Windows\nSo for example the following code which does not respect the guidelines (and which is not easy to read):\n1+1\n  for(i in 1:10){if(!i%%2){next}\nprint(i)\n }\nbecomes much more neat and readable:\n1 + 1\nfor (i in 1:10) {\n  if (!i %% 2) {\n    next\n  }\n  print(i)\n}\nRStudio addins\nRStudio addins are extensions which provide a simple mechanism for executing advanced R functions from within RStudio. In simpler words, when executing an addin (by clicking a button in the Addins menu), the corresponding code is executed without you having to write the code. RStudio addins have the advantage that they allow you to execute complex and advanced code much more easily than if you would have to write it yourself.\n\nFor more information about RStudio addins, check out:\nhttps://rstudio.github.io/rstudioaddins/\nhttps://statsandr.com/blog/tips-and-tricks-in-rstudio-and-r-markdown/\n\nOthers\nSimilar to many other programs, you can also use:\ncommand + Shift + N on Mac and Ctrl + Shift + N on Windows to open a new R Script\ncommand + S on Mac and Ctrl + S on Windows to save your current script or R Markdown document\nCheck out Tools –> Keyboard Shortcuts Help to see a long list of these shortcuts.\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nWhat is literate programming?\nWhat was the first literate statistical programming tool to weave together a statistical language (R) with a markup language (LaTeX)?\nWhat is knitr and how is different than other literate statistical programming tools?\nWhere can you find a list of other commands that help make your code writing more efficient in RStudio?\n\nAdditional Resources\n\nRMarkdown Tips and Tricks by Indrajeet Patil\nhttps://bookdown.org/yihui/rmarkdown/\nhttps://bookdown.org/yihui/rmarkdown-cookbook/\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nThis will become a hover-able footnote↩︎\n",
    "preview": "https://d33wubrfki0l68.cloudfront.net/61d189fd9cdf955058415d3e1b28dd60e1bd7c9b/9791d/images/rmarkdownflow.png",
    "last_modified": "2021-09-04T23:05:59-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-02-reference-management/",
    "title": "Reference management",
    "description": "How to use citations and incorporate references from a bibliography in R Markdown.",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-02",
    "categories": [
      "module 1",
      "week 1",
      "R Markdown",
      "programming"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nIntroduction\nCitation management software\nLinking .bib file with R Markdown\nInline citation\nCitation styles\nOther cool features\n\nOther useful tips\nPost-lecture materials\nPractice\n\n\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nAuthoring in R Markdown from RStudio\nCitations from Reproducible Research in R from the Monash Data Fluency initiative\nBibliography from R Markdown Cookbook\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://andreashandel.github.io/MADAcourse/\nhttps://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html\nhttps://bookdown.org/yihui/rmarkdown-cookbook/bibliography.html\nhttps://monashdatafluency.github.io/r-rep-res/citations.html\nLearning objectives\n\nAt the end of this lesson you will:\nKnow what types of bibliography file formats can be used in a R Markdown file\nLearn how to add citations to a R Markdown file\nKnow how to change the citation style (e.g. APA, Chicago, etc)\n\nIntroduction\nFor almost any data analysis, especially if it is meant for publication in the academic literature, you will have to cite other people’s work and include the references (bibliographies or citations) in your work. In this class, you are likely to need to include references and cite other people’s work like in a regular research paper.\nR provides nice function citation() that helps us generating citation blob for R packages that we have used. Let’s try generating citation text for rmarkdown package by using the following command\n\n\ncitation(\"rmarkdown\")\n\n\n\nTo cite the 'rmarkdown' package in publications, please use:\n\n  JJ Allaire and Yihui Xie and Jonathan McPherson and Javier\n  Luraschi and Kevin Ushey and Aron Atkins and Hadley Wickham\n  and Joe Cheng and Winston Chang and Richard Iannone (2021).\n  rmarkdown: Dynamic Documents for R. R package version 2.10.\n  URL https://rmarkdown.rstudio.com.\n\n  Yihui Xie and J.J. Allaire and Garrett Grolemund (2018). R\n  Markdown: The Definitive Guide. Chapman and Hall/CRC. ISBN\n  9781138359338. URL https://bookdown.org/yihui/rmarkdown.\n\n  Yihui Xie and Christophe Dervieux and Emily Riederer (2020).\n  R Markdown Cookbook. Chapman and Hall/CRC. ISBN\n  9780367563837. URL\n  https://bookdown.org/yihui/rmarkdown-cookbook.\n\nTo see these entries in BibTeX format, use 'print(<citation>,\nbibtex=TRUE)', 'toBibtex(.)', or set\n'options(citation.bibtex.max=999)'.\n\nI assume you are familiar with how citing references works, and hopefully, you are already using a reference manager. If not, let me know in the discussion boards.\nTo have something that plays well with R Markdown, you need file format that stores all the references. Click here to learn more other possible file formats available to you to use within a R Markdown file:\nhttps://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html\nCitation management software\nAs you can see, there are ton of file formats including .medline (MEDLINE), .bib (BibTeX), .ris (RIS), .enl (EndNote).\nI will not discuss underlying citational management software itself, but I will talk briefly how you might create one of these file formats.\nIf you recall the output from citation(\"rmarkdown\") above, we might consider manually copying and pasting the output into a citation management software, but instead we can use write_bib() function from knitr package to create a bibliography file ending in .bib.\nLet’s run the following code in order to generate a my-refs.bib file\n\n\nknitr::write_bib(\"rmarkdown\", file = \"my-refs.bib\")\n\n\n\nNow we can see we have the file saved locally.\n\n\nlist.files()\n\n\n[1] \"my-refs.bib\"                \"reference-management_files\"\n[3] \"reference-management.html\"  \"reference-management.Rmd\"  \n\nIf you open up the my-refs.bib file, you will see\n@Manual{R-rmarkdown,\n  title = {rmarkdown: Dynamic Documents for R},\n  author = {JJ Allaire and Yihui Xie and Jonathan McPherson and Javier Luraschi and Kevin Ushey and Aron Atkins and Hadley Wickham and Joe Cheng and Winston Chang and Richard Iannone},\n  year = {2021},\n  note = {R package version 2.8},\n  url = {https://CRAN.R-project.org/package=rmarkdown},\n}\n\n@Book{rmarkdown2018,\n  title = {R Markdown: The Definitive Guide},\n  author = {Yihui Xie and J.J. Allaire and Garrett Grolemund},\n  publisher = {Chapman and Hall/CRC},\n  address = {Boca Raton, Florida},\n  year = {2018},\n  note = {ISBN 9781138359338},\n  url = {https://bookdown.org/yihui/rmarkdown},\n}\n\n@Book{rmarkdown2020,\n  title = {R Markdown Cookbook},\n  author = {Yihui Xie and Christophe Dervieux and Emily Riederer},\n  publisher = {Chapman and Hall/CRC},\n  address = {Boca Raton, Florida},\n  year = {2020},\n  note = {ISBN 9780367563837},\n  url = {https://bookdown.org/yihui/rmarkdown-cookbook},\n}\n\nNote there are three keys that we will use later on:\nR-rmarkdown\nrmarkdown2018\nrmarkdown2020\n\nLinking .bib file with R Markdown\nIn order to use references within a R Markdown file, you will need to specify the name and a location of a bibliography file using the bibliography metadata field in a YAML metadata section. For example:\n---\ntitle: \"My top ten favorite R packages\"\noutput: html_document\nbibliography: my-refs.bib\n---\nYou can include multiple reference files using the following syntax, alternatively you can concatenate two bib files into one.\n---\nbibliography: [\"my-refs1.bib\", \"my-refs2.bib\"]\n---\nInline citation\nNow we can start using those bib keys that we have learned just before, using the following syntax\n[@key] for single citation\n[@key1; @key2] multiple citation can be separated by semi-colon\n[-@key] in order to suppress author name, and just display the year\n[see @key1 p 12; also this ref @key2] is also a valid syntax\nLet’s start by citing the rmarkdown package using the following code and press Knit button:\nI have been using the amazing Rmarkdown package (Allaire et al. 2021)! I should also go and read (Xie, Allaire, and Grolemund 2018; and Xie, Dervieux, and Riederer 2020) books.\nPretty cool, eh??\nTo celebrate, I’ll show you another one of my favorite art pieces from Allison Horst.\n\n\n\nFigure 1: R Markdown magic\n\n\n\n[Source: Artwork by Allison Horst]\nCitation styles\nBy default, Pandoc will use a Chicago author-date format for citations and references.\nTo use another style, you will need to specify a CSL (Citation Style Language) file in the csl metadata field, e.g.,\n---\ntitle: \"My top ten favorite R packages\"\noutput: html_document\nbibliography: my-refs.bib\ncsl: biomed-central.csl\n---\n\nTo find your required formats, we recommend using the Zotero Style Repository, which makes it easy to search for and download your desired style.\n\nCSL files can be tweaked to meet custom formatting requirements. For example, we can change the number of authors required before “et al.” is used to abbreviate them. This can be simplified through the use of visual editors such as the one available at https://editor.citationstyles.org.\nOther cool features\nAdd an item to a bibliography without using it\nBy default, the bibliography will only display items that are directly referenced in the document. If you want to include items in the bibliography without actually citing them in the body text, you can define a dummy nocite metadata field and put the citations there.\n---\nnocite: |\n  @item1, @item2\n---\nAdd all items to the bibliography\nIf we do not wish to explicitly state all of the items within the bibliography but would still like to show them in our references, we can use the following syntax:\n---\nnocite: '@*'\n---\nThis will force all items to be displayed in the bibliography.\n\nYou can also have an appendix appear after bibliography. For more on this, see:\nhttps://bookdown.org/yihui/rmarkdown-cookbook/bibliography.html\n\nOther useful tips\nWe have learned that inside your file that contains all your references (e.g. my-refs.bib), typically each reference gets a key, which is a shorthand that is generated by the reference manager or you can create yourself.\nFor instance, I use a format of lower-case first author last name followed by 4 digit year for each reference followed by a keyword (e.g name of a software package). Alternatively, you can omit the keyword. But note that if I cite a paper by the same first author that was published in the same year, then a lower case letter is added to the end. For instance, for a paper that I wrote as 1st author in 2010, my bibtex key might be hicks2021 or hicks2021a. You can decide what scheme to use, just pick one and use it forever.\nIn your R Markdown document, you can then cite the reference by adding the key, such as ...in the paper by Hicks et al. [@hicks2021]....\nPost-lecture materials\nPractice\nHere are some post-lecture tasks to practice some the material discussed.\n\nTry out the following:\nWhat do you notice that’s different when you run citation(\"tidyverse\") (compared to citation(\"rmarkdown\"))?\nInstall the following packages:\n\ninstall.packages(c(\"bibtex\", \"RefManageR\")\n\nWhat do they do? How might they be helpful to you in terms of reference management?\nInstead of using a .bib file, try using a different bibliography file format in an R Markdown document.\nPractice using a different CSL file to change the citation style.\n\n\n\n\nAllaire, JJ, Yihui Xie, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, Hadley Wickham, Joe Cheng, Winston Chang, and Richard Iannone. 2021. Rmarkdown: Dynamic Documents for r. https://CRAN.R-project.org/package=rmarkdown.\n\n\nXie, Yihui, J. J. Allaire, and Garrett Grolemund. 2018. R Markdown: The Definitive Guide. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown.\n\n\nXie, Yihui, Christophe Dervieux, and Emily Riederer. 2020. R Markdown Cookbook. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown-cookbook.\n\n\n\n\n",
    "preview": "https://github.com/allisonhorst/stats-illustrations/raw/master/rstats-artwork/rmarkdown_wizards.png",
    "last_modified": "2021-09-04T23:04:51-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-02-reproducible-research/",
    "title": "Reproducible Research",
    "description": "Introduction to reproducible research covering some basic concepts and ideas that are related to reproducible reporting",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-02",
    "categories": [
      "module 1",
      "week 1",
      "R",
      "reproducibility"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nIntroduction\nWhat is wrong with replication?\n\nReproducibility to the Rescue\nWhy does this matter?\nTypes of reproducibility\nElements of computational reproducibility\n\n“X” to “Computational X”\nExample: machine learning in the life sciences\n\nThe Data Science Pipeline\nAuthors and Readers\n\nPost-lecture materials\nFinal Questions\n\n\n\nAn article about computational results is advertising, not scholarship. The actual scholarship is the full software environment, code and data, that produced the result. —Claerbout and Karrenbach (1992)\n\n[Link to Claerbout and Karrenbach (1992) article]\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nStatistical programming, Small mistakes, big impacts by Simon Schwab and Leonhard Held\nReproducible Research: A Retrospective by Roger Peng and Stephanie Hicks\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://ropensci.github.io/reproducibility-guide/sections/introduction/\nhttps://rdpeng.github.io/Biostat776/\nReproducible Research: A Retrospective by Roger Peng and Stephanie Hicks\nLearning objectives\n\nAt the end of this lesson you will:\nKnow the difference between replication and reproducibility\nIdentify valid reasons why replication and/or reproducibility is not always possible\nIdentify the type of reproducibility\nIdentify key components to enable reproducible data analyses\n\nIntroduction\nThis lecture will be about reproducible reporting, and I want to take the opportunity to cover some basic concepts and ideas that are related to reproducible reporting, just in case you have not heard about it or don not know what it is.\nBefore we get to reproducibility, we need to cover a little background with respect to how science works (even if you are not a scientist, this is important). The basic idea is that in science, replication is the most important element of verifying and validating findings. So if you claim that X causes Y, or that Vitamin C improves disease, or that something causes a problem, what happens is that other scientists that are independent of you will try to investigate that same question and see if they come up with a similar result. If lots of different people come up with the same result and replicate the original finding, then we tend to think that the original finding was probably true and that this is a real relationship or real finding.\nThe ultimate standard in strengthening scientific evidence is replication. The goal is to have independent people to do independent things with different data, different methods, and different laboratories and see if you get the same result. There is a sense that if a relationship in nature is truly there, then it should be robust to having different people discover it in different ways. Replication is particularly important in areas where findings can have big policy impacts or can influence regulatory types of decisions.\nWhat is wrong with replication?\nThere is really nothing wrong with it. This is what science has been doing for a long time, through hundreds of years. And there is nothing wrong with it today. But the problem is that it is becoming more and more challenging to do replication or to replicate other studies. Here are some reasons:\nOften times studies are much larger and more costly than previously. If you want to do ten versions of the same study, you need ten times as much money and there is not as much money around as there used to be.\nSometimes it is difficult to replicate a study because if the original study took 20 years to do, it is difficult to wait around another 20 years for replication.\nSome studies are just plain unique, such as studying the impact of a massive earthquake in a very specific location and time. If you are looking at a unique situation in time or a unique population, you cannot readily replicate that situation.\nThere are a lot of good reasons why you cannot replicate a study. If you cannot replicate a study, is the alternative just to do nothing, just let that study stand by itself?\nThe idea behind a reproducible reporting is to create a kind of minimum standard (or a middle ground) where we will not be replicating a study, but maybe we can do something in between. The basic problem is that you have the gold standard, which is replication, and then you have the worst standard which is doing nothing. What can we do that’s in between the gold standard and doing nothing? That is where reproducibility comes in. That’s how we can kind of bridge the gap between replication and nothing.\nIn non-research settings, often full replication is not even the point. Often the goal is to preserve something to the point where anybody in an organization can repeat what you did (for example, after you leave the organization). In this case, reproducibility is key to maintaining the history of a project and making sure that every step along the way is clear.\nSummary\nReplication, whereby scientific questions are examined and verified independently by different scientists, is the gold standard for scientific validity.\nReplication can be difficult and often there are no resources to independently replicate a study.\nReproducibility, whereby data and code are re-analyzed by independent scientists to obtain the same results of the original investigator, is a reasonable minimum standard when replication is not possible.\nReproducibility to the Rescue\nWhy do we need this kind of middle ground? I have not clearly defined reproducibility yet, but the basic idea is that you need to make the data available for the original study and the computational methods available so that other people can look at your data and run the kind of analysis that you have run, and come to the same findings that you found.\nWhat reproducible reporting is about is a validation of the data analysis. Because you are not collecting independent data using independent methods, it is a little bit more difficult to validate the scientific question itself. But if you can take someone’s data and reproduce their findings, then you can, in some sense, validate the data analysis.\nThis involves having the data and the code because more likely than not, the analysis will have been done on the computer using some sort of programming language, like R. So you can take their code and their data and reproduce the findings that they come up with. Then you can at least have confidence that you can reproduce the analysis.\nRecently, there has been a lot of discussion of reproducibility in the media and in the scientific literature. The journal Science had a special issue on reproducibility and data replication.\nhttps://www.science.org/toc/science/334/6060\nOther journals have specific policies to promote reproducibility in manuscripts that are published in their journals. For example, the Journal of American Statistical Association (JASA) requires authors to submit their code and data to reproduce their analyses and a set of Associate Editors of Reproducibility review those materials as part of the review process:\nhttps://jasa-acs.github.io/repro-guide\nWhy does this matter?\nHere is an example. In 2012, a feature on the TV show 60 minutes looked at a major incident at Duke University where many results involving a promising cancer test were found to be not reproducible. This led to a number of studies and clinical trials having to be stopped, followed by an investigation which is still ongoing.\n\n\n\n\n[Source on YouTube]\nTypes of reproducibility\nWhat are the different kinds of reproducible research? Enabling reproducibility can be complicated, but by separating out some of the levels and degrees of reproducibility the problem can become more manageable because we can focus our efforts on what best suits our specific scientific domain. Victoria Stodden (2014), a prominent scholar on this topic, has identified some useful distinctions in reproducible research:\nComputational reproducibility: when detailed information is provided about code, software, hardware and implementation details.\nEmpirical reproducibility: when detailed information is provided about non-computational empirical scientific experiments and observations. In practice this is enabled by making data freely available, as well as details of how the data was collected.\nStatistical reproducibility: when detailed information is provided about the choice of statistical tests, model parameters, threshold values, etc. This mostly relates to pre-registration of study design to prevent p-value hacking and other manipulations.\n[Source]\nElements of computational reproducibility\nWhat do we need for computational reproducibility? There are a variety of ways to talk about this, but one basic definition that we hae come up with is that there are four things that are required to make results reproducible:\nAnalytic data. The data that were used for the analysis that was presented should be available for others to access. This is different from the raw data because very often in a data analysis the raw data are not all used for the analysis, but rather some subset is used. It may be interesting to see the raw data but impractical to actually have it. Analytic data is key to examining the data analysis.\nAnalytic code. The analytic code is the code that was applied to the analytic data to produce the key results. This may be preprocessing code, regression modeling code, or really any other code used to produce the results from the analytic data.\nDocumentation. Documentation of that code and the data is very important.\nDistribution. Finally, there needs to be some standard means of distribution, so all this data in the code is easily accessible.\nSummary\nReproducible reporting is about is a validation of the data analysis\nThere are multiple types of reproducibility\nThere are four elements to computational reproducibility\n“X” to “Computational X”\nWhat is driving this need for a “reproducibility middle ground” between replication and doing nothing? For starters, there are a lot of new technologies on the scene and in many different fields of study including, biology, chemistry and environmental science. These technologies allow us to collect data at a much higher throughput so we end up with these very complex and very high dimensional data sets. These datasets can be collected almost instantaneously compared to even just ten years ago—the technology has allowed us to create huge data sets at essentially the touch of a button. Furthermore, we the computing power to take existing (already huge) databases and merge them into even bigger and bigger databases. Finally, the massive increase in computing power has allowed us to implement more sophisticated and complex analysis routines.\nThe analyses themselves, the models that we fit and the algorithms that we run, are much much more complicated than they used to be. Having a basic understanding of these algorithms is difficult, even for a sophisticated person, and it is almost impossible to describe these algorithms with words alone. Understanding what someone did in a data analysis now requires looking at code and scrutinizing the computer programs that people used.\nThe bottom line with all these different trends is that for every field “X”, there is now “Computational X”. There’s computational biology, computational astronomy—whatever it is you want, there is a computational version of it.\nExample: machine learning in the life sciences\nOne example of an area where reproducibility is important comes from research that I have conducted in the area of machine learning in the life sciences.\n\n\n\nFigure 1: Article in Nature Methods on ‘Reproduciblity standards for machine learning in the life sciences’\n\n\n\n[Link to tweet and link to article]\nIn the above article, computational reproducibility is not throught of as a binary property, but rather it is on a sliding scale that reflects the time needed to reproduce. Published works fall somewhere on this scale, which is bookended by ‘forever’, for a completely irreproducible work, and ‘zero’, for a work where one can automatically repeat the entire analysis with a single keystroke.\nAs in many cases it is difficult to impose a single standard that divides work into ‘reproducible’ and ‘irreproducible’. Therefore, instead a menu is proposed of three standards with varying degrees of rigor for computational reproducibility:\nBronze standard. The authors make the data, models and code used in the analysis publicly available. The bronze standard is the minimal standard for reproducibility. Without data, models and code, it is not possible to reproduce a work.\nSilver standard. In addition to meeting the bronze standard: (1) the dependencies of the analysis can be downloaded and installed in a single command; (2) key details for reproducing the work are documented, including the order in which to run the analysis scripts, the operating system used and system resource requirements; and (3) all random components in the analysis are set to be deterministic. The silver standard is a midway point between minimal availability and full automation. Works that meet this standard will take much less time to reproduce than ones only meeting the bronze standard.\nGold standard. The work meets the silver standard, and the authors make the analysis reproducible with a single command. The gold standard for reproducibility is full automation. When a work meets this standard, it will take little to no effort for a scientist to reproduce it.\nThe Data Science Pipeline\nThe basic issue is when you read a description of a data analysis, such as in an article or a technical report, for the most part, what you get is the report and nothing else. Of course, everyone knows that behind the scenes there’s a lot that went into this report and that is what I call the data science pipeline.\nThe Data Science PipelineIn this pipeline, there are two “actors”: the author of the report/article and the reader. On the left side, the author is going from left to right along this pipeline. The reader is going from right to left. If you are the reader, you read the article, and you may want to know more about what happened e.g.\nWhere are the data?\nWhat methods were used here?\nThe basic idea behind computational reproducibility is to focus on the elements in the blue box: the analytic data and the computational results. With computational reproducibility the goal is to allow the author of a report and the reader of that report to “meet in the middle”.\nAuthors and Readers\nIt is important to realize that there are multiple players when you talk about reproducibility–there are different types of parties that have different types of interests. There are authors who produce research and they want to make their research reproducible. There are also readers of research and they want to reproduce that work. Everyone needs tools to make their lives easier.\nOne current challenge is that authors of research have to undergo considerable effort to make their results available to a wide audience. Publishing data and code today is not necessarily a trivial task. Although there are a number of resources available now, that were not available even five years ago, it is still a bit of a challenge to get things out on the web (or at least distributed widely). Resources like GitHub, kipoi, and RPubs and various data repositories have made a big difference, but there is still a ways to go with respect to building up the public reproducibility infrastructure.\nFurthermore, even when data and code are available, readers often have to download the data, download the code, and then they have to piece everything together, usually by hand. It’s not always an easy task to put the data and code together. Also, readers may not have the same computational resources that the original authors did. If the original authors used an enormous computing cluster, for example, to do their analysis, the readers may not have that same enormous computing cluster at their disposal. It may be difficult for readers to reproduce the same results.\nGenerally, the toolbox for doing reproducible research is small, although it’s definitely growing. In practice, authors often just throw things up on the web. There are journals and supplementary materials, but they are famously disorganized. There are only a few central databases that authors can take advantage of to post their data and make it available. So if you are working in a field that has a central database that everyone uses, that is great. If you are not, then you have to assemble your own resources.\nSummary\nThe process of conducting and disseminating research can be depicted as a “data science pipeline”\nReaders and consumers of data science research are typically not privy to the details of the data science pipeline\nOne view of reproducibility is that it gives research consumers partial access to the raw pipeline elements.\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nWhat is the difference between replication and reproducible?\nWhy can replication be difficult to achieve? Why is reproducibility a reasonable minimum standard when replication is not possible?\nWhat is needed to reproduce the results of a data analysis?\n\n\n\n\n",
    "preview": "posts/2021-09-02-reproducible-research/reproducible-research_files/figure-html5/noah-1.png",
    "last_modified": "2021-09-01T15:42:59-04:00",
    "input_file": {},
    "preview_width": 1100,
    "preview_height": 712
  },
  {
    "path": "posts/2021-08-31-introduction-to-gitgithub/",
    "title": "Introduction to git/GitHub",
    "description": "Version control is a game changer; or how I learned to love git/GitHub",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-08-31",
    "categories": [
      "module 1",
      "week 1",
      "programming",
      "version control",
      "git",
      "GitHub"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nIntroduction to git/GitHub\ngit\nGitHub\nWhy use git/GitHub?\nWhat to (not) do\nHow to use Git/GitHub\n\nGetting Started\nUsing git/GitHub in our course\nPost-lecture materials\nFinal Questions\nAdditional Resources\n\n\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nHappy Git with R from Jenny Bryan\nChapter on git and GitHub in dsbook from Rafael Irizarry\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://andreashandel.github.io/MADAcourse/\nLearning objectives\n\nAt the end of this lesson you will:\nKnow what Git and GitHub are.\nKnow why one might want to use them.\nHave created and set up a GitHub account.\n\nIntroduction to git/GitHub\nThis document gives a brief explanation of GitHub and how we will use it for this course.\ngit\nGit is what is called a version control system for file management. The main idea is that as you (and your collaborators) work on a project, the software tracks, and records any changes made by anyone.\nSimilar to the “track changes” features in Microsoft Word, but more rigorous, powerful, and scaled up to multiple files\nGreat for solo or collaborative work\nGitHub\nGitHub is a hosting service on internet for git-aware folders and projects\nSimilar to the DropBox or Google, but more structured, powerful, and programmatic\nGreat for solo or collaborative work!\nTechnically GitHub is distinct from Git. However, GitHub is in some sense the interface and Git the underlying engine (a bit like RStudio and R).\nSince we will only be using Git through GitHub, I tend to not distinguish between the two. In the following, I refer to all of it as just GitHub. Note that other interfaces to Git exist, e.g., Bitbucket, but GitHub is the most widely used one.\nWhy use git/GitHub?\nYou want to use GitHub to avoid this:\n\n\n\nFigure 1: How not to use GitHub [image from PhD Comics]\n\n\n\n[Source: PhD Comics]\nGitHub gives you a clean way to track your projects. It is also very well suited to collaborative work. Historically, version control was used for software development. However, it has become broader and is now used for many types of projects, including data science projects.\nTo learn a bit more about Git/GitHub and why you might want to use it, read this article by Jenny Bryan.\nNote her explanation of what’s special with the README.md file on GitHub.\nWhat to (not) do\nGitHub is ideal if you have a project with a fair number of files, most of those files are text files (such as code, LaTeX, (R)markdown, etc.) and different people work on different parts of the project.\nGitHub is less useful if you have a lot of non-text files (e.g. Word or Powerpoint) and different team members might want to edit the same document at the same time. In that instance, a solution like Google Docs, Word+Dropbox, Word+Onedrive, etc. might be better.\nHow to use Git/GitHub\nGit and GitHub is fundamentally based on commands you type into the command line. Lots of online resources show you how to use the command line. This is the most powerful, and the way I almost always interact with git/GitHub. However, many folks find this the most confusing way to use git/GitHub. Alternatively, there are graphical interfaces.\nGitHub itself provides a grapical interface with basic functionality.\nRStudio also has Git/GitHub integration. Of course this only works for R project GitHub integration.\nThere are also third party GitHub clients with many advanced features, most of which you won’t need initially, but might eventually.\nNote: As student, you can (and should) upgrade to the Pro version of GitHub for free (i.e. access to unlimited private repositories is one benefit), see the GitHub student developer pack on how to do this.\nGetting Started\nOne of my favorite resources for getting started with git/GitHub is the Happy Git with R from Jenny Bryan:\nhttps://happygitwithr.com\n\n\n\nFigure 2: A screenshot of the Happy Git with R online book from Jenny Bryan .\n\n\n\nIt truly is one of the best resources out there for getting started with git/GitHub, especially with the integration to RStudio. Therefore, at this point, I will encourage all of you to go read through the online book.\nSome of you may only need to skim it, others will need to spend some time reading through it. Either way, I will bet that you won’t regret the time investment.\nUsing git/GitHub in our course\nIn this course, you will use git/GitHub in the following ways:\nProject 0 (optional) - You will create a website introducing yourself to folks in the course and deploy it on GitHub.\nProjects 1-3 - You will be asked to practice using git locally (on your compute environment) to track your changes over time and, if you wish (but highly suggested), you can practice pushing your project solutions to a private GitHub repository on your GitHub account (i.e. git add, git commit, git push, git pull, etc) .\nLearning these skills will be useful down the road if you ever work collaboratively on a project (i.e. writing code as a group). In this scenario, you will use the skills you have been practicing in your projects to work together as a team in a single GitHub repository.\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nWhat is version control?\nWhat is the difference between git and GitHub?\nWhat are other version controls software/tools that are available besides git?\n\nAdditional Resources\n\ngit and GitHub in the dsbook by Rafael Irizarry.\n\n\n\n\n",
    "preview": "posts/2021-08-31-introduction-to-gitgithub/../../images/phdversioncontrol.gif",
    "last_modified": "2021-08-30T23:15:53-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-31-introduction-to-r-and-rstudio/",
    "title": "Introduction to R and RStudio",
    "description": "Let's dig into the R programming language and the RStudio integrated developer environment",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-08-31",
    "categories": [
      "module 1",
      "week 1",
      "R",
      "programming",
      "RStudio"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nOverview and history of R\ntl;dr (R in a nutshell)\nBasic Features of R\nFree Software\nDesign of the R System\nLimitations of R\n\nUsing R and RStudio\nInstalling R and RStudio\nRStudio default options\nInstalling and loading R packages\nGetting started in RStudio\n\nPost-lecture materials\nFinal Questions\nAdditional Resources\n\n\n\nThere are only two kinds of languages: the ones people complain about and the ones nobody uses. —Bjarne Stroustrup\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nAn overview and history of R from Roger Peng\nInstalling R and RStudio from Rafael Irizarry\nGetting Started in R and RStudio from Rafael Irizarry\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-introduction-and-overview.html\nhttps://rafalab.github.io/dsbook\nhttps://rmd4sci.njtierney.com\nhttps://andreashandel.github.io/MADAcourse\nLearning objectives\n\nAt the end of this lesson you will:\nLearn about (some of) the history of R.\nIdentify some of the strengths and weaknesses of R.\nInstall R and Rstudio on your computer.\nKnow how to install and load R packages.\n\nOverview and history of R\nBelow is a very quick introduction to R, to get you set up and running. We’ll go deeper into R and coding later.\ntl;dr (R in a nutshell)\nLike every programming language, R has its advantages and disadvantages. If you search the internet, you will quickly discover lots of folks with opinions about R. Some of the features that are useful to know are:\nR is open-source, freely accessible, and cross-platform (multiple OS).\nR is a “high-level” programming language, relatively easy to learn.\nWhile “Low-level” programming languages (e.g. Fortran, C, etc) often have more efficient code, they can also be harder to learn because it is designed to be close to a machine language.\nIn contrast, high-level languages deal more with variables, objects, functions, loops, and other abstract CS concepts with a focus on usability over optimal program efficiency.\n\nR is great for statistics, data analysis, websites, web apps, data visualizations, and so much more!\nR integrates easily with document preparation systems like \\(\\LaTeX\\), but R files can also be used to create .docx, .pdf, .html, .ppt files with integrated R code output and graphics.\nThe R Community is very dynamic, helpful and welcoming.\nCheck out the #rstats on Twitter, TidyTuesday podcast and community activity in the R4DS Online Learning Community, and r/rstats subreddit.\nIf you are looking for more local resources, check out R-Ladies Baltimore.\n\nThrough R packages, it is easy to get lots of state-of-the-art algorithms.\nDocumentation and help files for R are generally good.\nWhile we use R in this course, it is not the only option to analyze data. Maybe the most similar to R, and widely used, is Python, which is also free. There is also commercial software that can be used to analyze data (e.g., Matlab, Mathematica, Tableau, SAS, SPSS). Other more general programming languages are suitable for certain types of analyses as well (e.g., C, Fortran, Perl, Java, Julia).\nDepending on your future needs or jobs, you might have to learn one or several of those additional languages. The good news is that even though those languages are all different, they all share general ways of thinking and structuring code. So once you understand a specific concept (e.g., variables, loops, branching statements or functions), it applies to all those languages. Thus, learning a new programming language is much easier once you already know one. And R is a good one to get started with.\nWith the skills gained in this course, hopefully you will find R a fun and useful programming langauge for your future projects.\n\n\n\nFigure 1: Artwork by Allison Horst on learning R\n\n\n\n[Source: Artwork by Allison Horst]\nBasic Features of R\nToday R runs on almost any standard computing platform and operating system. Its open source nature means that anyone is free to adapt the software to whatever platform they choose. Indeed, R has been reported to be running on modern tablets, phones, PDAs, and game consoles.\nOne nice feature that R shares with many popular open source projects is frequent releases. These days there is a major annual release, typically in October, where major new features are incorporated and released to the public. Throughout the year, smaller-scale bugfix releases will be made as needed. The frequent releases and regular release cycle indicates active development of the software and ensures that bugs will be addressed in a timely manner. Of course, while the core developers control the primary source tree for R, many people around the world make contributions in the form of new feature, bug fixes, or both.\nAnother key advantage that R has over many other statistical packages (even today) is its sophisticated graphics capabilities. R’s ability to create “publication quality” graphics has existed since the very beginning and has generally been better than competing packages. Today, with many more visualization packages available than before, that trend continues. R’s base graphics system allows for very fine control over essentially every aspect of a plot or graph. Other newer graphics systems, like lattice and ggplot2 allow for complex and sophisticated visualizations of high-dimensional data.\nR has maintained the original S philosophy (see box below), which is that it provides a language that is both useful for interactive work, but contains a powerful programming language for developing new tools. This allows the user, who takes existing tools and applies them to data, to slowly but surely become a developer who is creating new tools.\n\nFor a great discussion on an overview and history of R and the S programming language, read through this chapter from Roger D. Peng.\n\nFinally, one of the joys of using R has nothing to do with the language itself, but rather with the active and vibrant user community. In many ways, a language is successful inasmuch as it creates a platform with which many people can create new things. R is that platform and thousands of people around the world have come together to make contributions to R, to develop packages, and help each other use R for all kinds of applications. The R-help and R-devel mailing lists have been highly active for over a decade now and there is considerable activity on web sites like Stack Overflow, Twitter #rstats and Reddit.\nFree Software\nA major advantage that R has over many other statistical packages and is that it’s free in the sense of free software (it’s also free in the sense of free beer). The copyright for the primary source code for R is held by the R Foundation and is published under the GNU General Public License version 2.0.\nAccording to the Free Software Foundation, with free software, you are granted the following four freedoms\nThe freedom to run the program, for any purpose (freedom 0).\nThe freedom to study how the program works, and adapt it to your needs (freedom 1). Access to the source code is a precondition for this.\nThe freedom to redistribute copies so you can help your neighbor (freedom 2).\nThe freedom to improve the program, and release your improvements to the public, so that the whole community benefits (freedom 3). Access to the source code is a precondition for this.\n\nYou can visit the Free Software Foundation’s web site to learn a lot more about free software. The Free Software Foundation was founded by Richard Stallman in 1985 and Stallman’s personal web site is an interesting read if you happen to have some spare time.\n\nDesign of the R System\nThe primary R system is available from the Comprehensive R Archive Network, also known as CRAN. CRAN also hosts many add-on packages that can be used to extend the functionality of R.\nThe R system is divided into 2 conceptual parts:\nThe “base” R system that you download from CRAN:\nLinux\nWindows\nMac\nEverything else.\nR functionality is divided into a number of packages.\nThe “base” R system contains, among other things, the base package which is required to run R and contains the most fundamental functions.\nThe other packages contained in the “base” system include utils, stats, datasets, graphics, grDevices, grid, methods, tools, parallel, compiler, splines, tcltk, stats4.\nThere are also “Recommended” packages: boot, class, cluster, codetools, foreign, KernSmooth, lattice, mgcv, nlme, rpart, survival, MASS, spatial, nnet, Matrix.\nWhen you download a fresh installation of R from CRAN, you get all of the above, which represents a substantial amount of functionality. However, there are many other packages available:\nThere are over 10,000 packages on CRAN that have been developed by users and programmers around the world.\nThere are also many packages associated with the Bioconductor project.\nPeople often make packages available on their personal websites; there is no reliable way to keep track of how many packages are available in this fashion.\n\nQuestions:\nHow many R packages are on CRAN today?\nHow many R packages are on Bioconductor today?\nHow many R packages are on GitHub today?\n\nLimitations of R\nNo programming language or statistical analysis system is perfect. R certainly has a number of drawbacks. For starters, R is essentially based on almost 50 year old technology, going back to the original S system developed at Bell Labs. There was originally little built in support for dynamic or 3-D graphics (but things have improved greatly since the “old days”).\nAnother commonly cited limitation of R is that objects must generally be stored in physical memory (though this is increasingly not true anymore). This is in part due to the scoping rules of the language, but R generally is more of a memory hog than other statistical packages. However, there have been a number of advancements to deal with this, both in the R core and also in a number of packages developed by contributors. Also, computing power and capacity has continued to grow over time and amount of physical memory that can be installed on even a consumer-level laptop is substantial. While we will likely never have enough physical memory on a computer to handle the increasingly large datasets that are being generated, the situation has gotten quite a bit easier over time.\nAt a higher level one “limitation” of R is that its functionality is based on consumer demand and (voluntary) user contributions. If no one feels like implementing your favorite method, then it’s your job to implement it (or you need to pay someone to do it). The capabilities of the R system generally reflect the interests of the R user community. As the community has ballooned in size over the past 10 years, the capabilities have similarly increased. When I first started using R, there was very little in the way of functionality for the physical sciences (physics, astronomy, etc.). However, now some of those communities have adopted R and we are seeing more code being written for those kinds of applications.\nUsing R and RStudio\n\nIf R is the engine and bare bones of your car, then RStudio is like the rest of the car. The engine is super critical part of your car. But in order to make things properly functional, you need to have a steering wheel, comfy seats, a radio, rear and side view mirrors, storage, and seatbelts. — Nicholas Tierney\n\n[Source]\nThe RStudio layout has the following features:\nOn the upper left, something called a Rmarkdown script\nOn the lower left, the R console\nOn the lower right, the view for files, plots, packages, help, and viewer.\nOn the upper right, the environment / history pane\n\n\n\nFigure 2: A screenshot of the RStudio integrated developer environment (IDE) – aka the working environment.\n\n\n\nThe R console is the bit where you can run your code. This is where the R code in your Rmarkdown document gets sent to run (we’ll learn about these files later).\nThe file/plot/pkg viewer is a handy browser for your current files, like Finder, or File Explorer, plots are where your plots appear, you can view packages, see the help files. And the environment / history pane contains the list of things you have created, and the past commands that you have run.\nInstalling R and RStudio\nIf you have not already, install R first. If you already have R installed, make sure it is a fairly recent version, version 4.0 or newer. If yours is older, I suggest you update (install a new R version).\nOnce you have R installed, install the free version of RStudio Desktop. Again, make sure it’s a recent version, it should be of the 1.4.X series.\n\nInstalling R and RStudio should be fairly straightforward. However, a great set of detailed instructions is in Rafael Irizarry’s dsbook\nhttps://rafalab.github.io/dsbook/installing-r-rstudio.html\n\nIf things don’t work, ask for help in the courseplus discussion board.\nI personally only have experience with Mac, but everything should work on all the standard operating systems (Windows, Mac, and even Linux).\nRStudio default options\nTo first get set up, I highly recommend changing the following setting\nTools > Global Options (or Cmd + , on macOS)\nUnder the General tab:\nFor workspace\nUncheck restore .RData into workspace at startup\nSave workspace to .RData on exit : “Never”\n\nFor History\nUncheck \"Always save history (even when not saving .RData)\nUncheck “Remove duplicate entries in history”\n\nThis means that you won’t save the objects and other things that you create in your R session and reload them. This is important for two reasons\nReproducibility: you don’t want to have objects from last week cluttering your session\nPrivacy: you don’t want to save private data or other things to your session. You only want to read these in.\nYour “history” is the commands that you have entered into R.\nAdditionally, not saving your history means that you won’t be relying on things that you typed in the last session, which is a good habit to get into!\nInstalling and loading R packages\nAs we discussed, most of the functionality and features in R come in the form of add-on packages. There are tens of thousands of packages available, some big, some small, some well documented, some not. We will be using many different packages in this course. Of course, you are free to install and use any package you come across for any of the assignments.\nThe “official” place for packages is the CRAN website. If you are interested in packages on a specific topic, the CRAN task views provide curated descriptions of packages sorted by topic.\nTo install an R package from CRAN, one can simply call the install.packages() function and pass the name of the package as an argument. For example, to install the ggplot2 package from CRAN: open RStudio,go to the R prompt (the > symbol) in the lower-left corner and type\n\n\ninstall.packages(\"ggplot2\")\n\n\n\nand the appropriate version of the package will be installed.\nOften, a package needs other packages to work (called dependencies), and they are installed automatically. It usually does not matter if you use a single or double quotation mark around the name of the package.\n\nQuestions:\nAs you installed the ggplot2 package, what other packages were installed?\nWhat happens if you tried to install GGplot2?\n\nIt could be that you already have all packages required by ggplot2 installed. In that case, you will not see any other packages installed. To see which of the packages above ggplot2 needs (and thus installs if it is not present), type into the R console:\n\n\ntools::package_dependencies(\"ggplot2\")\n\n\n\nIn RStudio, you can also install (and update/remove) packages by clicking on the ‘Packages’ tab in the bottom right window.\nIt is very common these days for packages to be developed on GitHub. It is possible to install packages from GitHub directly. Those usually contain the latest version of the package, with features that might not be available yet on the CRAN website. Sometimes, in early development stages, a package is only on GitHub until the developer(s) feel it is good enough for CRAN submission. So installing from GitHub gives you the latest. The downside is that packages under development can often be buggy and not working right. To install packages from GitHub, you need to install the remotes package and then use the following function\n\n\nremotes::install_github()\n\n\n\nWe will not do that now, but it is quite likely that at one point later in this course we will.\nYou only need to install a package once, unless you upgrade/re-install R. Once installed, you still need to load the package before you can use it. That has to happen every time you start a new R session. You do that using the library() command. For instance to load the ggplot2 package, type\n\n\nlibrary('ggplot2')\n\n\n\nYou may or may not see a short message on the screen. Some packages show messages when you load them, and others do not.\nThis was a quick overview of R packages. We will use a lot of them, so you will get used to them rather quickly.\nGetting started in RStudio\nWhile one can use R and do pretty much every task, including all the ones we cover in this class, without using RStudio, RStudio is very useful, has lots of features that make your R coding life easier and has become pretty much the default integrated development environment (IDE) for R. Since RStudio has lots of features, it takes time to learn them. A good resource to learn more about RStudio are the R Studio Essentials collection of videos.\n\nFor more information on setting up and getting started with R, RStudio, and R packages, read the Getting Started chapter in the dsbook:\nhttps://rafalab.github.io/dsbook/getting-started.html\nThis chapter gives some tips, shortcuts, and ideas that might be of interest even to those of you who already have R and/or RStudio experience.\n\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nIf a software company asks you, as a requirement for using their software, to sign a license that restricts you from using their software to commit illegal activities, is this consistent with the “Four Freedoms” of Free Software?\nWhat is an R package and what is it used for?\nWhat function in R can be used to install packages from CRAN?\nWhat is a limitation of the current R system?\n\nAdditional Resources\n\nR for Data Science by Wickham & Grolemund (2017). Covers most of the basics of using R for data analysis.\nAdvanced R by Wickham (2014). Covers a number of areas including object-oriented, programming, functional programming, profiling and other advanced topics.\nRStudio IDE cheatsheet\n\n\n\n\n",
    "preview": "https://github.com/allisonhorst/stats-illustrations/raw/master/rstats-artwork/r_first_then.png",
    "last_modified": "2021-08-30T23:12:24-04:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome!",
    "description": "Overview course information for students enrolled in JHSPH Biostatistics 140.776 in Fall 2021",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-08-31",
    "categories": [
      "course-admin",
      "module 1",
      "week 1"
    ],
    "contents": "\nWelcome! I am very excited to have you in our one-term (i.e. half a semester) course on Statistical Computing course number (140.776) offered by the Department of Biostatistics at the Johns Hopkins Bloomberg School of Public Health.\nThis course is designed for ScM and PhD students at Johns Hopkins Bloomberg School of Public Health. I am pretty flexible about permitting outside students, but I want everyone to be aware of the goals and assumptions so no one feels like they are surprised by how the class works.\n\nThe primary goal of the course is to teach you practical programming and computational skills required for the research and application of statistical methods.\n\nThis class is not designed to teach the theoretical aspects of statistical or computational methods, but rather the goal is to help with the practical issues related to setting up a statistical computing environment for data analyses, developing high-quality R packages, conducting reproducible data analyses, best practices for data visualization and writing code, and creating websites for personal or project use.\nAssumptions and pre-requisites\nThe course is designed for students in the Johns Hopkins Biostatistics Masters and PhD programs. However, we do not assume a significant background in statistics. Specifically we assume:\n1. You know the basics of at least one programming language (e.g. R or Python)\nIf it’s not R, we assume that you are willing to spend the time to learn R\nYou have heard of things such as control structures, functions, loops, etc\nKnow the difference between different data types (e.g. character, numeric, etc)\nKnow the basics of plotting (e.g. what is a scatterplot, histogram, etc)\n2. You know the basics of computing environments\nYou have access to a computing environment (i.e. locally on a laptop or working in the cloud)\nYou generally feel comfortable with installing and working with software\n3. You know the basics of statistics\nThe central dogma (estimates, standard errors, basic distributions, etc.)\nKey statistical terms and methods\nDifferences between estimation vs testing vs prediction\nKnow how to fit and interpret basic statistical models (e.g. linear models)\n4. You know the basics of reproducible research\nDifference between replication and reproducible\nKnow how to cite references (e.g. like in a publication)\nSomewhat familiar with tools that enable reproducible research (In complete transparency, we will briefly cover these topics in the first week, but depending on your comfort level with them, this may impact whether you choose to continue with the course).\nSince the target audience for this course is advanced students in statistics we will not be able to spend significant time covering these concepts and technologies. To give you some idea about how these prerequisites will impact your experience in the course, we will be turning in all assignments via R Markdown documents and you will be encouraged (not required) to use git/GitHub to track changes to your code over time. The majority of the assignments will involve learning the practical issues around performing data analyses, building software packages, building websites, etc all using the R programming language. Data analyses you will perform will also often involve significant data extraction, cleaning, and transformation. We will learn about tools to do all of this, but hopefully most of this sounds familiar to you so you can focus on the concepts we will be teaching around best practices for statistical computing.\n\nSome resources that may be useful if you feel you may be missing pieces of this background:\nStatistics - Mathematical Biostatistics Bootcamp I (Coursera); Mathematical Biostatistics Bootcamp II (Coursera)\nBasic Data Science - Cloud Data Science (Leanpub); Data Science Specialization (Coursera)\nVersion Control - Github Learning Lab; Happy Git and Github for the useR\nRmarkdown - Rmarkdown introduction\n\nGetting set up\nYou must install R and RStudio on your computing environment in order to complete this course. These are two different applications that must be installed separately before they can be used together:\nR is the core underlying programming language and computing engine that we will be learning in this course\nRStudio is an interface into R that makes many aspects of using and programming R simpler\nBoth R and RStudio are available for Windows, macOS, and most flavors of Unix and Linux. Please download the version that is suitable for your computing setup.\nThroughout the course, we will make use of numerous R add-on packages that must be installed over the Internet. Packages can be installed using the install.packages() function in R. For example, to install the tidyverse package, you can run\ninstall.packages(\"tidyverse\")\nin the R console.\nHow to Download R for Windows\nGo to https://cran.r-project.org and\nClick the link to “Download R for Windows”\nClick on “base”\nClick on “Download R 4.1.1 for Windows”\nVideo Demo for Downloading R for WindowsHow to Download R for the Mac\nGoto https://cran.r-project.org and\nClick the link to “Download R for (Mac) OS X”.\nClick on “R-4.1.1.pkg”\nVideo Demo for Downloading R for the MacHow to Download RStudio\nGoto https://rstudio.com and\nClick on “Products” in the top menu\nThen click on “RStudio” in the drop down menu\nClick on “RStudio Desktop”\nClick the button that says “DOWNLOAD RSTUDIO DESKTOP”\nClick the button under “RStudio Desktop” Free\nUnder the section “All Installers” choose the file that is appropriate for your operating system.\n\n\n\nVideo Demo for Downloading RStudioLearning Objectives\nThe goal is by the end of the class, students will be able to:\nInstall and configure software necessary for a statistical programming environment and with version control\nDiscuss generic programming language concepts as they are implemented in a high-level statistical language\nWrite and debug code in base R and the tidyverse (and integrate code from Python modules)\nBuild basic data visualizations using R and the tidyverse\nBuild and organize a software package with documentation for publishing on the internet\nDiscuss and implement basic statistical computing algorithms for optimization, linear regression, and Monte Carlo\nCourse Staff\nThe course instructor this year is Stephanie Hicks, but this course has been previously taught for a number of years by Roger Peng. We are both faculty in the Biostatistics Department at Johns Hopkins and Directors of the Johns Hopkins Data Science Lab.\nMy research focuses on developing fast, scalable, statistical methodology and open-source software for genomics and biomedical data analysis for human health and disease. My research is problem-forward: I develop statistical methods and software that are motivated by concrete problems, often with real-world, noisy, messy data. I’m also interested in developing theory for how to incorporate design thinking (alongside statistical thinking) in practice of data analysis.\nIf you want, you can find me on Twitter. I’m also a co-host of the The Corresponding Author podcast, member of the Editorial Board for Genome Biology, an Associate Editor for Reproducibility at the Journal of the American Statistical Association, and co-founder of R-Ladies Baltimore.\nRoger’s research focuses on air pollution, spatial statistics, and reproducibility. We have been colleagues and friends for over 3 years and I am really excited to have the opportunity to teach this course.\nWe also have a couple of amazing TAs this year:\nAthena Chen (achen70@jhu.edu). She is a fifth year PhD Candidate in the Department of Biostatistics working on characterizing antibody responses to various antigens including the human microbiome and viruses such as coronaviruses and HIV. She is broadly interested in immunology and statistical proteomics. In her free time, she enjoys weight lifting, yoga, cooking, and a nice cup of coffee.\nRuzhang Zhao (rzhao@jhu.edu). He is a third year PhD student from Department of Biostatistics. His research interests are single cell genomics and statistical genetics. For more information, please visit http://ruzhangzhao.com.\nCourse logistics\nAs with all things in a pandemic, this year we are continuing to teach this course virtually (similar to last year) to be able to have a large group of students benefit from it. The course webpage will be here at:\nhttps://www.stephaniehicks.com/jhustatcomputing2021\nAll communication for the course is going to take place on one of three platforms:\nCourseplus: for discussion, sharing resources, collaborating, and announcements\nGithub: for getting access to course materials (e.g. lectures, project assignments)\nCourse Github: https://github.com/stephaniehicks/jhustatcomputing2021\n\nZoom: for live class lectures\nCourse Zoom: Link available on Courseplus\nThe plan is for recorded lectures will be posted online after class ends\n\nThe primary communication for the class will go through Courseplus That is where we will post course announcements, host most of our asynchronous course discussion, and as the primary means of communication between course participants and course instructors.\n\nIf you are registered for the course, you should have access to Courseplus now. Once you have access you will also be able to find the course Zoom links. Zoom links for office hours will also be posted on Courseplus.\n\nAssignment Due Dates\nAll course assignment due dates appear on the Schedule and Syllabus.\nThe Pandemic\nThis is how 2020 felt:\n\n\n\nFigure 1: How 2020 felt\n\n\n\nWhile there are many positive things that have happened in 2021, for many folks, 2021 has not been much of an improvement\n\n\n\nFigure 2: How 2021 feels\n\n\n\nIt is super tough to be dealing with the pandemic, an economic crisis, challenges with visas and travel and coordinating school online. As your instructor, I understand that this is not an ordinary year. I am ultra sympathetic to family challenges and life challenges. I have three small children (who may make cameos in lectures frome time to time).\nMy goal is to make as much of the class asynchronous as possible so you can work whenever you have time. My plan is to be as understanding as possible when it comes to grading, and any issues that come up with the course. Please don’t hesitate to reach out to me (or the TAs) if you are having issues and we will do our best to direct you to whatever resources we have/accommodate you however we can.\nI think the material in this course is important, fun, and this is an opportunity to learn a lot. But life is more important than a course and if there was ever a time that life might get in the way of learning, it’s likely now.\nGrading\nPhilosophy\nWe believe the purpose of graduate education is to train you to be able to think for yourself and initiate and complete your own projects. We are super excited to talk to you about ideas, work out solutions with you, and help you to figure out how to produce professional data analyses. We do not think that graduate school grades are important for this purpose. This means that we do not care very much about graduate student grades.\nThat being said, we have to give you a grade so they will be:\nA - Excellent - 90%+\nB - Passing - 80%+\nC - Needs improvement - 70%+\nWe rarely give out grades below a C and if you consistently submit work, and do your best you are very likely to get an A or a B in the course.\nRelative weights\nThe grades are based on three projects (plus one entirely optional project to help you get set up). The breakdown of grading will be\n33% for Project 1\n33% for Project 2\n34% for Project 3\nIf you submit an project solution, it is your own work, and it meets a basic level of completeness and effort you will get 100% for that project. If you submit a project solution, but it doesn’t meet basic completeness and effort you will receive 50%. If you do not submit an solution you will receive 0%.\nSubmitting assignments\nPlease write up your project solutions using R Markdown. In some cases, you will compile a R Markdown file into an HTML file and submit your HTML file to the dropbox on Courseplus. In other cases, you may create an R package or website. In all of the above, when applicable, show all your code and provide as much explanation / documentation as you can.\nFor each project, we will provide a time when we download the materials. We will assume whatever version we download at that time is what you are turning in.\nReproducibility\nWe will talk about reproducibility a bit during class, and it will be a part of the homework assignments as well. Reproducibility of scientific code is very challenging, so the faculty and TAs completely understand difficulties that arise. But we think that it is important that you practice reproducible research. In particular, your project assignments should perform the tasks that you are asked to do and create the figures and tables you are asked to make as a part of the compilation of your document. We will have some pointers for some issues that have come up as we announce the projects.\nCode of Conduct\nWe are committed to providing a welcoming, inclusive, and harassment-free experience for everyone, regardless of gender, gender identity and expression, age, sexual orientation, disability, physical appearance, body size, race, ethnicity, religion (or lack thereof), political beliefs/leanings, or technology choices. We do not tolerate harassment of course participants in any form. Sexual language and imagery is not appropriate for any work event, including group meetings, conferences, talks, parties, Twitter and other online media. This code of conduct applies to all course participants, including instructors and TAs, and applies to all modes of interaction, both in-person and online, including GitHub project repos, Slack channels, and Twitter.\nCourse participants violating these rules will be referred to leadership of the Department of Biostatistics and the Title IX coordinator at JHU and may face expulsion from the class.\nAll class participants agree to:\nBe considerate in speech and actions, and actively seek to acknowledge and respect the boundaries of other members.\nBe respectful. Disagreements happen, but do not require poor behavior or poor manners. Frustration is inevitable, but it should never turn into a personal attack. A community where people feel uncomfortable or threatened is not a productive one. Course participants should be respectful both of the other course participants and those outside the course.\nRefrain from demeaning, discriminatory, or harassing behavior and speech. Harassment includes, but is not limited to: deliberate intimidation; stalking; unwanted photography or recording; sustained or willful disruption of talks or other events; inappropriate physical contact; use of sexual or discriminatory imagery, comments, or jokes; and unwelcome sexual attention. If you feel that someone has harassed you or otherwise treated you inappropriately, please alert Stephanie Hicks.\nTake care of each other. Refrain from advocating for, or encouraging, any of the above behavior. And, if someone asks you to stop, then stop. Alert Stephanie Hicks if you notice a dangerous situation, someone in distress, or violations of this code of conduct, even if they seem inconsequential.\nNeed Help?\nPlease speak with Stephanie Hicks or one of the TAs. You can also reach out to Karen Bandeen-Roche, chair of the department of Biostatistics or Margaret Taub, Ombudsman for the Department of Biostatistics.\nYou may also reach out to any Hopkins resource for sexual harassment, discrimination, or misconduct:\nJHU Sexual Assault Helpline, 410-516-7333 (confidential)\nUniversity Sexual Assault Response and Prevention website\nJohns Hopkins Compliance Hotline, 844-SPEAK2US (844-733-2528)\nHopkins Policies Online\nJHU Office of Institutional Equity 410-516-8075 (nonconfidential)\nJohns Hopkins Student Assistance Program (JHSAP), 443-287-7000\nUniversity Health Services, 410-955-1892\nThe Faculty and Staff Assistance Program (FASAP), 443-997-7000\nFeedback\nWe welcome feedback on this Code of Conduct.\nLicense and attribution\nThis Code of Conduct is distributed under a Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) license. Portions of above text comprised of language from the Codes of Conduct adopted by rOpenSci and Django, which are licensed by CC BY-SA 4.0 and CC BY 3.0. This work was further inspired by Ada Initiative’s ‘’how to design a code of conduct for your community’’ and Geek Feminism’s Code of conduct evaluations and expanded by Ashley Johnson and Shannon Ellis in the Jeff Leek group.\nAcademic Ethics\nStudents enrolled in the Bloomberg School of Public Health of The Johns Hopkins University assume an obligation to conduct themselves in a manner appropriate to the University’s mission as an institution of higher education. A student is obligated to refrain from acts which he or she knows, or under the circumstances has reason to know, impair the academic integrity of the University. Violations of academic integrity include, but are not limited to: cheating; plagiarism; knowingly furnishing false information to any agent of the University for inclusion in the academic record; violation of the rights and welfare of animal or human subjects in research; and misconduct as a member of either School or University committees or recognized groups or organizations.\nStudents should be familiar with the policies and procedures specified under Policy and Procedure Manual Student-01 (Academic Ethics), available on the school’s portal.\nThe faculty, staff and students of the Bloomberg School of Public Health and the Johns Hopkins University have the shared responsibility to conduct themselves in a manner that upholds the law and respects the rights of others. Students enrolled in the School are subject to the Student Conduct Code (detailed in Policy and Procedure Manual Student-06) and assume an obligation to conduct themselves in a manner which upholds the law and respects the rights of others. They are responsible for maintaining the academic integrity of the institution and for preserving an environment conducive to the safe pursuit of the School’s educational, research, and professional practice missions.\nDisability support services\nIf you are a student with a documented disability who requires an academic accommodation, please contact the Office of Disability Support Services at 410-502-6602 or via email at JHSPH.dss@jhu.edu. Accommodations take effect upon approval and apply to the remainder of the time for which a student is registered and enrolled at the Bloomberg School of Public Health.\nPrevious versions of the class\nhttps://rdpeng.github.io/Biostat776\nTypos and corrections\nFeel free to submit typos/errors/etc via the github repository associated with the class: https://github.com/stephaniehicks/jhustatcomputing2021. You will have the thanks of your grateful instructor!\n\n\n\n",
    "preview": "https://media.giphy.com/media/XdIOEZTt6dL7zTYWIo/giphy.gif",
    "last_modified": "2021-08-31T12:31:02-04:00",
    "input_file": {}
  }
]
